[
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "Correspondence to: Denis Newman-Griffis, d.r.newman-griffis@sheffield.ac.uk  AI Thinking: A framework for rethinking artificial intelligence in practice   Denis Newman-Griffis Information School, University of Sheffield, Sheffield, UK Research on Research Institute, London, UK   Abstract Artificial intelligence is transforming the way we work with information across disciplines and practical contexts. A growing range of disciplines are now involved in studying, developing, and assessing the use of AI in practice, but these disciplines often employ conflicting understandings of what AI is and what is involved in its use. New, interdisciplinary approaches are needed to bridge competing conceptualisations of AI in practice and help shape the future of AI use. I propose a novel conceptual fram"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "uture of AI use. I propose a novel conceptual framework called AI Thinking, which models key decisions and considerations involved in AI use across disciplinary perspectives. The AI Thinking model addresses five practice-based competencies involved in applying AI in context: motivating AI use in information processes, formulating AI methods, assessing available tools and technologies, selecting appropriate data, and situating AI in the sociotechnical contexts it is used in. A hypothetical case study is provided to illustrate the application of AI Thinking in practice. This article situates AI Thinking in broader cross-disciplinary discourses of AI, including its connections to ongoing discussions around AI literacy and AI-driven innovation. AI Thinking can help to bridge divides between ac"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": " AI Thinking can help to bridge divides between academic disciplines and diverse contexts of AI use, and to reshape the future of AI in practice.  Keywords: Artificial intelligence; interdisciplinarity; AI applications; machine learning; AI Thinking; Critical data studies 1. Introduction Artificial intelligence has been positioned as a key driver of global change through a fourth industrial revolution, and AI advances are actively transforming how we process, analyse, and learn from information (1–3). The increasing relevance of AI across disciplines and sectors has led to a wide variety of views on the nature of this transformation, but it is most often envisioned through a technological lens: self-contained AI technologies that change (or replace) the processes we use to work with inform"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": " replace) the processes we use to work with information. However, the technology-centric perspective that drives much of AI development and innovation fails to capture important aspects of the complex, \nAI Thinking: A framework for rethinking artificial intelligence in practice \n 2 human contexts in which AI is used, and is increasingly insufficient to address the challenges of effectively and ethically using AI in real-world settings (4–6).  The ongoing changes in the AI landscape are not only affecting the scope and scale of AI use: they are also reshaping what it looks like to work with AI technologies in practice. For most of its history, AI has been a specialised, technical discipline, focused on methodological innovation and requiring advanced training and significant computational r"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": " advanced training and significant computational resources to use effectively. As technologies have matured, AI is in the midst of an expansion from expert technique to everyday toolkit, opening up new opportunities for its use to ask novel questions and perform complex analyses without the need for deep technical expertise or innovation (7,8). This growth in the purposes for which AI is used has outpaced the professional training and development of best practice required to ensure that AI use is both effective and ethical. This has left significant knowledge gaps in the specific skills and competencies involved in using AI in practice, especially in newer and more interdisciplinary contexts, and the specific skills and competencies needed to work with AI as part of processing information."
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "to work with AI as part of processing information.  Part and parcel of these knowledge gaps, and a key challenge created by the growth of AI application and research, is the fact that different disciplines, fields, application contexts, and stakeholders have different conceptualisations of what AI is and what is involved in using it. Each discipline in which AI is studied or used tends to define and operationalise AI in its own ways, with explicitly interdisciplinary approaches to AI in practice being few and far between (9–11). These different conceptualisations of AI in disciplinary literature and applied contexts are rarely surfaced and often in conflict with one another, functioning as competing epistemologies rather than complementary viewpoints. As AI use becomes more urgently interd"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "viewpoints. As AI use becomes more urgently interdisciplinary, by virtue of the diversity of AI users and the breadth of impact from AI applications, we need strategies to help partially reconcile these different understandings of AI in practice, and to work effectively across varied perspectives on AI to develop shared practices for AI use.  In this article, I propose a novel framework, which I call AI Thinking, which holistically models key decisions and considerations involved in bringing AI into practical use. AI Thinking is a conceptual, competency-based model of the practices involved in applying AI in context, designed to help users of AI systems ensure that their applications are both goal-driven and context-sensitive. To do this, I approach AI as an information methodology, rather"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": " approach AI as an information methodology, rather than an information technology, in the process describing and affecting how we work with information as a whole. AI Thinking bridges disciplinary perspectives, illustrating how the design and implementation of AI systems is inextricable from the practical contexts they operate in, and how the social systems surrounding AI are reshaped by the process of computation. By providing practice-based connections between different disciplines and practical contexts, AI Thinking \nAI Thinking: A framework for rethinking artificial intelligence in practice \n 3 encourages users to work with AI as an interconnected set of technical processes and social practices. By framing AI use in terms of the practices that connect specific problems, technologies, a"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "es that connect specific problems, technologies, and contexts, AI Thinking highlights shared concerns and reference points that help connect different definitions and practical understandings of AI. The AI Thinking framework thus makes clearer links between academic disciplines and across diverse contexts of AI use, and can help to lower communication costs between different perspectives on what AI means and how to understand, train people in, and manage its use.  2. Bridging AI divides across disciplines and contexts To illustrate the need for a more interdisciplinary conceptualisation of how we work with AI in practice, I first outline key differences in the broad range of conceptualisations of AI. The aim of the AI Thinking model is to bridge between competing (and sometimes incompatibl"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "ridge between competing (and sometimes incompatible) perspectives and approaches across disciplines, including computer science and engineering, science and technology studies, critical data/AI studies, and the broad landscape of AI applications. Negotiating the varying conceptualisations adopted in these fields, and the different visions they present of what matters most in studying and managing AI, is one of the most significant current challenges in responsibly applying and managing AI in practice. The competency-based model of AI Thinking presented in this article reflects on these varied understandings together with the growing body of evidence on AI practices in diverse settings, including research and innovation, business, health, and policy (12–18). In the sections below, I briefly"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "d policy (12–18). In the sections below, I briefly outline major differences in AI understandings and the historical contexts from which AI practices and the AI Thinking model have emerged.  2.1 Operational terminology The current environment of rapid change in AI technologies and applications is also one of rapid change in language. The range of disciplinary perspectives addressed in this article make it necessary to establish an operational terminology with which to describe AI in practice, and to set the scope of what aspects of AI technologies and practice are in focus.  AI use: I focus in this article on the use of AI for information processing—eg, transforming and analysing data, learning from evidence, and informing action—common across many applications in business, research, polic"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "oss many applications in business, research, policy, healthcare, and other decision-making settings. The AI Thinking framework has potential to inform more creative uses of AI, eg for content generation or creative practice, and may prove helpful in exploring evolving practice in this area. This focus on information processing does not exclude the use of generative AI, which has significant (and developing) applications for both information processing and creative use (cf (19,20)).  \nAI Thinking: A framework for rethinking artificial intelligence in practice \n 4 AI technologies: Artificial intelligence, as a family of technologies, remains notoriously difficult to define. The evolving scope of AI technologies—ie, specific models, algorithms, and technological products—includes interactive "
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": ", and technological products—includes interactive systems such as generative AI, foundation models for adapting general-purpose knowledge, and more specialised AI systems (sometimes referred to as “narrow” AI) targeting specific applications through bespoke machine learning or expert systems (21,22). The use of “AI” in this article does not distinguish between these types of AI technologies, but focuses on common processes and ways of thinking that inform all of these types of AI in practice.   AI systems: AI use is often discussed in terms of “AI systems,” but the nature and boundaries of these systems have varied definitions. In many cases, for example in industrial practice (23) or international policy contexts (22), AI systems are defined in terms of software and technical systems alon"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "ed in terms of software and technical systems alone. In presenting the framework of AI Thinking, I follow thinking in critical data studies by taking a wider view of the contexts in which AI software and technologies are inextricably situated, and recognising the essential role of non-technical stakeholders and perspectives in setting the shape and the purpose of AI technologies (24). Attending to the broader milieu in which AI operates calls for a systems thinking approach (25), and reflects the inherently sociotechnical nature of AI (6). Thus, from an AI Thinking perspective, an ‘AI system’ includes not only the software and technologies through which AI computation occurs, but also the broader system of people, data, and processes which motivate and make use of AI computation.  2.2 Diff"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "motivate and make use of AI computation.  2.2 Differences in AI understandings across disciplines The historical contexts of different fields and areas of AI application have naturally led to different conceptualisations of the same ideas. These differences, which are not always immediately visible, hinder the interdisciplinary dialogue and collaboration necessary to make AI use ethical, explainable, and effective in practice (7,26,27). Differences especially in the language around AI and the scope of who is involved in its application can also reflect deeper underlying differences in how AI is conceptualised between disciplines and contexts, and what aspects of AI systems are considered most important.   In my proposed AI Thinking framework, I aim to provide points of common reference and"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "k, I aim to provide points of common reference and comparison to work across different understandings of AI by joining up technical and social practices involved in AI use. Here, I briefly outline three common ways of framing AI observed across different disciplinary contexts: AI as a linear, cyclical, and relational system. I show how an AI Thinking approach can help to bridge their distinct perspectives on data, computation, and the scope of AI systems.  \nAI Thinking: A framework for rethinking artificial intelligence in practice \n 5 A linear understanding of AI, commonly encountered in computer science and engineering disciplines, focuses on the process of computation: beginning with input data, performing a sequence of transformations and analyses, and producing output for further use."
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "nd analyses, and producing output for further use. On this framing, computation is emphasised and data are pre-existing, material objects to be consumed, transformed, and/or generated. AI systems are typically considered through a technological lens, as separate concerns from the production of data or use of AI outputs. This perspective is reflected in the directional flow diagrams commonly used to illustrate AI model structures and data analysis in computing literature (cf (28,29)) and some applications (30).  A cyclical understanding of AI, commonly seen in AI applications such as in healthcare or finance, takes a more process-oriented view of AI use, including data collection, analysis, interpretation and assessment, and action taken in response, which frequently produces new data for a"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "response, which frequently produces new data for a subsequent cycle. On this framing, computation is one piece of a larger process which forms the broader context for AI systems as embedded technologies. Data are emphasised, as representative of the information being moved, understood, and acted upon in the overarching process. This perspective is reflected in visualisations and discussion of AI systems in professional practice (31,32) and in the growing literature on human-AI teaming (33,34).  A relational understanding of AI, more commonly encountered in social sciences and humanities disciplines, focuses on the people, perspectives, and purposes behind the use of AI in sociotechnical contexts. A wide range of theoretical approaches are used to investigate AI from more relational perspec"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "sed to investigate AI from more relational perspectives, including systems thinking (25) and assemblage theory (35), as well as more specific models such as data journeys (36). On this framing, computation is frequently a means rather than a process, emphasising what Amoore describes as ‘the capacity to generate an actionable output from a set of attributes’ (37), and the data that describe these attributes are a dynamic and contested site of practice. This perspective is frequently reflected in critical studies of AI and analyses of AI harms (38,39). The definition of AI systems adopted for this article most closely aligns with a relational understanding of AI.  To create a cohesive model for AI use across disciplines, these differing understandings of AI must be treated as what they are:"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "rstandings of AI must be treated as what they are: equally necessary and informative for a full picture of AI Thinking. The linear framing is essential for choosing between and working with specific AI technologies, which are engineered to reflect a linear pathway from input to output. At the same time, the cyclical framing is key to assessing data sources and their implications for stakeholders in the processes where AI is brought to bear. And a relational perspective is required to understand the contextual motivators and impacts of AI use, and how success and risk can be most meaningfully measured.  \nAI Thinking: A framework for rethinking artificial intelligence in practice \n 6 The AI Thinking framework reflects elements of each of these framings, helping to reveal interconnections bet"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "e framings, helping to reveal interconnections between them in practice and to find common ground within the interdisciplinary teams necessary for AI research, application, and regulation. AI Thinking thus helps to join up aspects of AI systems: illustrating how the relational networks and data cultures affect the input data used for linear computation, and interpret and use the output of that computation; and how the cyclical nature of pre-existing processes helps to shape the linear process implemented in the design of an AI technology for practical use. By understanding that these conceptualisations of AI are not in competition, but rather reflect different and equally necessary parts of the same sociotechnical systems, AI Thinking can help begin to create a cohesive model of AI use acr"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "elp begin to create a cohesive model of AI use across disciplines.  2.3 Roots of AI Thinking: specialisation & practice As well as drawing on a variety of disciplinary conceptualisations of AI, the AI Thinking framework explicitly builds on the practices of Statistical Thinking developed in the late nineteenth and early twentieth centuries. Similar to the way statistical methodologies matured and gave rise to wider statistical practices at the turn of the twentieth century, AI Thinking is rooted in the maturation of the AI field and its expansion from a technical specialism into a broader methodology.  Early development and use of statistical methods was primarily a matter of mathematical research, though often motivated by and entwined with the needs of states to understand changing popul"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "h the needs of states to understand changing populations (40). As statistical methods matured, however, they began to have significant value for other disciplines grappling with measuring and understanding complex phenomena, and by the mid twentieth century statistical methodologies had grown far beyond the original bounds of statistics as a specialised discipline and become essential to research and practice in psychology, medicine, and the natural sciences (41). In the later twentieth and twenty-first centuries, statistical methods have become increasingly essential to decision-making in business also (42). Whilst statistics has certainly continued to develop as a rich discipline and its own specialism, fundamentals of statistical thinking—considerations of sample size, recognition of ra"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "g—considerations of sample size, recognition of random effects, uncertainty as part and parcel of research (43)—have become core competencies for contemporary research and practice across disciplines and a wide variety of applied contexts (44).   AI methods are beginning to play a similar role, providing new tools for working with the increasingly complex combinations of data and knowledge sources that characterise contemporary information systems. AI techniques, particularly machine learning, are reshaping our ability to learn from multiple information sources and combine data in complex ways. For example, in research spheres, the rapid diffusion of AI methods across research practice, from combining exascale imaging data (45) and protein folding (46) to supporting data analysis in \nAI Th"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "folding (46) to supporting data analysis in \nAI Thinking: A framework for rethinking artificial intelligence in practice \n 7 clinical trials (47) and digital humanities (48), is moving us towards a research ecosystem where using AI methods is necessary for competitiveness in many areas of inquiry. Similar transformations are occurring in business, healthcare, policy, and other areas of information processing (15,49–51). Thus, while AI research as its own specialism will also continue to mature, fundamentals of AI Thinking are needed to help inform why, when, and how to use AI methods effectively, and to understand both their limitations and their strengths.  Like statistical thinking, AI Thinking is rooted in the professional practices and knowledge that are the foundation of training for "
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "knowledge that are the foundation of training for experts in the field. Statisticians learn how to assess sample size, to measure uncertainty, to characterise populations, and so forth, and these are key building blocks with which deeper expert knowledge is built (43). Similarly, AI experts learn fundamental skills such as problem formulation (i.e., mapping a particular application to a known type of task with established methodologies), selecting and managing data, choosing models and algorithms, and interpreting the results of AI computation. But at this point, practices diverge. Expert statisticians develop deep knowledge of mechanics and principles, and may innovate with new measures and rich characterisations of complex phenomena; AI experts develop deep knowledge of mathematical and "
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "xperts develop deep knowledge of mathematical and cognitive theory, and may innovate complex new model structures and learning algorithms. But for most users of these methodologies, this depth of expertise is unnecessary: the key knowledge is operational, knowing how to develop an appropriate process to ensure reliable evidence or synthesise information. This operational knowledge can then be combined with a deep understanding of the context in which the methods are applied, whether in specialised research, policy, decision-making, or other kinds of information processing. This bifurcation into technical expertise on the one hand and operational methodology on the other has come to characterise the dual—though actively-contested—nature of statistics in practice (52), and is a productive fr"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "tatistics in practice (52), and is a productive framing for approaching the emerging dual nature of AI.  It is important to note that AI Thinking is not the next evolution of statistical thinking, nor does it supplant statistical thinking. Indeed, as most contemporary AI relies on statistical methodologies, statistical competence is essential to AI competence, and helps AI users to cast a critical eye on the uncertainty inherent in using AI methods. AI Thinking and statistical thinking thus sit alongside one another as fundamental competencies shared across areas of working with information.  3. AI Thinking: An interdisciplinary, competency-based framework for AI use in practice AI Thinking is a conceptual, competency-based model of the practices, decisions, and considerations involved in "
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "ctices, decisions, and considerations involved in applying AI systems in context. AI Thinking is process-oriented and practice-based; competency models thus offer a natural fit for defining what AI Thinking looks \nAI Thinking: A framework for rethinking artificial intelligence in practice \n 8 like, and a practical foundation for teaching AI Thinking as a foundational information skill across disciplines and contexts (53). I outline below five core competencies of AI Thinking, representing key skills for working with AI methods as a practical information processing tool. As AI systems are typically a team effort, it is not necessary that any one person master all five of these competencies; rather, they offer a way for identifying and seeking out the skills needed in an AI team. These compe"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "g out the skills needed in an AI team. These competencies present a starting point for further expansion into a systematic AI Thinking framework, akin to the statistical thinking model of Wild and Pfannkuch (43), and drawing on early development of professional competency frameworks for AI (49,54).   The AI Thinking framework, illustrated in Figure 1, comprises five major elements of working with AI methods in practice: 1) initial alignment of AI use with process-oriented needs; 2) the formulation of AI approaches to respond to those needs; 3) assessing and selecting potential AI tools and technologies, to implement the chosen formulation; 4) assessing and selecting data sources to inform the chosen tools and technologies; and 5) ensuring the full lifecycle of AI system design, implementat"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "he full lifecycle of AI system design, implementation, and management is rooted in the context of use. Figure 2 \nFigure 1. Five key competencies of AI Thinking. These competencies are arranged in a circle to show that they are interrelated and used in parallel, rather than separate concerns addressed in sequence. \nAI Thinking: A framework for rethinking artificial intelligence in practice \n 9 summarises key elements of each of these five competencies, and illustrates their interrelationships within a linearised process of AI system design.  3.1 Process: Motivating AI use by specific needs Definitions of AI have changed as technologies have evolved, but AI research and innovation has been consistently characterised by a focus on doing specific tasks, even as AI technologies themselves have "
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "ic tasks, even as AI technologies themselves have become more general-purpose (22). However, adopting AI methods in practice is often motivated by a pressure to be innovation-led and stay ahead of the technology curve, rather than as a response to specific needs for information processing that AI methods can help address (55). The greatest practical benefits from AI have historically been achieved when its use \nFigure 2. The competencies of the AI Thinking model. Each competency is aligned with one or more portions of a linearised Illustration of the process of using AI. \nAI Thinking: A framework for rethinking artificial intelligence in practice \n 10 is motivated by specific needs in the service of broader goals: e.g., development of AlphaFold for identifying candidate protein structures "
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "Fold for identifying candidate protein structures from a vast possibility space (46); now-ubiquitous speech recognition technologies initially developed for supporting call centres (56); and automated analysis of medical imaging to support cancer diagnosis (57). In each case, a particular part of the process was identified for AI intervention, and AI use was guided by an understanding of that process.  Adopting a process-oriented motivation for AI use, and ensuring that AI applications are goal-driven rather than innovation-led, is the first key competency of AI Thinking. Taking a process-oriented approach to AI involves three distinct elements: ● AI use is goal-driven: developing a new AI technology or bringing an existing technology to bear is motivated by a desire to make use of informa"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "ar is motivated by a desire to make use of information in service of a specific goal. ● AI use has a defined scope within the process of achieving that goal, typically in terms of specific steps or distinct operations needed. There may be multiple distinct scopes for AI use within a process, which may be approached individually or jointly. ● AI systems are designed to respond to specific opportunities arising within each scope for use, in terms of specific information availability and information needs at each step. Not all steps of a process will present appropriate opportunities for AI use.  This process-based approach does not preclude opportunities for innovation and discovery from using AI in a more “end-to-end” fashion to complete multiple steps or the entire process at once (discuss"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "tiple steps or the entire process at once (discussed further below), but approaching AI use from this foundation in process clarifies opportunities and scope for intervention, provides multiple points for innovation as well as risk mitigation, and ensures that AI use is guided directly by desired goals. This approach of breaking down goals into distinct steps for AI intervention is particularly valuable when information processes need to demonstrably adhere to community norms and standards, as in research practice or business processes, or even by statutory or regulatory requirements.  Beginning from a foundation in process, AI use in practice is then informed by the next AI Thinking competency, describing and formulating the specific problem that AI use is intended to help solve.  3.2 For"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "em that AI use is intended to help solve.  3.2 Formulation: Describing the problem to solve with AI AI is a discipline of problem-solving, and identifying how best to describe the problems to be solved has been a core component of AI development and use from its earliest days (58). Problem formulation involves creating formal descriptions of AI problems (typically what type \nAI Thinking: A framework for rethinking artificial intelligence in practice \n 11 of output should be produced from some type of input), and is an essential step in bridging from the complexity of real-world problems to the well-defined domain of mathematical models in AI systems. AI problem formulations have evolved from stricter mathematical design under planning paradigms of AI (59,60) to more operational definitions"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "igms of AI (59,60) to more operational definitions of tasks to perform and ways to conceptualise AI computation in current machine learning paradigms (61), but the core step of formalising the operation of an AI system remains essential for AI development and evaluation.  These shared definitions are powerful tools, making it possible to reuse and repurpose a common base of AI models and methods. For example, approaching a new, complex text labelling problem as a named entity recognition problem (in which text segments describing real-world referents are identified) enables repurposing a wide existing base of technical methods for a new purpose (62). However, as a variety of recent critical analyses have illustrated (63–65), problem formulation is highly epistemically loaded, involving nor"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "tion is highly epistemically loaded, involving normative decisions about what matters in the operation of an AI system and what that system can meaningfully say about its inputs and the world. Negotiating the tension between the benefits of shared problem formulations and the epistemic positions they represent is thus an essential part of AI use in practical contexts.  The second key competency of AI Thinking is thus understanding and negotiating the process of problem formulation for AI systems in practice. This comprises three main elements: ● The AI task the system is to perform, such as image classification, text generation, or speech recognition. This defines the templates by which the system should operate, and opens up opportunities for using some methods and models while closing th"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "for using some methods and models while closing the door on others. ● The output the system will produce: will it be one categorical label? Multiple numeric scores? A sequence of text? This affects how the AI system can inform its users, and what it is capable of saying about its inputs. ● If using a machine learning approach, the training signal from which the system will learn. This includes both what will be used to signal the desired or undesired outcome of AI processing, and the measures used to penalise errors and fit the AI model to the observed data.  Any practical information processing need where there is appropriate scope for AI intervention is likely to be open to multiple alternative problem formulations, and may in fact consist of multiple related sub-problems. This space of "
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "t of multiple related sub-problems. This space of possible formulations and decomposition of sub-problems is illustrated by a wide range of examples of practical AI applications: Cheng et al. decomposed planning-based models for business decision-making into multiple related decision processes for greater clarity and optimisation (66); Newman-Griffis and Fosler-Lussier compare AI paradigms for categorising functional status information (67), and Isagah and Ben Dhaou describe several diverse examples of AI problem formulations in government (68). \nAI Thinking: A framework for rethinking artificial intelligence in practice \n 12  Once a process-oriented goal for AI use has been transformed into specific problem formulations, the third AI Thinking competency addresses using the affordances off"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "ing competency addresses using the affordances offered by different AI tools and technologies to select appropriate methodologies.  3.3 Tools and technologies: Assessing AI affordances As AI technologies advance at a rapid pace, keeping track of the changing landscape can seem unmanageable, and AI users often feel like new skills must be constantly relearned with each new advance (69). Comparing and contrasting available technologies for a given use of AI, and adapting and responding to new technological advancements, is thus a key skill for working with AI in practice and building sustainable AI systems that can weather the storms of technological change.  The third key competency of AI Thinking is to conceptualise AI tools and technologies in terms of the technological affordances they p"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "s in terms of the technological affordances they present, and how these affordances align with the intended use of AI. Technology affordances (70) extend psychological affordance theory to describe the perceived possibilities that a given technology offers to potential users, and the strengths and weaknesses it presents for different possible uses. While any specific technology may be repurposed, hacked, or extended to do a wide variety of things beyond its original intended purpose, affordances are a powerful way of characterising likely and easily perceptible opportunities a technology offers, which form a core functionality for practice. Focusing on affordances, and how they align to specific information processing needs, thus provides a clear, process-oriented strategy for comparing av"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": " clear, process-oriented strategy for comparing available AI technologies and for assessing new technologies—and the risk and reward in their adoption—when they emerge (71).  Each AI tool and technology is engineered with a particular task in mind, and each offers specific perceptible affordances for information processing. These affordances are dynamic, as new uses are discovered and reshaped, and may go far beyond the original motivating task; for example, the growth of prompt-based learning in generative AI (31), or the shift in use of AlexNet (29) and other benchmark computer vision models from image classification to broader representation of image features for many purposes (72). Assessing the affordances of AI technologies involves four key components: ● The purpose and AI paradigm "
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "our key components: ● The purpose and AI paradigm for which a given technology was designed, including its connections to specific problem formulations as well as identifying where overlapping or complementary task definitions provide opportunities (and limitations) for interoperability and reuse. \nAI Thinking: A framework for rethinking artificial intelligence in practice \n 13 ● AI technologies have variable complexity and data requirements that affect their viability and effectiveness for different applications. As a general rule, more complex AI models have greater capacity to capture complex patterns and relationships, but also require larger volumes of data to work effectively. Task complexity and data availability vary widely across contexts and are essential for selecting appropriat"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "ontexts and are essential for selecting appropriate technologies.  ● AI technologies must also be assessed in terms of their computational requirements, ie, the hardware required to execute model calculations efficiently. While the shift to cloud computing has reduced the need to manage on-premise hardware, financial cost and environmental impact of different AI technology options are directly driven by computing requirements.  ● Each technology, and each task formulation underpinning it, presents strengths and limitations for performing different tasks. These may be characterised by the initial research and development of new models, emerge from later, evolving uses of those models, or be considered as adaptations to an underlying framework.  Rather than assuming that newer or higher-perf"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "k.  Rather than assuming that newer or higher-performing technologies are necessarily better, assessing the affordances of AI technologies along these dimensions provides a balanced way to select the appropriate tool or technology for a given application. For example, in detecting and modelling rare diseases, a simpler model may be preferable to data-hungry deep learning (73), while the reverse is true for cancer detection and prognosis, where complex relationships between hundreds of variables are needed and rich multimodal data are available to learn from (74). A large language model may be an appropriate choice for processing common financial reports (75), while more bespoke analysis may be a better fit for analysing individual forms or detailed patterns of language use (76). As new tec"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "detailed patterns of language use (76). As new technologies emerge, the same process of assessing their affordances can provide a clear pathway to determining whether they should be adopted over current approaches, and identifying what new problems they may be fit to tackle.  With a process in mind and an understanding of the affordances on offer from different AI tools and technologies to support that process, the next key competency of AI Thinking is identifying the appropriate data sources to guide AI implementation for the desired goal.  3.4 Data: Informing AI use with appropriate information Data are often said to be the foundation of AI systems (16). What is less often noted is that data are also a choice, both in the process of their production and in their use in AI. The idea of “r"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": " production and in their use in AI. The idea of “raw data,” unprocessed and therefore neutral, lingers in technical discourses (23), but extensive scholarship has shown how the production of data reflects multiple choices about what counts, what to collect, and what to record about a given subject (77–79). There are also practical concerns of the compatibility of different data sources and datasets with the available tools and \nAI Thinking: A framework for rethinking artificial intelligence in practice \n 14 technologies, reflected in the well-established central role of choosing appropriate training data in machine learning research (80). In a process-oriented AI Thinking approach, the fourth key competency is thus to assess which of the available data fields, sets, and sources are most ap"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "ailable data fields, sets, and sources are most appropriate to inform the desired goals and the processes in which AI is to be used.  Assessing data sources is thus a matter of both epistemology and practicality, with each having significant power to affect bias and fairness of data and the ethical practices they reflect and act on (77). Different data choices will embed different distinctions and perspectives on the task at hand, and will work differently with available tools and technologies. While there are as many ways to assess data as there are data points, practice-based assessment of data begins with three key criteria of representativeness, informativeness, and reliability. ● Representativeness has many (and contested) meanings (81) but can be broadly considered in terms of data’s"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": ") but can be broadly considered in terms of data’s ability to reflect the full diversity of the people or situations they are meant to convey information about. ● Informativeness can be distinguished from representativeness, in terms of how well data are aligned with the desired goal for AI, and how clearly they evince useful learning or patterns for that goal (82). ● Reliability includes noisiness as well as the likelihood of data being collected consistently and accurately in practice (83,84).  Each of these criteria may be balanced differently for different situations and goals. For example, foundation models are built on the principle of an extraordinary volume of data being sufficiently informative and representative to capture diverse general patterns despite lower reliability; in sc"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": " general patterns despite lower reliability; in scientific analysis, data reliability may be paramount; in exploring alternative decisions a smaller set of diverse and informative data may be more valuable than data representing population norms. In addition, practical AI applications may involve multiple entry points for data with distinct balances between these criteria: eg, a diagnostic model in healthcare that begins with a foundation model and is progressively fine-tuned with smaller datasets which may be less representative of human diversity but are more informative for the diagnostic goal.   When exploring AI tools and technologies as options for intervening in a specific process, it is thus essential to assess the appropriateness and fit of different data sources for the task at h"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "nd fit of different data sources for the task at hand. The final key competency of AI Thinking ties these three elements of process, tools, and data for using AI together in the specific contexts where AI is used in practice.  \nAI Thinking: A framework for rethinking artificial intelligence in practice \n 15 3.5 Context: Shaping AI use, benefits, and risks in situated practice Using AI is, at its heart, about making use of computation to help us learn something, produce something, or do something in the world. As we have seen, the processes in which we bring AI to bear, the tools and technologies we choose to use, and the data sources we draw from each carry with them pieces of a broader picture in which AI use is only one element moving between different goals, actors, and perspectives. Wh"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "ween different goals, actors, and perspectives. When this broader picture is ignored, or left as a separate concern from the technical design and use of AI systems, those systems inevitably miss the mark (85) and may create substantive harm (86). Recognising the contexts in which AI use is always situated, and understanding the implications of those contexts for AI’s effectiveness, benefits, and risks, is thus the fifth key competency of AI Thinking.  There are, of course, many contextual aspects that bear on AI use, and analysing the sociotechnical assemblages of AI in practice is the subject of a growing research literature [cf (87–89)]. Throughout the design, implementation, and management of bringing AI methodologies into practice, a strong foundation for assessing AI contexts begins w"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "rong foundation for assessing AI contexts begins with: ● The stakeholders for the proposed use of AI. This includes those who set the initial goal for AI use to support, as well as those who produce the data used by AI systems and make use in turn of the outputs those systems produce.  ● The rationales these different stakeholders have using the AI system and what it is meant to accomplish. For example, a senior leader may look to AI use for efficiency gains, while a researcher or business analyst looks to the same system to produce a specific insight from the complex data they work with. ● The risks posed by different points of failure or undesired behaviour in the AI system, with respect to these rationales. For example, an LLM hallucination or a spurious relationship from a machine lear"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "ion or a spurious relationship from a machine learning model will have different impacts on an AI user, who may be concerned with accuracy in a specific instance, than on someone affected by an AI-informed decision process, for whom recourse and minimising harm are paramount. ● The measures of success for stakeholders in the AI system. Task-based performance metrics assess how well an AI technology performs its stated role, but may not capture the impact of that technology on someone using it as one piece among many, for example in a decision-making process.  These components of AI context may seem distant from the process of developing and implementing AI tools and technologies, but they are essential to connecting that process to the people and processes in which AI will actually be used"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "le and processes in which AI will actually be used. AI Thinking, as a model for bringing AI methodologies into practice, thus aims to connect the dots between technology, the people who use it, and the purposes it is meant to serve.  \nAI Thinking: A framework for rethinking artificial intelligence in practice \n 16 4. Case study of applying AI Thinking in practice To illustrate the role of the AI Thinking framework in practice, I outline a hypothetical case study of using AI for processing information about health and well-being, an area of practice where the rich tradition of medical ethics highlights gaps in current approaches to AI ethics (90). The case study examines the use of AI methods as part of prioritisation of patients for organ transplants, and illustrates how AI application mig"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "ransplants, and illustrates how AI application might unfold with and without AI Thinking.   4.1 Scenario Prioritising potential recipients for organ donation, and ensuring successful transplantation, are highly time-sensitive and information-intensive processes. The use of rich data sources and AI systems to help tackle these challenges has been growing, and we will take as our first illustration of putting AI Thinking into practice a recent paper using machine learning-based simulations of transplant outcomes to inform decision making (91). We will imagine an extension of this methodology, implemented for prioritising potential transplant recipients in a large health system serving a demographically diverse, primarily rural population. We will consider the practices of the health informat"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "will consider the practices of the health informaticians building the AI system, the healthcare professionals using it to inform transplants, the patients receiving transplants, and the health system administrators assessing service outcomes and quality.  4.2 Without AI Thinking Health informaticians design the AI system to tackle the motivating need directly, and rank potential recipients by outputting a single score based on an overall simulation of post-transplant health outcomes. The score is calculated based on the simulated outcome of the transplant, which is based in turn on a combination of demographic and health data, including records of past healthcare encounters. The AI technology is implemented using a single central deep neural network, which is trained based on a large volum"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "l network, which is trained based on a large volume of data sourced from the implementing health system as well as reference data from a large urban system elsewhere in the country. This is done to provide the greatest amount of data to learn from, in order to improve the model’s robustness and sensitivity to variation. The model’s performance is measured by its fidelity to past transplantation decisions.  Most healthcare professionals take the output score of the AI system and combine it with other key information, such as specific relevant lab results, distance to the patient, and size of the organ relative to the patient, to decide who to call for transplant. Some professionals mistrust the AI system and manually review health data to make recommendations, frequently arriving at differe"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "ke recommendations, frequently arriving at different priority scores than the AI technology. Health system administrators measure success of the process using common service metrics such as 30-day mortality and rehospitalisation rate. A \nAI Thinking: A framework for rethinking artificial intelligence in practice \n 17 periodic audit after a year of using the AI system reveals that patients from urban areas are recommended for transplants at a higher rate than patients from rural areas, and that racial disparities in health outcomes are more pronounced for rural patients. These differences are consistent across professionals who use the AI system, and less pronounced for some who avoid it while being more pronounced for others. Patients and families are left with limited information about wh"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "amilies are left with limited information about why they were not selected for transplant or why outcomes were worse than expected, with minimal transparency into the decision process or recourse to contest it.  4.3 With AI Thinking Re-examining the situation with the AI Thinking framework helps to bridge gaps between understandings and practices in this scenario, and to reduce the racial and geographic disparities observed in health outcomes. Process: Prioritising transplant recipients is the overall goal, but this is implemented in professional practice by combining a number of standard key variables with an overall understanding of a patient’s health status (92). Through discussion between the health informaticians and healthcare professionals, the AI technology is instead designed to t"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "ionals, the AI technology is instead designed to target the more specific scope of assigning a transplant score to the patient’s health status specifically. The other variables used in the ranking process are clearly defined by clinically-motivated rules, and not considered targets for AI intervention. Formulation: The need for transparency and accountability for health system administrators, and for insight to support clinical discretion for healthcare professionals, mean that rather than a single atomic score the task is formulated as simulating multiple component health outcomes, represented as clinically-motivated categories or continuous scores. These component outcomes are then transparently combined to an overall score by an adjustable formula. This system is trained based on the fi"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "le formula. This system is trained based on the final ranking, which combines the AI-produced health score with the other, rule-based variables used by the clinicians, to model its use in practice. Tools and Technologies: Rather than a single, large deep neural network, which imposes significant data and computation requirements for the health system, the component health outcome models are implemented using a mixture of Support Vector Machines, random forests, and smaller, targeted neural network models, with significantly lower technical requirements and greater opportunity for accountability and explainability in the modelling process. Data: A review of the available data, motivated by healthcare professionals’ observations, shows that a significant fraction of rural patients accessing "
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": " significant fraction of rural patients accessing the health system do so only through visits to the emergency department (93), and that structural racial bias emerging from the system’s historical context has produced unequal delivery of services to some racial groups. To reduce the structural bias embedded into the AI system, the interdisciplinary team works together to avoid methods that assume data are race neutral (94) and minimise reliance on data that are systematically missing for rural patients. \nAI Thinking: A framework for rethinking artificial intelligence in practice \n 18 Context: Input from all stakeholders—-informaticians, healthcare professionals, health system administrators, and patients—-is incorporated throughout the application of all other competencies. System perform"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "lication of all other competencies. System performance and outcomes are measured by monitoring mortality and rehospitalisation rates for transplant patients and explicitly measuring population disparities. The additional transparency created in the system by a process-oriented approach enables more proactive and constructive dialogue with patients and families who are affected by the system decisions.  4.4 Summary In this hypothetical scenario, the AI Thinking framework provides tangible opportunities to bring together the interdisciplinary perspectives and experiences in the AI team in question. The decisions and considerations represented in the AI Thinking model clearly identify points of reflection and intervention to bridge disciplinary divides, and help to capture risks of inequitabl"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "y divides, and help to capture risks of inequitable outcomes from the system that were missed when not joining up stakeholders and disciplines. While this example might well play out differently in practice, it helps to illustrate the role of AI Thinking in working across disciplinary differences to bring AI into more thoughtful interdisciplinary use in practice.   5. AI Thinking in the broader AI discourse Ongoing rapid transformations in AI have led to many different ways of framing important questions: what AI can (and should) do and why, who should use AI and how, and what is required to achieve real benefits with AI while managing its risks. This section situates AI Thinking within this broader discourse, with respect to three major reference points: AI literacy, end-to-end approaches"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "ference points: AI literacy, end-to-end approaches to AI technologies, and ‘AI-driven’ innovation.  5.1 AI literacy and AI Thinking The need for broader AI literacy is well-recognised, and strategies for developing AI literacy in students, professionals, and the public are rapidly evolving (95–97). AI Thinking, as a model for framing AI use in practice, complements current AI literacy efforts in two ways. AI literacy discussions focus primarily on formal education, whether in schools or higher education (98–100); AI Thinking provides a model to guide training and self-learning in professional and practice-based settings. AI literacy also focuses primarily on understanding how AI works and what its implications are, either from others’ use or from one’s own use of AI tools produced by other"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "r from one’s own use of AI tools produced by others (97,101); AI Thinking describes the competencies required to use AI methodologies as one piece among many in the context of broader evidence-based practice. AI Thinking may be \nAI Thinking: A framework for rethinking artificial intelligence in practice \n 19 considered as an element of AI literacy, at least for practice-based audiences, but its focus on AI practice and its methodological, context-based perspective is important to distinguish from more general-purpose AI literacy efforts.  5.2 A process model vs “end-to-end” innovation As fifteen years of deep learning advances have made more complex tasks more achievable for machine learning, the movement towards “end-to-end” AI approaches has continued to pick up steam. End-to-end AI trea"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "has continued to pick up steam. End-to-end AI treats a sequence of related tasks, each of which may be simple or difficult on its own, as a single, highly complex problem, such as in drug discovery (102), speech recognition (103), or even autonomous driving (104); this allows (it is argued) for greater innovation and discovery of unanticipated relationships and approaches to solving difficult problems. AI Thinking, as a process-oriented approach involving decomposing goals into multiple specific points for AI intervention, seems at first to run counter to the end-to-end model. However, a process-based orientation does not necessitate a process-based intervention: with appropriate AI tools and technologies and supporting data sources available, an end-to-end approach may well be supported u"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "le, an end-to-end approach may well be supported under the AI Thinking model. What AI Thinking provides is a clearer framework for identifying whether an end-to-end or a step-by-step approach is most appropriate to the specific goal, tools, data, and context at hand. Process-based AI Thinking can also help to identify clearer points for evaluation, control, and risk mitigation in complex AI systems, helping to tackle some of the longstanding problems in end-to-end approaches (105).  5.3 AI Thinking or AI doing? Being AI-informed rather than AI-driven The idea of “AI-driven” innovation has gained common currency in recent years, in diverse settings including policy (106), science (107), and business (51). While the meaning of the term varies in practice, an “AI-driven” framing creates a per"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": " in practice, an “AI-driven” framing creates a persistent narrative of innovation and discovery being led directly by the use of AI technologies, as opposed to AI supporting the process as one tool among many. The framing of “AI Thinking,” rather than a more action-oriented “AI doing,” emphasises the role of AI as a support tool, and reflects the purpose of the model to describe the design considerations and decisions made in setting up the use of AI, before that informed use then leads to specific action. AI Thinking is thus more oriented towards AI-informed design and use, in which AI functions as part of a larger sociotechnical system (108).   \nAI Thinking: A framework for rethinking artificial intelligence in practice \n 20 6. Conclusion AI methodologies are transforming the way we work"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": " AI methodologies are transforming the way we work with information. A wide range of disciplines study, use, and develop AI and its transformations, but the different perspectives on AI and its use adopted by these disciplines are often in conflict. New approaches are needed to effectively combine the practical insights and analytic methods offered by different disciplines, and to build more holistic interdisciplinary practices informing AI applications in context.  In this article, I have proposed a novel conceptual framework called AI Thinking, which models key decisions and considerations to help bridge disciplinary perspectives on AI in practice. Just as statistical thinking transformed our use and understanding of growing information in the nineteenth and twentieth centuries (109), so"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "n the nineteenth and twentieth centuries (109), so AI Thinking can help transform information processing in the twenty-first century. I have presented a competency-based model of AI Thinking, representing the key practices involved in applying AI in context. These five competencies—orienting AI use in process, formulating AI implementations, assessing the affordances of AI tools and technologies, selecting appropriate data sources, and situating AI use in context—provide a starting point for rethinking AI skills and training to facilitate more interdisciplinary application of AI, as well as bridging disciplinary perspectives on how AI systems can be better designed, assessed, and managed.  As AI Thinking is brought more systematically into practice, and AI use continues to become more inte"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "practice, and AI use continues to become more interdisciplinary in nature, the initial competencies presented here will be further tested and expanded. Wider empirical study of AI practices and needs will be essential to further develop the AI Thinking model and to keep it aligned with evolving AI practice. As the AI discourse continues to grow and change, the relationships between AI Thinking, AI literacy, and other key framings in AI innovation and practice outlined here will further develop, enabling new connections and comparisons to inform richer conceptualisations of AI in practice. Strengthening dialogue and collaboration on AI practices across disciplines is vital to achieving practical benefits from AI use while reducing and managing its risks. By bridging conceptual and disciplin"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "ng its risks. By bridging conceptual and disciplinary divides in AI research and practice, the AI Thinking framework aims to help shape a more purpose-driven, effective, and manageable future of AI in practice.    \nAI Thinking: A framework for rethinking artificial intelligence in practice \n 21 Acknowledgments I am deeply grateful to Anna Newman-Griffis and Susan Oman for invaluable discussions of language and framing and provocations to the framework which have helped shape the arguments presented here, and to Xin Zhao, Andrew M. Cox, Michael Thewall, Stephen Pinfield, and Ben Steyn for discussions and feedback on early drafts of this article. Many thanks to the organisers and attendees of WOEPSR 2023, and to the teams, Steering Groups, and partners of the GRAIL and FRAIM projects for wid"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "d partners of the GRAIL and FRAIM projects for wider discussions on key ideas.  The author declares that this work was completed in the absence of competing interests. Funding The research in this article was supported in part by the Framing Responsible AI Implementation and Management (FRAIM) project, funded by the Bridging Responsible AI Divides (BRAID) programme of the UKRI Arts and Humanities Research Council (AHRC; award AH/Z505596/1). It was also supported as part of the GRAIL project of the Research on Research Institute (RoRI). RoRI’s second phase (2023–2027) is funded by an international consortium of partners, including: Australian Research Council (ARC); Canadian Institutes of Health Research (CIHR); Digital Science; Dutch Research Council (NWO); Gordon and Betty Moore Foundatio"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "ch Council (NWO); Gordon and Betty Moore Foundation [Grant number GBMF12312; DOI 10.37807/GBMF12312]; King Baudouin Foundation; La Caixa Foundation; Leiden University; Luxembourg National Research Fund (FNR); Michael Smith Health Research BC; National Research Foundation of South Africa; Novo Nordisk Foundation [Grant number NNF23SA0083996]; Research England (part of UK Research and Innovation); Social Sciences and Humanities Research Council of Canada (SSHRC); Swiss National Science Foundation (SNSF); University College London (UCL); Volkswagen Foundation; and Wellcome Trust [Grant number 228086/Z/23/Z]. Sincere thanks to all our partners for their engagement and support. Responsibility for the content of RoRI-supported outputs lies with the authors and RoRI CIC. The views expressed in th"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "he authors and RoRI CIC. The views expressed in this article do not necessarily reflect those of RoRI’s partners, the BRAID programme, or AHRC. RoRI is committed to open research as an important enabler of our mission, as set out in our Open Research Policy. Any errors or omissions remain our own.    References 1. Kulkov I, Kulkova J, Rohrbeck R, Menvielle L, Kaartemo V, Makkonen H. Artificial intelligence - driven sustainable development: Examining organizational, technical, and processing approaches to achieving global goals. Sustain Dev. 2024;32(3):2253–67. 2. Makridakis S. The forthcoming Artificial Intelligence (AI) revolution: Its impact on society and firms. Futures. 2017 Jun 1;90:46–60. \nAI Thinking: A framework for rethinking artificial intelligence in practice \n 22 3. Schwab K. T"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "icial intelligence in practice \n 22 3. Schwab K. The Fourth Industrial Revolution. Crown; 2017. 194 p. 4. Bommasani R, Hudson DA, Adeli E, Altman R, Arora S, von Arx S, et al. On the Opportunities and Risks of Foundation Models [Internet]. arXiv; 2022 [cited 2023 Dec 26]. Available from: http://arxiv.org/abs/2108.07258 5. Jarrahi MH, Lutz C, Boyd K, Oesterlund C, Willis M. Artificial intelligence in the work context. J Assoc Inf Sci Technol. 2023;74(3):303–10. 6. Sartori L, Theodorou A. A sociotechnical perspective for the future of AI: narratives, inequalities, and human control. Ethics Inf Technol. 2022 Jan 24;24(1):4. 7. Dwivedi YK, Hughes L, Ismagilova E, Aarts G, Coombs C, Crick T, et al. Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunitie"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": " perspectives on emerging challenges, opportunities, and agenda for research, practice and policy. Int J Inf Manag. 2021 Apr 1;57:101994. 8. Jan Z, Ahamed F, Mayer W, Patel N, Grossmann G, Stumptner M, et al. Artificial intelligence for industry 4.0: Systematic review of applications, challenges, and opportunities. Expert Syst Appl. 2023 Apr 15;216:119456. 9. Kusters R, Misevic D, Berry H, Cully A, Le Cunff Y, Dandoy L, et al. Interdisciplinary Research in Artificial Intelligence: Challenges and Opportunities. Front Big Data [Internet]. 2020 Nov 23 [cited 2024 Aug 22];3. Available from: https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2020.577974/full 10. Mazarakis A, Bernhard-Skala C, Braun M, Peters I. What is critical for human-centered AI at work? – Toward an interd"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": " for human-centered AI at work? – Toward an interdisciplinary theory. Front Artif Intell [Internet]. 2023 Oct 27 [cited 2024 Aug 22];6. Available from: https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2023.1257057/full 11. Newman-Griffis D, Lehman JF, Rosé C, Hochheiser H. Translational NLP: A New Paradigm and General Principles for Natural Language Processing Research. In: Toutanova K, Rumshisky A, Zettlemoyer L, Hakkani-Tur D, Beltagy I, Bethard S, et al., editors. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies [Internet]. Online: Association for Computational Linguistics; 2021 [cited 2024 Aug 22]. p. 4125–38. Available from: https://aclanthology.org/2021.naac"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "Available from: https://aclanthology.org/2021.naacl-main.325 12. Chui M, Manyika J, Miremadi M. What AI can and can’t do (yet) for your business. McKinsey Quarterly [Internet]. 2018 Jan 11 [cited 2024 Jul 29]; Available from: https://www.mckinsey.com/capabilities/quantumblack/our-insights/what-ai-can-and-cant-do-yet-for-your-business 13. Kanbach DK, Heiduk L, Blueher G, Schreiter M, Lahmann A. The GenAI is out of the bottle: generative artificial intelligence from a business model innovation perspective. Rev Manag Sci. 2024 Apr 1;18(4):1189–220. 14. Mariani MM, Machado I, Magrelli V, Dwivedi YK. Artificial intelligence in innovation research: A systematic review, conceptual framework, and future research directions. Technovation. 2023 Apr 1;122:102623. 15. Papadakis T, Christou IT, Ipektsi"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": ";122:102623. 15. Papadakis T, Christou IT, Ipektsidis C, Soldatos J, Amicone A. Explainable and transparent artificial intelligence for public policymaking. Data Policy. 2024 Jan;6:e10. 16. The Royal Society. Science in the age of AI [Internet]. 2023 [cited 2024 Jul 29]. Available \nAI Thinking: A framework for rethinking artificial intelligence in practice \n 23 from: https://royalsociety.org/news-resources/projects/science-in-the-age-of-ai/ 17. Valle-Cruz D, Criado JI, Sandoval-Almazán R, Ruvalcaba-Gomez EA. Assessing the public policy-cycle framework in the age of artificial intelligence: From agenda-setting to policy evaluation. Gov Inf Q. 2020 Oct 1;37(4):101509. 18. Wang H, Fu T, Du Y, Gao W, Huang K, Liu Z, et al. Scientific discovery in the age of artificial intelligence. Nature. 202"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "in the age of artificial intelligence. Nature. 2023 Aug;620(7972):47–60. 19. Birhane A, Kasirzadeh A, Leslie D, Wachter S. Science in the age of large language models | Nature Reviews Physics. Nat Rev Phys. 2023 May;5(5):277–80. 20. Grossmann I, Feinberg M, Parker DC, Christakis NA, Tetlock PE, Cunningham WA. AI and the transformation of social science research. Science. 2023 Jun 16;380(6650):1108–9. 21. Bartneck C, Lütge C, Wagner A, Welsh S. An Introduction to Ethics in Robotics and AI [Internet]. Springer Nature; 2021 [cited 2024 Jul 29]. Available from: https://library.oapen.org/handle/20.500.12657/41303 22. European Commission. A definition of Artificial Intelligence: main capabilities and scientific disciplines [Internet]. 2018 Dec [cited 2024 Jul 30]. Available from: https://digital"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "ited 2024 Jul 30]. Available from: https://digital-strategy.ec.europa.eu/en/library/definition-artificial-intelligence-main-capabilities-and-scientific-disciplines 23. Google. Google AI. [cited 2024 Aug 1]. Google Responsible AI Practices. Available from: https://ai.google/responsibility/responsible-ai-practices/ 24. Köves A, Feher K, Vicsek L, Fischer M. Entangled AI: artificial intelligence that serves the future. AI Soc [Internet]. 2024 Aug 9 [cited 2024 Aug 16]; Available from: https://doi.org/10.1007/s00146-024-02037-4 25. Arnold RD, Wade JP. A Definition of Systems Thinking: A Systems Approach. Procedia Comput Sci. 2015 Jan 1;44:669–78. 26. Amann J, Blasimme A, Vayena E, Frey D, Madai VI, the Precise4Q consortium. Explainability for artificial intelligence in healthcare: a multidisci"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "rtificial intelligence in healthcare: a multidisciplinary perspective. BMC Med Inform Decis Mak. 2020 Nov 30;20(1):310. 27. Bates J, Kennedy H, Medina Perea I, Oman S, Pinney L. Socially meaningful transparency in data-based systems: reflections and proposals from practice. J Doc. 2023 Jan 1;80(1):54–72. 28. Devlin J, Chang MW, Lee K, Toutanova K. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In: Burstein J, Doran C, Solorio T, editors. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) [Internet]. Minneapolis, Minnesota: Association for Computational Linguistics; 2019 [cited 2024 Aug 16]. p. 4171–86. Available from: https://aclan"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "Aug 16]. p. 4171–86. Available from: https://aclanthology.org/N19-1423 29. Krizhevsky A, Sutskever I, Hinton GE. ImageNet Classification with Deep Convolutional Neural Networks. In: Advances in Neural Information Processing Systems [Internet]. Curran Associates, Inc.; 2012 [cited 2024 Aug 16]. Available from: https://proceedings.neurips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html \nAI Thinking: A framework for rethinking artificial intelligence in practice \n 24 30. Li J, Lim K, Yang H, Ren Z, Raghavan S, Chen PY, et al. AI Applications through the Whole Life Cycle of Material Discovery. Matter. 2020 Aug 5;3(2):393–432. 31. Liu P, Yuan W, Fu J, Jiang Z, Hayashi H, Neubig G. Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural L"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "ystematic Survey of Prompting Methods in Natural Language Processing. ACM Comput Surv. 2023 Jan 16;55(9):195:1-195:35. 32. Sandbank J, Bataillon G, Nudelman A, Krasnitsky I, Mikulinsky R, Bien L, et al. Validation and real-world clinical application of an artificial intelligence algorithm for breast cancer detection in biopsies. Npj Breast Cancer. 2022 Dec 6;8(1):1–11. 33. Caldwell S, Sweetser P, O’Donnell N, Knight MJ, Aitchison M, Gedeon T, et al. An Agile New Research Framework for Hybrid Human-AI Teaming: Trust, Transparency, and Transferability. ACM Trans Interact Intell Syst. 2022 Jul 26;12(3):17:1-17:36. 34. Pflanzer M, Traylor Z, Lyons JB, Dubljević V, Nam CS. Ethics in human–AI teaming: principles and perspectives. AI Ethics. 2023 Aug 1;3(3):917–35. 35. Kitchin R, Lauriault T. Tow"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "Aug 1;3(3):917–35. 35. Kitchin R, Lauriault T. Towards Critical Data Studies: Charting and Unpacking Data Assemblages and Their Work [Internet]. Rochester, NY; 2014 [cited 2024 Aug 17]. Available from: https://papers.ssrn.com/abstract=2474112 36. Bates J, Lin YW, Goodale P. Data journeys: Capturing the socio-material constitution of data objects and flows. Big Data Soc [Internet]. 2016 Jul 13 [cited 2024 Aug 17]; Available from: https://journals.sagepub.com/doi/full/10.1177/2053951716654502 37. Amoore L. Cloud Ethics: Algorithms and the Attributes of Ourselves and Others. Duke University Press; 2020. 143 p. 38. Crawford K, Calo R. There is a blind spot in AI research. Nature. 2016 Oct;538(7625):311–3. 39. Thylstrup NB, Hansen KB, Flyverbom M, Amoore L. Politics of data reuse in machine lea"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "M, Amoore L. Politics of data reuse in machine learning systems: Theorizing reuse entanglements. Big Data Soc. 2022 Jul 1;9(2):20539517221139785. 40. Stigler SM. Statistics on the Table: The History of Statistical Concepts and Methods. Harvard University Press; 2002. 514 p. 41. Fienberg SE. A Brief History of Statistics in Three and One-Half Chapters: A Review Essay. Daston LJ, Gigerenzer G, Swijtink Z, Porter T, Daston L, Beatty J, et al., editors. Stat Sci. 1992;7(2):208–25. 42. Hahn GJ, Doganaksoy N. The Role of Statistics in Business and Industry. John Wiley & Sons; 2011. 302 p. 43. Wild CJ, Pfannkuch M. Statistical Thinking in Empirical Enquiry. Int Stat Rev. 1999;67(3):223–48. 44. Cox DR, Efron B. Statistical thinking for 21st century scientists. Sci Adv. 2017 Jun 14;3(6):e1700768. 4"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": " scientists. Sci Adv. 2017 Jun 14;3(6):e1700768. 45. Medeiros L, Psaltis D, Lauer TR, Özel F. The Image of the M87 Black Hole Reconstructed with PRIMO. Astrophys J Lett. 2023 Apr;947(1):L7. 46. Jumper J, Evans R, Pritzel A, Green T, Figurnov M, Ronneberger O, et al. Highly accurate protein structure prediction with AlphaFold. Nature. 2021 Aug;596(7873):583–9. \nAI Thinking: A framework for rethinking artificial intelligence in practice \n 25 47. Idnay B, Dreisbach C, Weng C, Schnall R. A systematic review on natural language processing systems for eligibility prescreening in clinical research. J Am Med Inform Assoc. 2022 Jan 1;29(1):197–206. 48. Gefen A, Saint-Raymond L, Venturini T. AI for Digital Humanities and Computational Social Sciences. In: Braunschweig B, Ghallab M, editors. Reflecti"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": ". In: Braunschweig B, Ghallab M, editors. Reflections on Artificial Intelligence for Humanity [Internet]. Cham: Springer International Publishing; 2021 [cited 2024 Jul 29]. p. 191–202. Available from: https://doi.org/10.1007/978-3-030-69128-8_12 49. Cox A. The impact of AI, machine learning, automation and robotics on information professions [Internet]. 2021 May [cited 2024 Jul 30]. Available from: https://www.cilip.org.uk/page/researchreport 50. Jiang F, Jiang Y, Zhi H, Dong Y, Li H, Ma S, et al. Artificial intelligence in healthcare: past, present and future. Stroke Vasc Neurol. 2017 Jun 21;2(4):230–43. 51. Jorzik P, Klein SP, Kanbach DK, Kraus S. AI-driven business model innovation: A systematic review and research agenda. J Bus Res. 2024 Sep 1;182:114764. 52. Bessant KC, MacPherson ED."
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "4 Sep 1;182:114764. 52. Bessant KC, MacPherson ED. Thoughts on the Origins, Concepts, and Pedagogy of Statistics as a “Separate Discipline”. Am Stat. 2002 Feb 1;56(1):22–8. 53. Burnette DM. The Renewal of Competency-Based Education: A Review of the Literature. J Contin High Educ. 2016 May 3;64(2):84–93. 54. Alliance for Data Science Professionals. The Alliance for Data Science Professionals Certification Guidance and Process: Advanced Data Science Professional [Internet]. 2022 Jun [cited 2024 Jul 30]. Available from: https://alliancefordatascienceprofessionals.co.uk/guidelines/latest 55. Kabalisa R, Altmann J. AI Technologies and Motives for AI Adoption by Countries and Firms: A Systematic Literature Review. In: Tserpes K, Altmann J, Bañares JÁ, Agmon Ben-Yehuda O, Djemame K, Stankovski V,"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "s JÁ, Agmon Ben-Yehuda O, Djemame K, Stankovski V, et al., editors. Economics of Grids, Clouds, Systems, and Services. Cham: Springer International Publishing; 2021. p. 39–51. 56. Rabiner L, Juang BH. Historical Perspective of the Field of ASR/NLU. In: Benesty J, Sondhi MM, Huang YA, editors. Springer Handbook of Speech Processing [Internet]. Berlin, Heidelberg: Springer; 2008 [cited 2024 Jul 30]. p. 521–38. Available from: https://doi.org/10.1007/978-3-540-49127-9_26 57. Savage N. How AI is improving cancer diagnostics. Nature. 2020 Mar 25;579(7800):S14–6. 58. Minsky ML. Problems of formulation for artificial intelligence. In: Proceedings of a Symposium on Mathematical Problems in Biology. American Mathematical Society Providence, RI; 1962. p. 35–46. 59. Binbasioglu M, Jarke M. Knowledge-"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": ". p. 35–46. 59. Binbasioglu M, Jarke M. Knowledge-Based Formulation of Linear Planning Models. In: Sol HG, Takkenberg CATh, De Vries Robbé PF, editors. Expert Systems and Artificial Intelligence in Decision Support Systems: Proceedings of the Second Mini Euroconference, Lunteren, The Netherlands, 17–20 November 1985 [Internet]. Dordrecht: Springer Netherlands; 1987 [cited 2024 Aug 22]. p. 113–36. Available from: https://doi.org/10.1007/978-94-009-3805-2_7 60. Cramer EJ, Dennis, Jr. JE, Frank PD, Lewis RM, Shubin GR. Problem Formulation for \nAI Thinking: A framework for rethinking artificial intelligence in practice \n 26 Multidisciplinary Optimization. SIAM J Optim. 1994 Nov;4(4):754–76. 61. Blagec K, Barbosa-Silva A, Ott S, Samwald M. A curated, ontology-based, large-scale knowledge graph "
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "ated, ontology-based, large-scale knowledge graph of artificial intelligence tasks and benchmarks. Sci Data. 2022 Jun 17;9(1):322. 62. Trewartha A, Walker N, Huo H, Lee S, Cruse K, Dagdelen J, et al. Quantifying the advantage of domain-specific pre-training on named entity recognition tasks in materials science. Patterns [Internet]. 2022 Apr 8 [cited 2024 Aug 22];3(4). Available from: https://www.cell.com/patterns/abstract/S2666-3899(22)00073-3 63. Amironesei R, Denton E, Hanna A. Notes on Problem Formulation in Machine Learning. IEEE Technol Soc Mag. 2021 Sep;40(3):80–3. 64. Newman-Griffis D, Rauchberg JS, Alharbi R, Hickman L, Hochheiser H. Definition drives design: Disability models and mechanisms of bias in AI technologies. First Monday [Internet]. 2023 Feb 7 [cited 2024 Aug 12]; Avail"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": " [Internet]. 2023 Feb 7 [cited 2024 Aug 12]; Available from: https://firstmonday.org/ojs/index.php/fm/article/view/12903 65. Passi S, Barocas S. Problem Formulation and Fairness. In: Proceedings of the Conference on Fairness, Accountability, and Transparency [Internet]. New York, NY, USA: Association for Computing Machinery; 2019 [cited 2024 Aug 22]. p. 39–48. (FAT* ’19). Available from: https://dl.acm.org/doi/10.1145/3287560.3287567 66. Cheng L, Subrahmanian E, Westerberg AW. Design and planning under uncertainty: issues on problem formulation and solution. Comput Chem Eng. 2003 Jun 15;27(6):781–801. 67. Newman-Griffis D, Fosler-Lussier E. Automated Coding of Under-Studied Medical Concept Domains: Linking Physical Activity Reports to the International Classification of Functioning, Disabi"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "nternational Classification of Functioning, Disability, and Health. Front Digit Health [Internet]. 2021 Mar 10 [cited 2024 Aug 23];3. Available from: https://www.frontiersin.org/journals/digital-health/articles/10.3389/fdgth.2021.620828/full 68. Isagah T, Ben Dhaou SI. Problem Formulation and Use Case Identification of AI in Government: Results from the Literature Review. In: Proceedings of the 24th Annual International Conference on Digital Government Research [Internet]. New York, NY, USA: Association for Computing Machinery; 2023 [cited 2024 Aug 22]. p. 434–9. (dg.o ’23). Available from: https://doi.org/10.1145/3598469.3598518 69. Ciarli T, Kenney M, Massini S, Piscitello L. Digital technologies, innovation, and skills: Emerging trajectories and challenges. Res Policy. 2021 Sep 1;50(7):"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "ries and challenges. Res Policy. 2021 Sep 1;50(7):104289. 70. Gaver WW. Technology affordances. In: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems [Internet]. New York, NY, USA: Association for Computing Machinery; 1991 [cited 2024 Aug 22]. p. 79–84. (CHI ’91). Available from: https://dl.acm.org/doi/10.1145/108844.108856 71. Strauss LM, Klein AZ, Scornavacca E. Adopting emerging information technology: A new affordances process framework. Int J Inf Manag. 2024 Jun 1;76:102772. 72. Chai J, Zeng H, Li A, Ngai EWT. Deep learning in computer vision: A critical review of emerging techniques and application scenarios. Mach Learn Appl. 2021 Dec 15;6:100134. 73. Roman-Naranjo P, Parra-Perez AM, Lopez-Escamez JA. A systematic review on machine learning approaches in the "
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "atic review on machine learning approaches in the diagnosis and prognosis of rare genetic diseases. J Biomed \nAI Thinking: A framework for rethinking artificial intelligence in practice \n 27 Inform. 2023 Jul 1;143:104429. 74. Tran KA, Kondrashova O, Bradley A, Williams ED, Pearson JV, Waddell N. Deep learning in cancer diagnosis, prognosis and treatment selection. Genome Med. 2021 Sep 27;13(1):152. 75. Li Y, Wang S, Ding H, Chen H. Large Language Models in Finance: A Survey. In: Proceedings of the Fourth ACM International Conference on AI in Finance [Internet]. New York, NY, USA: Association for Computing Machinery; 2023 [cited 2024 Jul 30]. p. 374–82. (ICAIF ’23). Available from: https://dl.acm.org/doi/10.1145/3604237.3626869 76. Li S, Wang G, Luo Y. Tone of language, financial disclosure"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "g G, Luo Y. Tone of language, financial disclosure, and earnings management: a textual analysis of form 20-F. Financ Innov. 2022 May 10;8(1):43. 77. Dalmer N, Newman-Griffis D, Ibrahimi M, Jia X, Allhutter D, Amelang K, et al. Configuring Data Subjects. In: Jarke J, Bates J, editors. Dialogues in Data Power: Shifting Response-abilities in a Datafied World. 2024. p. 10–30. 78. Gitelman L, editor. ‘Raw Data’ Is an Oxymoron [Internet]. The MIT Press; 2013 [cited 2024 Aug 1]. Available from: https://direct.mit.edu/books/book/3992/Raw-Data-Is-an-Oxymoron 79. Haraway DJ. 103875. 1991 [cited 2024 Aug 22]. SIMIANS, CYBORGS, AND WOMEN: THE REINVENTION OF NATURE. Available from: https://repository.library.georgetown.edu/handle/10822/545138 80. Jordan MI, Mitchell TM. Machine learning: Trends, perspe"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": " MI, Mitchell TM. Machine learning: Trends, perspectives, and prospects. Science. 2015 Jul 17;349(6245):255–60. 81. Chasalow K, Levy K. Representativeness in Statistics, Politics, and Machine Learning. In: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency [Internet]. New York, NY, USA: Association for Computing Machinery; 2021 [cited 2024 Aug 1]. p. 77–89. (FAccT ’21). Available from: https://dl.acm.org/doi/10.1145/3442188.3445872 82. Du B, Wang Z, Zhang L, Zhang L, Liu W, Shen J, et al. Exploring Representativeness and Informativeness for Active Learning. IEEE Trans Cybern. 2017 Jan;47(1):14–26. 83. Agmon N, Ahituv N. Assessing Data Reliability in an Information System. J Manag Inf Syst. 1987;4(2):34–44. 84. Gupta S, Gupta A. Dealing with Noise Problem i"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "84. Gupta S, Gupta A. Dealing with Noise Problem in Machine Learning Data-sets: A Systematic Review. Procedia Comput Sci. 2019 Jan 1;161:466–74. 85. Eckert C, Isaksson O, Eckert C, Coeckelbergh M, Hagström MH. Data Fairy in Engineering Land: The Magic of Data Analysis as a Sociotechnical Process in Engineering Companies. J Mech Des [Internet]. 2020 Aug 4 [cited 2024 Aug 4];142(121402). Available from: https://doi.org/10.1115/1.4047813 86. Eubanks V. Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor. St. Martin’s Publishing Group; 2018. 273 p. 87. Ehsan U, Saha K, De Choudhury M, Riedl MO. Charting the Sociotechnical Gap in Explainable AI: A Framework to Address the Gap in XAI. Proc ACM Hum-Comput Interact. 2023 Apr 16;7(CSCW1):34:1-34:32. \nAI Thinking: A frame"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": " Apr 16;7(CSCW1):34:1-34:32. \nAI Thinking: A framework for rethinking artificial intelligence in practice \n 28 88. Xu W, Dainoff MJ, Ge L, Gao Z. Transitioning to Human Interaction with AI Systems: New Challenges and Opportunities for HCI Professionals to Enable Human-Centered AI. Int J Human–Computer Interact. 2023 Feb 7;39(3):494–518. 89. Yu X, Xu S, Ashton M. Antecedents and outcomes of artificial intelligence adoption and application in the workplace: the socio-technical system theory perspective. Inf Technol People. 2022 Jan 1;36(1):454–74. 90. Mittelstadt B. Principles alone cannot guarantee ethical AI. Nat Mach Intell. 2019 Nov;1(11):501–7. 91. Yoo D, Divard G, Raynaud M, Cohen A, Mone TD, Rosenthal JT, et al. A Machine Learning-Driven Virtual Biopsy System For Kidney Transplant Pat"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "en Virtual Biopsy System For Kidney Transplant Patients. Nat Commun. 2024 Jan 16;15(1):554. 92. UNOS [Internet]. [cited 2024 Aug 23]. How we match organs. Available from: https://unos.org/transplant/how-we-match-organs/ 93. Haggerty JL, Roberge D, Pineault R, Larouche D, Touati N. Features of Primary Healthcare Clinics Associated with Patients’ Utilization of Emergency Rooms: Urban–Rural Differences. Healthc Policy. 2007 Nov;3(2):72–85. 94. Obermeyer Z, Powers B, Vogeli C, Mullainathan S. Dissecting racial bias in an algorithm used to manage the health of populations. Science. 2019 Oct 25;366(6464):447–53. 95. Benton P. AI Literacy: A Primary Good. In: Pillay A, Jembere E, J. Gerber A, editors. Artificial Intelligence Research. Cham: Springer Nature Switzerland; 2023. p. 31–43. 96. Domíngu"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "er Nature Switzerland; 2023. p. 31–43. 96. Domínguez Figaredo D, Stoyanovich J. Responsible AI literacy: A stakeholder-first approach. Big Data Soc. 2023 Jul 1;10(2):20539517231219958. 97. Ng DTK, Leung JKL, Chu SKW, Qiao MS. Conceptualizing AI literacy: An exploratory review. Comput Educ Artif Intell. 2021 Jan 1;2:100041. 98. Casal-Otero L, Catala A, Fernández-Morante C, Taboada M, Cebreiro B, Barro S. AI literacy in K-12: a systematic literature review. Int J STEM Educ. 2023 Apr 19;10(1):29. 99. Kreinsen M, Schulz S. Towards the Triad of Digital Literacy, Data Literacy and AI Literacy in Teacher Education – A Discussion in Light of the Accessibility of Novel Generative AI [Internet]. OSF; 2023 [cited 2024 Mar 31]. Available from: https://osf.io/xguzk 100. Southworth J, Migliaccio K, Glov"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "osf.io/xguzk 100. Southworth J, Migliaccio K, Glover J, Glover J, Reed D, McCarty C, et al. Developing a model for AI Across the curriculum: Transforming the higher education landscape via innovation in AI literacy. Comput Educ Artif Intell. 2023 Jan 1;4:100127. 101. Long D, Magerko B. What is AI Literacy? Competencies and Design Considerations. In: Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems [Internet]. New York, NY, USA: Association for Computing Machinery; 2020 [cited 2024 Jul 29]. p. 1–16. (CHI ’20). Available from: https://doi.org/10.1145/3313831.3376727 102. Ekins S, Puhl AC, Zorn KM, Lane TR, Russo DP, Klein JJ, et al. Exploiting machine learning for end-to-end drug discovery and development. Nat Mater. 2019 May;18(5):435–41. 103. Amodei D, Ananthana"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "r. 2019 May;18(5):435–41. 103. Amodei D, Ananthanarayanan S, Anubhai R, Bai J, Battenberg E, Case C, et al. Deep \nAI Thinking: A framework for rethinking artificial intelligence in practice \n 29 Speech 2 : End-to-End Speech Recognition in English and Mandarin. In: Proceedings of The 33rd International Conference on Machine Learning [Internet]. PMLR; 2016 [cited 2024 Aug 5]. p. 173–82. Available from: https://proceedings.mlr.press/v48/amodei16.html 104. Chib PS, Singh P. Recent Advancements in End-to-End Autonomous Driving Using Deep Learning: A Survey. IEEE Trans Intell Veh. 2024 Jan;9(1):103–18. 105. Glasmachers T. Limits of End-to-End Learning. In: Proceedings of the Ninth Asian Conference on Machine Learning [Internet]. PMLR; 2017 [cited 2024 Aug 5]. p. 17–32. Available from: https://pr"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": " 2024 Aug 5]. p. 17–32. Available from: https://proceedings.mlr.press/v77/glasmachers17a.html 106. Paunov C, Planes-Satorra S, Ravelli G. Review of national policy initiatives in support of digital and AI-driven innovation [Internet]. Paris: OECD; 2019 Oct [cited 2024 Aug 5]. Available from: https://www.oecd-ilibrary.org/content/paper/15491174-en 107. Essien A. AI-Driven Innovation: Leveraging Big Data Analytics for Innovation. In: Innovation Analytics [Internet]. WORLD SCIENTIFIC (EUROPE); 2023 [cited 2024 Aug 5]. p. 119–37. Available from: https://www.worldscientific.com/doi/abs/10.1142/9781786349989_0006 108. Böckle M, Kouris I. Design Thinking and AI: A New Frontier for Designing Human-Centered AI Solutions. Des Manag J. 2023;18(1):20–31. 109. Porter TM. The Rise of Statistical Thinkin"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "1. 109. Porter TM. The Rise of Statistical Thinking, 1820–1900 [Internet]. Princeton University Press; 2020 [cited 2023 Jun 7]. Available from: https://books.google.co.uk/books?hl=en&lr=&id=7pPTDwAAQBAJ&oi=fnd&pg=PA193&dq=statistical+thinking+history+of+science&ots=4pTVu8mHWi&sig=pnMXISVYCqbAjxUn3V9QS-ZZU_g&redir_esc=y#v=onepage&q=statistical%20thinking%20history%20of%20science&f=false 110. Tenório K, Romeike R. AI Competencies for non-computer science students in undergraduate education: Towards a competency framework. In: Proceedings of the 23rd Koli Calling International Conference on Computing Education Research [Internet]. New York, NY, USA: Association for Computing Machinery; 2024 [cited 2024 Aug 5]. p. 1–12. (Koli Calling ’23). Available from: https://dl.acm.org/doi/10.1145/3631802"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": "lable from: https://dl.acm.org/doi/10.1145/3631802.3631829 111. Garrett N, Beard N, Fiesler C. More Than ‘If Time Allows’: The Role of Ethics in AI Education. In: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society [Internet]. New York, NY, USA: Association for Computing Machinery; 2020 [cited 2024 Aug 5]. p. 272–8. (AIES ’20). Available from: https://dl.acm.org/doi/10.1145/3375627.3375868 112. Langenkamp M, Yue DN. How Open Source Machine Learning Software Shapes AI. In: Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society [Internet]. New York, NY, USA: Association for Computing Machinery; 2022 [cited 2023 Jun 8]. p. 385–95. (AIES ’22). Available from: https://dl.acm.org/doi/10.1145/3514094.3534167 113. Arun C. AI and the Global South: Designing for Other W"
  },
  {
    "arxiv_id": "2409.12922",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in\n  practice",
    "chunk": " C. AI and the Global South: Designing for Other Worlds [Internet]. Rochester, NY; 2019 [cited 2023 Jun 16]. Available from: https://papers.ssrn.com/abstract=3403010 114. Chubb J, Reed D, Cowling P. Expert views about missing AI narratives: is there an AI story crisis? AI Soc [Internet]. 2022 Aug 25 [cited 2023 Jun 9]; Available from: https://doi.org/10.1007/s00146-022-01548-2 \nAI Thinking: A framework for rethinking artificial intelligence in practice \n 30 115. Bareis J, Katzenbach C. Talking AI into Being: The Narratives and Imaginaries of National AI Strategies and Their Performative Politics. Sci Technol Hum Values. 2022 Sep 1;47(5):855–81.  \n"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "Intersymbolic AI⋆\nInterlinking Symbolic AI and Subsymbolic AI\nAndr´ e Platzer\nKarlsruhe Institute of Technology, Karlsruhe, Germany platzer@kit.edu\nAbstract. This perspective piece calls for the study of the new field\nofIntersymbolic AI , by which we mean the combination of symbolic AI ,\nwhose building blocks have inherent significance/meaning, with subsym-\nbolic AI , whose entirety creates significance/effect despite the fact that\nindividual building blocks escape meaning. Canonical kinds of symbolic\nAI are logic, games and planning. Canonical kinds of subsymbolic AI\nare (un)supervised machine and reinforcement learning. Intersymbolic\nAI interlinks the worlds of symbolic AI with its compositional symbolic\nsignificance and meaning and of subsymbolic AI with its summative sig-\nnificance or "
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "bsymbolic AI with its summative sig-\nnificance or effect to enable culminations of insights from both worlds\nby going between and across symbolic AI insights with subsymbolic AI\ntechniques that are being helped by symbolic AI principles. For example,\nIntersymbolic AI may start with symbolic AI to understand a dynamic\nsystem, continue with subsymbolic AI to learn its control, and end with\nsymbolic AI to safely use the outcome of the learned subsymbolic AI con-\ntroller in the dynamic system. The way Intersymbolic AI combines both\nsymbolic and subsymbolic AI to increase the effectiveness of AI compared\nto either kind of AI alone is likened to the way that the combination of\nboth conscious and subconscious thought increases the effectiveness of\nhuman thought compared to either kind of thought "
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "\nhuman thought compared to either kind of thought alone. Some success-\nful contributions to the Intersymbolic AI paradigm are surveyed here but\nmany more are considered possible by advancing Intersymbolic AI.\nKeywords: Artificial Intelligence ·Symbolic AI ·Subsymbolic AI ·In-\ntersymbolic AI ·Logic ·Verification ·Machine Learning\n1 Introduction\nArtificial Intelligence (AI) has received both waves of significant attention and\nof significant success [89]. AI comes in two flavors: symbolic and subsymbolic AI.\nSymbolic AI [66] is ultimately rooted in symbolic techniques whose individual\nbuilding blocks carry meaning and are, thus, interpretable (at least in princi-\nple although not necessarily at the full required scale) and where individual\nbuilding blocks are typically (de)composed to obtain "
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "lding blocks are typically (de)composed to obtain a whole. Subsymbolic AI is\nultimately rooted in numerical techniques where the individual building blocks\n⋆Funding has been provided by an Alexander von Humboldt Professorship for AI and\nthe Helmholtz KiKIT Kerninformatik / Core Informatics at KIT.arXiv:2406.11563v3  [cs.AI]  26 Jul 2024\n2 Andr´ e Platzer\ndo not carry direct meaning (beyond the role they happen to play in the context\nof the computation) and where the result is given indirectly, e.g., by an itera-\ntive optimization procedure fed from training data or experience. Both flavors\nof AI have remarkably different and sometimes quite complementary strengths\nand weaknesses. Their combination in what we call Intersymbolic AI , thus, has\nthe potential to add the strengths of symbolic A"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "s\nthe potential to add the strengths of symbolic AI and of subsymbolic AI while\ncanceling out the weaknesses of symbolic AI and of subsymbolic AI, respectively.\nThe difference is in significance. Symbolic AI studies AI whose building blocks\nand operating principles carry meaning, which makes them compositionally sig-\nnificant in a semiotic sense [18]. Subsymbolic AI studies AI whose building blocks\ndo not carry individual clear meaning and yet whose overall outcome carries\n(phenomenologically) pragmatic significance or effect. Intersymbolic AI inter-\nlinks symbolic AI and subsymbolic AI such that some building blocks and oper-\nating principles carry meaning, while others do not, in ways to achieve overall\noutcomes of increased significance. Just hoping that the respective advantages\nof sym"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": " Just hoping that the respective advantages\nof symbolic and subsymbolic AI are additive while their respective weaknesses\nnaturally cancel each other out is, of course, na¨ ıve. But careful combinations of\nsymbolic and subsymbolic AI fulfill this promise of the best of both worlds.\nOne analogy why Intersymbolic AI combinations of symbolic AI and sub-\nsymbolic AI can be more effective than either part alone is similar to how com-\nbinations of conscious and subconscious thought can increase the effectiveness\nof human thought. Just like symbolic AI, conscious thought seems more sym-\nbolic and can be explained better compared to subconscious thought that seems\nharder to give direct meaning to or explain, even if this is elusive to know [81].\nHumans combine both thought processes effectively, w"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "mans combine both thought processes effectively, which is why such combi-\nnations were proposed as analogies for cognitive architectures [98].\nCanonical examples of symbolic AI include logic [65,64], planning [30], and\ngame play [63,42]. Canonical examples of subsymbolic AI include neural net-\nworks [89] and reinforcement learning [99]. Probabilistic reasoning is a canonical\nexample but either falls on the symbolic AI side of Bayesian inference in Bayesian\nnetworks [72,89] or falls on the subsymbolic AI side of Bayesian statistics and\nMarkov chain Monte Carlo [54]. Due to the fundamentally different character-\nistics, advantages, and disadvantages of many forms of symbolic AI compared\nto many forms of subsymbolic AI, the combination of symbolic AI and subsym-\nbolic AI in intersymbolic AI h"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "olic AI and subsym-\nbolic AI in intersymbolic AI has much to gain if exercised with suitable care.\nThe weights at individual neurons in a neural network are hard to interpret,\nwhile each operator in a logical formula is easily interpreted. Conversely, neural\nnetworks have a simple and powerful mechanism for learning how they best re-\nproduce training data [89] while logical formulas are harder to learn [4] and it\nis, sadly, challenging to integrate logical knowledge into neural networks [102].\nSuccessful combinations contributing to Intersymbolic AI include a range:\n1. combining reinforcement learning with hybrid systems theorem proving and\nproof-based synthesis to enable Safe AI in CPS [25,26,24,78],\nIntersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI 3\n2. combining symbolic pro"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": " AI and Subsymbolic AI 3\n2. combining symbolic proof, first integrals and Darboux polynomials with\neigensystems, sum-of-squares programming and linear programming for in-\nvariant generation of differential equations [96],\n3. combining AlphaZero for reinforcement learning on deep neural networks\nwith theorem proving and nondeterministic programming for loop invariant\nsynthesis [49],\n4. combining neural network path tracking and model-predictive control with\nhybrid systems theorem proving for safe waypoint following vehicles [52],\n5. combining arithmetic simplification heuristics and refinement approxima-\ntions with hybrid systems verification and game theory for control envelope\nsynthesis [44],\n6. combining neural network verification with complete arithmetic linearization\ntechniques and hy"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "omplete arithmetic linearization\ntechniques and hybrid systems theorem proving for safety verification of\nNeural Network Control Systems [100],\n7. combining Graph Neural Networks with symbolic proof to help automate\nhigher-order logic provers [69,10].\nWhile quite different, these uses all share the design principle that symbolic AI\nprinciples are combined with subsymbolic techniques. While the relative propor-\ntion of the use of symbolic AI versus subsymbolic AI in these combinations is\ndifferent, this range shows the potential of Intersymbolic AI for vastly different\npurposes. Two technical connections arise in many of these Intersymbolic AI\nuses, dynamic logic proving ideas [75] arise in combinations 1–6 and ideas from\nthe shielding technique ModelPlex [60] arise in combinations 1 and 4–"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "ique ModelPlex [60] arise in combinations 1 and 4–6. But\nmany other combinations contributing to Intersymbolic AI are conceivable more\nbroadly as well, which is why this perspective piece is meant as a call to the\nscientific community to notice the challenge and contribute to its solution. The\nrecord here is slanted by the author’s prior specific experience with manifesta-\ntions of Intersymbolic AI, but the general phenomenon of Intersymbolic AI is\nby no means meant to be understood exclusive to the existing combinations.\nThis paper is not the first calling for distinctions and combinations of dif-\nferent kinds of AI (good old fashioned AI [41], neuro-symbolic AI and related\nnotions [28,12,46], considering such combinations the most important AI chal-\nlenges [6]) and it certainly will not "
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "nt AI chal-\nlenges [6]) and it certainly will not be the last.1The author’s hope is to contribute\nto the state of understanding by better identifying the logical root cause for the\ndivide between symbolic versus subsymbolic AI, calling for a pervasive and sig-\nnificantly more broadly interpreted combination in the form of Intersymbolic AI,\nand inspiring by showcasing a wide range of successful novel styles of achieving\nIntersymbolic AI with entirely different flavors of combinations.\nThe plan for this perspective piece is to characterize typical symbolic AI in\nSection 2, subsymbolic AI in Section 3, then contrast and compare their strengths\nleading to the call for intersymbolic AI in Section 4. This paper will then make\n1The difference between symbolic AI versus subsymbolic AI has also som"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "een symbolic AI versus subsymbolic AI has also sometimes been\nsought in the contrast of traditional versus nontraditional AI, or in rule-based versus\nlearning-based AI, or in good old fashioned AI [41] versus neural networks. Symbolic\nAI versus subsymbolic AI better characterizes the essential source of the difference.\n4 Andr´ e Platzer\na short foray into one particularly compelling application area for mixing inter-\nsymbolic AI in cyber-physical systems in Section 5, and give a short overview\nof successful intersymbolic AI uses in Section 6 to demonstrate its wide range\nbefore concluding in Section 7.\n2 Symbolic AI\nSymbolic AI was pioneered already in its purest form of AI by deductive proof\nin the first AI tools, the Logic Theory Machine and General Problem Solver, in\nthe 1950s by Allen "
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "and General Problem Solver, in\nthe 1950s by Allen Newell and Herbert Simon [65,64] based on traditional logic\n[23,33,31]. Even the origin of logic such as in Aristotle’s syllogisms, Frege’s Be-\ngriffsschrift [23], and Gentzen’s natural deduction [31,82] were already meant to\nrigorously capture the human process of argumentative thought and reasoning.\nSince every symbol and operator in logic carries intrinsic meaning, and since de-\nductive proof preserves meaning, this pure style of symbolic AI has the clearest\nsignificance (in the semiotic sense [18,97] that its parts and outcomes are mean-\ningful), the clearest interpretations (originating from the compositions of the\nmeanings of the individual symbols and deductive proof rules), and the clearest\nexplanations (the justification in the for"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "learest\nexplanations (the justification in the form of the resulting deductive proof).\nLogical inferences such as the reasoning captured in the logical formula\n(rain→wet)∧ ¬wet→ ¬rain (1)\ncan be justified by deductive proof and carry clear meaning independent from\nthe question what exactly one means with raining and what exactly one means\nby a wet road. The inference stands and can be justified just from the logical\nstructure of the formula (1) and the meaning of its logical connectives ∧,¬,→.\nIn an application, formula (1) can be read to say that if an AI assumes that\nthe road is wet when it is raining, but the AI observes that the road is not wet,\nthen the AI concludes that it cannot be raining. This is an undeniably correct\ninference. Of course, the observation from a visual image that "
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": " course, the observation from a visual image that the road is, indeed,\nnot wet is an entirely different matter, and better handled using subsymbolic AI\nin the form of neural networks for image classification. The main mode of using\nlogic is by deductive proof [22,9,88,74] or satisfiability modulo theory solving\n[67], but some uses include symbolic learning [4,17].\nSymbolic AI by planning [21,30] and game play [63] are also such that the\nindividual actions in a plan or game have clear symbolic and meaningful char-\nacter, where it is clear what these building blocks mean and what the effect of\ntaking those actions is. Even in games where the actual outcome of an action\npartially depends on chance, the game itself is still described in a meaningful\nway, which is ultimately the prerequisite to"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "ngful\nway, which is ultimately the prerequisite to solving it strategically [62], because\nit is hard to play a game strategically that one does not understand. Likewise,\nplanning requires a clear understanding of the meaning and effect of actions,\notherwise a plan would reduce to mere guesswork, which is the exact opposite of\na rational plan. This principle continues to apply in the significantly more chal-\nlenging field of planning under uncertainty [30], which, despite the uncertainty,\nIntersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI 5\nstill necessitates a clear understanding of the meaning of each action, as well as\nof the remaining uncertainty about its outcome and/or the interference by the\nenvironment. Several different approaches have been introduced for representing\nkn"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "pproaches have been introduced for representing\nknowledge about actions for planning, including situation calculus [84,85], fluent\ncalculus [101], event calculus [93], see [61] for a survey.\n8rmbl0skZ\n7ZpZ0ZpZ0\n60Zpo0ZpZ\n5o0ZPo0Zp\n4PZPZPZ0O\n3Z0Z0ZPZ0\n20O0Z0ZPZ\n1SNAQJBMR\na b c d e f g h8rmbl0skZ\n7ZpZ0ZpZ0\n60Zpo0ZpZ\n5o0ZPo0Zp\n4PZPZPZ0O\n3Z0Z0ZPZ0\n20O0J0ZPZ\n1SNAQZBMR\na b c d e f g h\nFig. 1. A move of the white king at e1tod2in a chess play\nIt takes sufficient symbolic understanding of the rules of chess to find that the\nmove of the white king from field e1tod2in Fig. 1, for example, is a legal move.\nWithout an understanding of the game at least in the form of which moves are\nlegal when, and what the objective of the game is in the first place, no strategic\ngame play beyond random trial and err"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "no strategic\ngame play beyond random trial and error is possible. Symbolic game knowledge\nincludes the fact that kings can move one field to any of the up to 8 adjacent fields\nnot occupied by pieces of the same player and that the objective is to capture\nthe opponent’s king without losing your own. After a computer program won\ncheckers, which much later led to solving the game of checkers with perfect game\nplays [90], Deep Blue was the first computer to defeat the human world chess\nchampion [42]. It worked by incredibly efficient heuristic search [71] in games\nenabling the exploration possibilities many moves into the future despite the\nresulting combinatorial explosion.\nDespite an initial excitement about logic and game search based symbolic\nAI and additional waves of symbolic AI such as "
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "ic\nAI and additional waves of symbolic AI such as in so-called expert systems [20]\nand AI planning [32], purely symbolic AI also has its shortcomings that were\ninitially underestimated considerably. It is, retroactively somewhat unsurpris-\ningly, exceedingly difficult to exhaustively formalize all required facts about the\nworld to enable general common-sense reasoning. And, yet, symbolic AI has\nachieved impressive feats that go beyond solving chess [71], and use logic to\nprove new theorems in Boolean algebra [57], verify hardware and software [15],\nsolve extraordinarily large SAT instances [11], find bugs and prove fixes in the\nJava collections library [38], or prove safety and find bugs in the FAA’s Next-\ngeneration Airborne Collision Avoidance System ACAS X [43], formalize the\nproof of t"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "dance System ACAS X [43], formalize the\nproof of the Four Color Theorem [35], the prime number theorem [7], and the\nFeit-Thompson theorem on the solvability of finite groups of odd order [36], or\nprove the 400 year old Kepler conjecture on sphere packing [39].\n6 Andr´ e Platzer\n3 Subsymbolic AI\nThe most canonical example of subsymbolic AI is that of a neural network\n(NN) [89], including Convolutional Neural Networks (CNNs) [50], for machine\nlearning [58]. In its basic form, a (feed-forward) NN is a way of representing a\nreal vectorial function via a fixed finite sequence of layers\nx(n+1)=f(n)\u0000\nA(n)x(n)+b(n)\u0001\n(2)\nof real vectors x(n)with fixed nonlinear activation functions f(n):R→R\nand matrices A(n)of real-valued weights and vectors b(n)of real-valued offsets\n(of compatible dimensions) des"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "real-valued offsets\n(of compatible dimensions) describing the linear transformation A(n)x(n)+b(n).\nLearning or training an NN consists of backpropagation via the partial deriva-\ntives of (2) to optimize A(n)andb(n)so that they better fit to a given training\nset of expected input/output data (starting from the final output at the last n).\nEven if the data propagation (2) as well as its backward propagation for train-\ning have easily comprehensible computations, the individual weights as well as\nthe entirety of a NN is nearly impossible to give a clear meaning to, especially\ngiven the size of deep neural networks as in Large Language Models [104] which\npresently already have up to a trillion weights. NNs are particularly successful\nfor supervised learning where an expected input/output funct"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "ised learning where an expected input/output function is being learned\nbased on a significant amount of training data.\nIf a sufficiently large number of given training images are labeled raining\nor not raining, then NN training leads to NNs that can classify other images\neffectively provided the original training data was representative for the real\nsituations. Of course, if all training images with rain were taken at night while\nall training images without rain were taken during daytime, then the NN might\nclassify all training data correctly even if it accidentally learned the concept\nof nighttime instead of rain and, thus, generalizes poorly. This is the difficult\nphenomenon of a distribution shift that fools learning when the distribution of\nexamples during learning is not representativ"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "n of\nexamples during learning is not representative of the distribution experienced in\nreality. Splitting off a small validation data set from the training data, which is\nusually done to avoid overfitting that merely memorizes the available data with-\nout generalization capabilities, does not help alleviate those concerns, because\nthe validation data is chosen from the same distribution as the training data.\nReinforcement learning (RL) [99] is another canonical example of subsym-\nbolic AI bridging ideas of machine learning with game theory to learn how to act\nin a game scenario from experience. The basic idea is to repeatedly act accord-\ning to some fixed policy, observe the outcome, and increase the probability with\nwhich the policy takes that action in the previous state if the actual ou"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "that action in the previous state if the actual outcome is\nfavorable, otherwise decrease it. For actions that directly reach a terminal state\nin which the assigned outcome can be read off, this is easy to tell. For actions\nthat do not reach a terminal state, their future outcome is uncertain so that an\nestimate of the expected payoff based on the present experience reflected in the\npresent policy is used instead. In its basic form, it suffices to give RL agents\nan experiential black box access to the system they are trying to learn how to\nIntersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI 7\ncontrol, without the need for a symbolic understanding of its pieces and their\nmeanings. RL is very flexible for a range of different problems but is also not\nparticularly sample-efficient. T"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "s but is also not\nparticularly sample-efficient. That is, it takes a very large number of experiments\nfor RL to actually learn.\nTo understand why RL is sample-inefficient, imagine having to learn chess\nstrategies by repeatedly taking random actions on a chess board and observing\nwhether you’ve won or lost in the end. It is not at all easy to trace this ultimate\nfate back to the impact of the individual move decisions along the way (such\nas the one in Fig. 1) and takes an absurdly large number of practice plays to\nlearn meaningful generalizable strategies this way. And yet, this is what the\nAlphaZero algorithm ultimately achieved for games including chess and Go using\na massive number of practice rounds by combining Monte Carlo Tree Search\n(MCTS) and subsymbolic AI of DNNs [95,94]. AlphaZer"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "MCTS) and subsymbolic AI of DNNs [95,94]. AlphaZero is already combining\nsymbolic and subsymbolic AI principles even if its pieces are still further away\nfrom the compositional and interpretable aspects of canonical symbolic AI.\nThe blackbox access of RL to the system is simultaneously its blessing and\nits curse. The advantage is precisely that not much is needed to use RL. The\ndownside is that not much is known about the resulting policy. For principle rea-\nsons, at most statistical guarantees can be provided for every blackbox numerical\naccess even to fairly tame continuous systems [79]. For finite state systems with\nfinite actions, guarantees can still be given in the limit if the (initial) policy never\nconsiders any actions entirely impossible [99]. The reason for the latter require-\nm"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "ossible [99]. The reason for the latter require-\nment is exactly that the only good action could otherwise have been considered\nimpossible initially and would, thus, almost surely never even have been tried.\nThe performance demonstrated by subsymbolic AI techniques is impressive,\nbut they are subsymbolic since one would be hard-pressed to give the meaning\nand justification for one individual weight picked arbitrarily in the middle of an\nunderlying NN like (2), for example. This, after all, is not their working principle.\nRather, the overall outputs of an NN and the decisions of an RL agent are a direct\nfunction of the examples and experiences they have seen during training. More\nto the point, it is exceedingly difficult to know whether the particular answer\ngenerated by subsymbolic AI is t"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "particular answer\ngenerated by subsymbolic AI is the right one [3,48]. The working principle of\nsubsymbolic AI, after all, is not one of a composition of lots of little parts with\nclear meaning as is the case in symbolic AI such as logic.\n4 Intersymbolic AI\nBoth symbolic AI and subsymbolic AI are useful and have been demonstrated\nto solve impressive challenges, even if both also have limitations. In a nutshell,\nexemplified for emphasis in the case of its purest form of logic, symbolic AI :\n1. has high accuracy and precision: logical inferences follow sound proof rules\nthat only ever draw correct conclusions from correct premises. While a proof\nsearch does not have to succeed reliably if the proof search procedure is not\ndeterministic, the proof always justifies correct consequences of the "
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "roof always justifies correct consequences of the premises.\n8 Andr´ e Platzer\n2. is self-explanatory: logical symbols carry clear meaning that, by the logi-\ncal principle of compositionality, give meaning to any combination of them.\nMoreover, a proof of a logical formula gives syntactic meaning to and ex-\nplains the formula in a way that can be syntactically introspected (which is\nparticularly apparent in constructive logic [34,55]).\n3. depends on manual effort to formalize: one of the biggest strengths of logic\nis also its biggest weakness, the fact that logic is ontologically neutral. The\nrules of logic do not depend on whatever the specific application is. Having\nsaid that, application-specific conclusions then depend on formalizing the\nunderlying principles as formulas in the logic tha"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "underlying principles as formulas in the logic that can be used as premises\nto draw interesting conclusions from. Knowledge representation is the area\nof symbolic AI devoted to this nontrivial challenge [97,13,61].\n4. is better at correct reasoning than at common-sense reasoning: it is precisely\nlogic’s insistence on correctness that holds it back in common-sense reason-\ning. In fact, symbolic AI would be perfect at common-sense reasoning, but\nonly after the nearly infeasible challenge of formalizing all relevant parts of\nthe world to obtain the required common-sense knowledge. And even if that\nwere possible, fast symbolic reasoning is difficult to achieve in huge knowl-\nedge bases without useful structures [8]. Besides, a typical common-sense\nconclusion of the fact that Tweety is a bird i"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "nse\nconclusion of the fact that Tweety is a bird is that Tweety flies. Except that\nthis is, of course, an incorrect conclusion even if Tweety is a bird, in case\nTweety is a penguin. Except that this is also wrong in case Tweety the pen-\nguin found a jet pack. Nonomonotonic logics partially alleviate this curse\nbut also lack some of the strengths of classical logics [29,53,14,19].\n5. is challenging at scale: while incredibly big and complicated proofs have\nbeen conducted in logic and logic-based reasoning [35,15,11,36,43,39], proof\nsearch is, nevertheless, challenging for complicated questions. The fact that\nlogic is symbolic so that all of its parts carry meaning and that logic naturally\nsupports explanations makes sure that humans can help in interactive proof\n[35,36,43,39]. But this is b"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": " in interactive proof\n[35,36,43,39]. But this is both a blessing and a curse, because, for sufficiently\nchallenging problems, the ability of humans to help also comes with the need\nfor humans to help. Proving the Kepler conjecture without human help is\nnot realistic. But a proof of the Kepler conjecture was possible with the\ncombined help of automatic symbolic logic and manual human effort [39].\nSubsymbolic AI has mostly complementary strengths and weaknesses. In a nut-\nshell, exemplified for emphasis in the case of neural networks, subsymbolic AI :\n1. works without explicit manual programming: even if a lot of art, cleverness,\nand manual effort is in the design of a neural network architecture its input\nfeatures and in the suitable selection of training data as well as the hyper\nparameter"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "on of training data as well as the hyper\nparameter tuning, the actual training runs are fully automatic and conve-\nniently implemented on highly efficient libraries with GPU bindings such as\nTensorFlow [1] and PyTorch [70].\n2. scales computationally: the individual operations of a learning algorithm\nparallelize well and the outcome (approximately) fits the given data better\nIntersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI 9\nthe more computation time is spent learning from the available data ac-\ncording to experimental learning rate curves that are approximately linear\nin the data [45]. This is in contrast to symbolic AI, where spending more\ncompute time does not make a proof search succeed any better if it used the\nentirely wrong proof strategy to begin with. Parallel computat"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "ng proof strategy to begin with. Parallel computation still helps\nfind proofs if different proof strategies are used in clever ways or proofs are\nparallelized across separate subgoals [105,86,83,40,91]. But, except for very\nfew special fragments, logical proof search techniques are at least exponential\nor NP-hard or even undecidable [16,103,5,87].\n3. depends on absurd amounts of data: unlike humans with their (sometimes\nfallible) generalization bias [73], machine learning techniques need absurd\namounts of training data [104], but then tend to tolerate errors and generalize\nwell if the training distribution meets the real-life distribution.\n4. is difficult to get correct: the other side of the coin that subsymbolic AI does\nnotneed explicit programming since its behavior is defined implicitl"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "rogramming since its behavior is defined implicitly from\ndata is that it does not support explicit programming so its behavior is merely\nimplicit and not explicitly clear either. That is why there are numerous\nchallenges to making AI safe [3,24,47,48]. The unpredictability is particularly\nchallenging in generative AI, such as represented by Generative Adversarial\nNetworks (GAN) [37] and Large Language Models (LLMs) [27].\n5. is hard to explain: as made prominent with the area of eXplainable AI (XAI),\nsubsymbolic AI techniques are famously at odds with explanations and even\nXAI’s explanation mechanisms in image classification are easily fooled [68].\nThis complementary distribution of strengths and weaknesses calls for a syn-\nergy of symbolic AI with subsymbolic AI called Intersymbolic AI.\nIn"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "AI with subsymbolic AI called Intersymbolic AI.\nIntersymbolic AI interlinks symbolic AI and subsymbolic AI such that some\nof its building blocks carry semantic significance while others remain subsymbolic\ncreating effectful significance only in the composition, not in the parts. While\nnot all parts of intersymbolic AI are meaningful, its symbolic parts are and can\ncarry and preserve a clear semantics within those parts, rendering itself to natural\nexplanations. While not all parts of intersymbolic AI are held to the principles of\ncareful compositional semantical design, its subsymbolic parts can benefit from\nindirect characterizations of optimization such as end-to-end expectations in\nmachine learning. While not all parts of intersymbolic AI have equally justifiable\noutcomes, its symbolic "
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "I have equally justifiable\noutcomes, its symbolic parts provide clear correctness arguments. While not\nall parts of intersymbolic AI need explicit characterizations of common-sense\nreasoning aspects, its subsymbolic parts can generalize (within distribution) from\nthe knowledge that is so hard to capture explicitly except in experience. While\nnot all parts of intersymbolic AI are explainable, its symbolic parts provide\nnatural mechanisms for explainability stemming from the symbolic semantics.\nIntersymbolic AI supports the mode of operation where some of its parts\nobey the symbolic principles of compositional semantic meaning and obtain the\nresulting trust, while other parts remain opaque and sidestep the challenging\nquestions of interpretability and trust by remaining subsymbolic. Intersym"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "ility and trust by remaining subsymbolic. Intersymbolic\nAI does not have to choose one of the above two extremes but can go in between,\n10 Andr´ e Platzer\nenabling combinations of techniques with complementary strengths. By clever\nstrategic choices which parts of intersymbolic AI best operate symbolically and\nwhich parts best operate subsymbolically, the respective intrinsic advantages of\nboth arise naturally in the respective intersymbolic AI parts. Canonical inter-\nsymbolic AI features symbolic AI in parts where a clear compositional meaning\ncan be obtained or in which correct outcomes are crucial, and features sub-\nsymbolic AI in parts where crystal clear definitions of expected behavior are\nchallenging or in which explicit solutions are difficult or impractical to obtain\nand implicit o"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": " difficult or impractical to obtain\nand implicit optimization outcomes are better. Other combinations of symbolic\nand subsymbolic AI are perfectly conceivable for intersymbolic AI, however.\n5 A Case for Intersymbolic AI in Cyber-Physical Systems\nOne area with a particularly obvious need for mixing symbolic AI and sub-\nsymbolic AI to form Intersymbolic AI is that of Autonomous Cyber-Physical\nSystems. Cyber-Physical Systems (CPS) combine cyber capabilities such as com-\nputation with physical capabilities such as the motion of a car or robot or the\nflight of an aircraft [74,51,2,77,59,56]. The fact that CPS are safety-critical is\nthe reason behind the well-deserved attention of symbolic AI generalizations to\ncover CPS correctness [74,2,77,59].\nIn a pursuit to enable more autonomy, i.e., the s"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "\nIn a pursuit to enable more autonomy, i.e., the self-propelled operation of\nCPS without external influence or extensive supervision by humans, Autonomous\nCPSs see an increased use of subsymbolic AI, such as machine learning with\nneural networks, because that increases the flexibility and lessens the need for\nexplicit manual programming. But the challenge is that this use of subsymbolic\nAI in Autonomous CPS significantly complicates safety, exactly because the\nCPS is no longer programmed explicitly but is partially optimized or learned\nfrom data. Of course, autonomy can also be achieved through careful design\nin sufficiently narrow domains, but the use of learning techniques precisely has\nthe appeal that not all situations need to be considered explicitly, because of\nthe generalization cap"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "ered explicitly, because of\nthe generalization capabilities of learning techniques. Despite a lot of progress\nin specific domains, it has proven exceedingly difficult to write programs for\nimage recognition without the use of learning from training data, for instance.\nAs the development of the CPS moves from being based on intent (in the form\nof a manually written program) to experience (in the form of training data), it\ngets increasingly difficult, however, to then justify why the resulting autonomous\nCPS is safe to use.\nMore generally, while AI-based autonomy is, in some contexts, the only plau-\nsible route toward achieving system operability, the resulting lack of predictabil-\nity is more detrimental to required safety guarantees, e.g., in self-driving cars,\nautonomous delivery drones, "
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "in self-driving cars,\nautonomous delivery drones, or mobile robots. Limits in communication, situa-\ntional awareness, and scale require autonomous operation for a sufficient dura-\ntion until a human driver, pilot, or operator can step in safely. AI helps adapt\nto situations flexibly without requiring explicit programming for all situations.\nThe ironic Catch-22 is that purely data-driven subsymbolic AI technologies fun-\nIntersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI 11\ndamentally lack the guarantees and predictions required for the safe operation\nof a CPS. This is where a solid logical foundation for Safe AI in CPS [24,78,100]\ndevelops intersymbolic AI combining symbolic and subsymbolic AI aspects to\nget the best of both worlds: compelling predictions that symbolic AI proof t"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "s: compelling predictions that symbolic AI proof tech-\nniques provide alongside compelling flexibility that subsymbolic AI provides.\nWhile it is fairly obvious that a supervisor could interfere with an AI or\nlearning agent whenever those go astray, its crucial requirement for CPSs is that\na safety analysis has to identify what exact interference is needed and when in\norder to guarantee a safe operation of the AI in the CPS going forward. It is also\nimportant that all interference minimizes its impact on the AI’s learning capa-\nbilities. Otherwise, for instance, an excessive safety net might lead to a trivial\nlearning outcome. A reinforcement learning agent for a car sandboxed in a safe\noperating regime might merely learn to always accelerate at full speed, because\nits safety net will handl"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "e at full speed, because\nits safety net will handle the rest to brake on time before impact. Safe AI for\nCPS [24] can exploit the provably correct model monitoring and shielding tech-\nnique ModelPlex [60] to identify when and how to interfere with an AI’s decision\nin order to keep the CPS safe [25]. This approach even works for (limited) un-\ncertainty about the physical model [26]. The resulting guarantees of Safe AI for\nCPS could also help overcome the explainability crisis that hinders the safe use\nof AI in railway applications [92].\n6 Some Successful Intersymbolic AI Combinations\nWhile the idea of intersymbolic AI combining symbolic AI with subsymbolic\nAI is general, some successful combinations may help illustrate the generality\nand flexibility of intersymbolic AI. Even if the respecti"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "xibility of intersymbolic AI. Even if the respective intersymbolic AI fea-\ntures are remarkably different, the approaches are still discussed in groups to\nbetter tease out how similarities in goals may nevertheless manifest in remark-\nably different ways of forming intersymbolic AI out of different symbolic AI and\nsubsymbolic AI combinations. Intersymbolic AI is intentionally meant to be un-\nderstood broadly and not limited solely to continuations of the following list of\nintersymbolic AI successes.\nSafe AI in CPS. As explained in Section 5, there is a clear and nontrivial need\nto make the use of AI safe in cyber-physical systems safe. Safe AI in CPS [24,78]\nhas been made possible for the case of reinforcement learning with Justified\nSpeculative Control [25,26] as well as for neural networ"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "ative Control [25,26] as well as for neural network control systems with\nVerSAILLE [100]. In both cases, the symbolic AI technique of hybrid systems\ntheorem proving for safe CPS [74,77] has been combined with the subsymbolic\nAI of reinforcement learning for learning from active experimentation [99] or\nneural networks for learning from experimental data [89], respectively, which\nare interlinked with the symbolic AI technique of shielding via ModelPlex to\novercome action discrepancies between known safe and possibly unsafe outcomes\n[60]. Besides working for reinforcement learning in CPS and neural network\ncontrol systems, respectively, the difference is that safety combines offline and\n12 Andr´ e Platzer\nonline analysis elements [25,26] or is entirely conducted offline before launch of\nthe s"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": " entirely conducted offline before launch of\nthe system [100]. The neural network control systems verification technique also\nuses additional AI techniques of complete arithmetic linearization techniques\n[100] to scale symbolic AI arithmetic verification to the scale of subsymbolic AI\nneural networks for control. Intersymbolic AI in the form of combinations of the\nsubsymbolic AI techniques of neural network path tracking and model-predictive\ncontrol with symbolic AI hybrid systems theorem proving has also been used for\nsafe waypoint following vehicles [52] using some of the Safe AI for CPS ideas.\nAutomatic Symbolic AI for CPS Proofs. Two very different intersymbolic AI\napproaches both support totally different ends of the spectrum of making sym-\nbolic AI for CPS proof search automatic [74,"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": " sym-\nbolic AI for CPS proof search automatic [74,74]. One concerns Pegasus, the\nautomatic invariant generator for differential equations [96]. The other concerns\nCESAR, the automatic control envelope synthesizer for hybrid systems [44].\nPegasus automates the inner loop of the sophisticated mathematical task of\ngenerating invariants for differential equations, with which differential equations\nverification becomes decidable [80]. CESAR automates the creative control task\nof finding maximally flexible hybrid systems control constraints that make the\nhybrid system with its controllers and differential equations safe to begin with.\nEven more different than the part of the safety verification task where they\nare used are the symbolic AI and subsymbolic AI techniques that are combined\nto make t"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "symbolic AI techniques that are combined\nto make them happen. Both Pegasus and CESAR share the use of symbolic\nAI ideas from logic for hybrid systems with differential equations [74,77]. But\nPegasus also uses the symbolic concepts of first integrals of differential equa-\ntions, which represent conserved quantities, and Darboux polynomials whose\nLie derivatives are polynomial multiples of themselves. CESAR, instead, uses\nthe symbolic AI concepts of hybrid systems game theory to characterize opti-\nmal solutions [76,77]. Pegasus uses subsymbolic principles of eigensystems for\nlinear differential equations with numerical sum-of-squares programming or lin-\near programming to find parameter choices for nonlinear differential equation\ninvariants, while CESAR uses systematic underapproximation ref"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "while CESAR uses systematic underapproximation refinements and\narithmetic simplification heuristics.\nLearning to Learn and Teach Theorems. Proving theorems is hard and learning\nto prove theorems is not only difficult, but also made impossible by the acutely\nlimited number of theorems and proofs to learn from, compared to the absurd\ndata hunger of machine learning techniques. That is why the Looprl prover [49]\ncircumvents the issue by providing both a student agent that learns how to\nprove theorems and a teacher agent that learns how to pose effective theorem\nproving challenges that the student agent can learn to prove theorems from. Both\nuse an AlphaZero agent [95] with Monte-Carlo Tree Search for a symbolic AI\nnondeterministic programming language to describe very high level generic proof"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "language to describe very high level generic proof\nsearch strategies (such as: “find an invariant that proves the postcondition of a\nwhile loop program”) refined by subsymbolic AI deep neural networks trained\nvia the AlphaZero agent.\nIntersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI 13\n7 Conclusions and Outlook\nThe most important observation in this perspective piece is the characterization\nof the different features of symbolic AI and subsymbolic AI and explanations\nwhere they come from, from the symbolic and subsymbolic nature respectively,\nas well as an analysis of their complementary strengths and weaknesses. The\nmost important contribution is the call to action in this new field that is here\ncoined intersymbolic AI combining symbolic AI and subsymbolic AI to increase\nthe ef"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": " symbolic AI and subsymbolic AI to increase\nthe effectiveness of AI likened to the way that the combination of conscious and\nsubconscious thought increases the effectiveness of human thought. In vastly dif-\nferent applications, a wide variety of entirely different combinations of remark-\nably different symbolic AI and remarkably different subsymbolic AI techniques\nto form Intersymbolic AI highlight showcases for the power and potential of In-\ntersymbolic AI. It is this scientific diversity alongside the common underlying\nmethodological metaprinciple captured in Intersymbolic AI that gives this per-\nspective piece the biggest significance. Future work abounds in the hope that an\nequally wide range of scientists notice the even wider potential of the use and\nadvances of intersymbolic AI.\nAck"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "l of the use and\nadvances of intersymbolic AI.\nAcknowledgment. We thank Jonathan Laurent, Noah Abou El Wafa, Alexander\nWeigl, Peter Sanders, Bernhard Beckert and Veit Hagenmeyer for their feedback.\nReferences\n1. Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado,\nG.S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I.J., Harp, A.,\nIrving, G., Isard, M., Jia, Y., J´ ozefowicz, R., Kaiser, L., Kudlur, M., Levenberg,\nJ., Man´ e, D., Monga, R., Moore, S., Murray, D.G., Olah, C., Schuster, M., Shlens,\nJ., Steiner, B., Sutskever, I., Talwar, K., Tucker, P.A., Vanhoucke, V., Vasude-\nvan, V., Vi´ egas, F.B., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu,\nY., Zheng, X.: TensorFlow: Large-scale machine learning on heterogeneous dis-\ntributed systems. CoR"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "arning on heterogeneous dis-\ntributed systems. CoRR abs/1603.04467 (2016)\n2. Alur, R.: Principles of Cyber-Physical Systems. MIT Press, Cambridge (2015)\n3. Amodei, D., Olah, C., Steinhardt, J., Christiano, P.F., Schulman, J., Man´ e, D.:\nConcrete problems in AI safety. CoRR abs/1606.06565 (2016)\n4. Angluin, D.: Learning regular sets from queries and counterexamples. Inf. Com-\nput.75(2), 87–106 (1987)\n5. Atserias, A., M¨ uller, M.: Automating resolution is NP-hard. J. ACM 67(5), 31:1–\n31:17 (2020). doi: 10.1145/3409472\n6. Avigad, J.: Mathematics and the formal turn. Bull. Amer. Math. Soc. 61, 225–240\n(2024). doi: 10.1090/bull/1832\n7. Avigad, J., Donnelly, K., Gray, D., Raff, P.: A formally verified proof of the\nprime number theorem. ACM Trans. Comput. Log. 9(1), 2 (2007). doi: 10.1145/\n1297"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "s. Comput. Log. 9(1), 2 (2007). doi: 10.1145/\n1297658.1297660, https://doi.org/10.1145/1297658.1297660\n8. Baader, F., Horrocks, I., Lutz, C., Sattler, U.: An Introduction to Description\nLogic. Cambridge University Press (2017)\n14 Andr´ e Platzer\n9. Bachmair, L., Ganzinger, H.: Rewrite-based equational theorem proving with se-\nlection and simplification. J. Log. Comput. 4(3), 217–247 (1994). doi: 10.1093/\nlogcom/4.3.217\n10. Bansal, K., Loos, S.M., Rabe, M.N., Szegedy, C., Wilcox, S.: HOList: An environ-\nment for machine learning of higher order logic theorem proving. In: Chaudhuri,\nK., Salakhutdinov, R. (eds.) Proceedings of the 36th International Conference on\nMachine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA.\nProceedings of Machine Learning Research, vol. 97, pp. 45"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "ings of Machine Learning Research, vol. 97, pp. 454–463. PMLR (2019)\n11. Biere, A., Heule, M., van Maaren, H., Walsh, T. (eds.): Handbook of Satisfiability\n- Second Edition, Frontiers in Artificial Intelligence and Applications, vol. 336.\nIOS Press (2021). doi: 10.3233/FAIA336\n12. Booch, G., Fabiano, F., Horesh, L., Kate, K., Lenchner, J., Linck, N., Loreg-\ngia, A., Murugesan, K., Mattei, N., Rossi, F., Srivastava, B.: Thinking fast and\nslow in AI. In: Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI\n2021, Thirty-Third Conference on Innovative Applications of Artificial Intelli-\ngence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artifi-\ncial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021. pp. 15042–15046.\nAAAI Press (2021). doi: 10.1609/AAAI.V35I17"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "15046.\nAAAI Press (2021). doi: 10.1609/AAAI.V35I17.17765\n13. Brachman, R., Levesque, H.: Knowledge Representation and Reasoning. Morgan\nKaufmann (2014)\n14. Brewka, G.: Nonmonotonic Reasoning. Cambridge Univ. Press (1991)\n15. Clarke, E.M., Emerson, E.A., Sifakis, J.: Model checking: algorithmic verifica-\ntion and debugging. Commun. ACM 52(11), 74–84 (2009). doi: 10.1145/1592761.\n1592781\n16. Cook, S.A.: The complexity of theorem-proving procedures. In: Harrison, M.A.,\nBanerji, R.B., Ullman, J.D. (eds.) STOC. pp. 151–158. ACM, New York (1971).\ndoi: 10.1145/800157.805047\n17. Cropper, A., Dumancic, S., Muggleton, S.H.: Turning 30: New ideas in induc-\ntive logic programming. In: Bessiere, C. (ed.) Proceedings of the Twenty-Ninth\nInternational Joint Conference on Artificial Intelligence, IJCAI 20"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "nt Conference on Artificial Intelligence, IJCAI 2020. pp. 4833–\n4839. ijcai.org (2020). doi: 10.24963/IJCAI.2020/673, https://doi.org/10.24963/\nijcai.2020/673\n18. Eco, U.: A Theory of Semiotics. Advances in Semiotics, Indiana Univ. Press (1976)\n19. Eiter, T., Gottlob, G.: The complexity of logic-based abduction. J. ACM 42(1),\n3–42 (1995). doi: 10.1145/200836.200838\n20. Feigenbaum, E.A.: How the ”what” becomes the ”how” - turing award lecture.\nCommun. ACM 39(5), 97–104 (1996). doi: 10.1145/229459.229471\n21. Fikes, R., Nilsson, N.J.: STRIPS: A new approach to the application of theorem\nproving to problem solving. In: Cooper, D.C. (ed.) Proceedings of the 2nd Inter-\nnational Joint Conference on Artificial Intelligence. London, UK, September 1-3,\n1971. pp. 608–620. William Kaufmann (1971)\n22. "
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "3,\n1971. pp. 608–620. William Kaufmann (1971)\n22. Fitting, M.: First-Order Logic and Automated Theorem Proving. Springer, New\nYork, 2nd edn. (1996)\n23. Frege, G.: Begriffsschrift, eine der arithmetischen nachgebildete Formelsprache\ndes reinen Denkens. Verlag von Louis Nebert, Halle (1879). doi: 10.1007/\n978-3-662-45011-6\n24. Fulton, N., Platzer, A.: Safe AI for CPS. In: IEEE International Test Conference,\nITC 2018, Phoenix, AZ, USA, October 29 - Nov. 1, 2018. pp. 1–7. IEEE (2018).\ndoi: 10.1109/TEST.2018.8624774\nIntersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI 15\n25. Fulton, N., Platzer, A.: Safe reinforcement learning via formal methods: Toward\nsafe control through proof and learning. In: McIlraith, S.A., Weinberger, K.Q.\n(eds.) AAAI. pp. 6485–6492. AAAI Press (2018)\n26. Fulto"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": ") AAAI. pp. 6485–6492. AAAI Press (2018)\n26. Fulton, N., Platzer, A.: Verifiably safe off-model reinforcement learning. In: Voj-\nnar, T., Zhang, L. (eds.) TACAS, Part I. LNCS, vol. 11427, pp. 413–430. Springer\n(2019). doi: 10.1007/978-3-030-17462-0 28\n27. Ganguli, D., Hernandez, D., Lovitt, L., Askell, A., Bai, Y., Chen, A., Conerly,\nT., DasSarma, N., Drain, D., Elhage, N., Showk, S.E., Fort, S., Hatfield-Dodds,\nZ., Henighan, T., Johnston, S., Jones, A., Joseph, N., Kernian, J., Kravec, S.,\nMann, B., Nanda, N., Ndousse, K., Olsson, C., Amodei, D., Brown, T.B., Kaplan,\nJ., McCandlish, S., Olah, C., Amodei, D., Clark, J.: Predictability and surprise\nin large generative models. In: FAccT ’22: 2022 ACM Conference on Fairness,\nAccountability, and Transparency, Seoul, Republic of Korea, June 21 "
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "d Transparency, Seoul, Republic of Korea, June 21 - 24, 2022.\npp. 1747–1764. ACM (2022). doi: 10.1145/3531146.3533229\n28. d’Avila Garcez, A.S., Lamb, L.C., Gabbay, D.M.: Neural-Symbolic Cognitive Rea-\nsoning. Cognitive Technologies, Springer (2009). doi: 10.1007/978-3-540-73246-4\n29. Geffner, H.: Default reasoning: causal and conditional theories. MIT Press (1992)\n30. Geffner, H., Bonet, B.: A Concise Introduction to Models and Methods\nfor Automated Planning. Synthesis Lectures on Artificial Intelligence and\nMachine Learning, Morgan & Claypool Publishers (2013). doi: 10.2200/\nS00513ED1V01Y201306AIM022\n31. Gentzen, G.: Untersuchungen ¨ uber das logische Schließen I. Math. Zeit. 39(2),\n176–210 (1935). doi: 10.1007/BF01201353\n32. Ghallab, M., Nau, D., Traverso, P.: Automated Planning and Acti"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "Nau, D., Traverso, P.: Automated Planning and Acting. Cambridge\nUniv. Press (2016)\n33. G¨ odel, K.: Die Vollst¨ andigkeit der Axiome des logischen Funktionenkalk¨ uls.\nMonatshefte Math. Phys. 37, 349–360 (1930). doi: 10.1007/BF01696781\n34. G¨ odel, K.: Zum intuitionistischen Aussagenkalk¨ ul. Anzeiger Akademie der Wis-\nsenschaften Wien 69, 65–66 (1932)\n35. Gonthier, G.: A computer-checked proof of the four colour theorem. Tech. Rep.\nhal-04034866, INRIA (2005), hAL report 2023\n36. Gonthier, G., Asperti, A., Avigad, J., Bertot, Y., Cohen, C., Garillot, F.,\nRoux, S.L., Mahboubi, A., O’Connor, R., Biha, S.O., Pasca, I., Rideau, L.,\nSolovyev, A., Tassi, E., Th´ ery, L.: A machine-checked proof of the odd order\ntheorem. In: Blazy, S., Paulin-Mohring, C., Pichardie, D. (eds.) Interactive The-\nore"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "ing, C., Pichardie, D. (eds.) Interactive The-\norem Proving - 4th International Conference, ITP 2013, Rennes, France, July\n22-26, 2013. Proceedings. LNCS, vol. 7998, pp. 163–179. Springer (2013). doi:\n10.1007/978-3-642-39634-2 14\n37. Goodfellow, I.J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,\nS., Courville, A.C., Bengio, Y.: Generative adversarial networks. Commun. ACM\n63(11), 139–144 (2020). doi: 10.1145/3422622\n38. de Gouw, S., Rot, J., de Boer, F.S., Bubel, R., H¨ ahnle, R.: OpenJDK’s\nJava.utils.Collection.sort() is broken: The good, the bad and the worst case.\nIn: Kroening, D., Pasareanu, C.S. (eds.) Computer Aided Verification - 27th\nInternational Conference, CAV 2015, San Francisco, CA, USA, July 18-24,\n2015, Proceedings, Part I. LNCS, vol. 9206, pp. 273–289. Spr"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "eedings, Part I. LNCS, vol. 9206, pp. 273–289. Springer (2015). doi:\n10.1007/978-3-319-21690-4 16\n39. Hales, T.C., Adams, M., Bauer, G., Dang, D.T., Harrison, J., Hoang, T.L.,\nKaliszyk, C., Magron, V., McLaughlin, S., Nguyen, T.T., Nguyen, T.Q., Nipkow,\nT., Obua, S., Pleso, J., Rute, J.M., Solovyev, A., Ta, A.H.T., Tran, T.N., Trieu,\n16 Andr´ e Platzer\nD.T., Urban, J., Vu, K.K., Zumkeller, R.: A formal proof of the Kepler conjecture.\nForum of Mathematics, Pi 5, e2 (2017). doi: DOI:10.1017/fmp.2017.1\n40. Hamadi, Y., Sais, L. (eds.): Handbook of Parallel Constraint Reasoning. Springer\n(2018). doi: 10.1007/978-3-319-63516-3\n41. Haugeland, J.: Artificial Intelligence: the very idea. MIT Press, USA (1989)\n42. Hsu, F.h.: Behind Deep Blue: building the computer that defeated the world chess\nchamp"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "g the computer that defeated the world chess\nchampion. Princeton University Press, Princeton (2002)\n43. Jeannin, J., Ghorbal, K., Kouskoulas, Y., Schmidt, A., Gardner, R., Mitsch, S.,\nPlatzer, A.: A formally verified hybrid system for safe advisories in the next-\ngeneration airborne collision avoidance system. STTT 19(6), 717–741 (2017). doi:\n10.1007/s10009-016-0434-1\n44. Kabra, A., Laurent, J., Mitsch, S., Platzer, A.: CESAR: Control envelope synthe-\nsis via angelic refinements. In: Finkbeiner, B., Kov´ acs, L. (eds.) TACAS. LNCS,\nvol. 14570, pp. 144–164. Springer (2024). doi: 10.1007/978-3-031-57246-3 9\n45. Kaplan, J., McCandlish, S., Henighan, T., Brown, T.B., Chess, B., Child, R.,\nGray, S., Radford, A., Wu, J., Amodei, D.: Scaling laws for neural language mod-\nels. CoRR abs/2001.08361 "
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "for neural language mod-\nels. CoRR abs/2001.08361 (2020)\n46. Kautz, H.A.: The third AI summer: AAAI Robert S. Engelmore memorial lecture.\nAI Mag. 43(1), 93–104 (2022). doi: 10.1609/AIMAG.V43I1.19122\n47. Kwiatkowska, M., Norman, G., Parker, D.: Probabilistic model checking and\nautonomy. Annu. Rev. Control. Robotics Auton. Syst. 5, 385–410 (2022). doi:\n10.1146/annurev-control-042820-010947\n48. Kwiatkowska, M., Zhang, X.: When to trust AI: advances and challenges for\ncertification of neural networks. In: Ganzha, M., Maciaszek, L.A., Paprzycki, M.,\nSlezak, D. (eds.) Proceedings of the 18th Conference on Computer Science and\nIntelligence Systems, FedCSIS 2023, Warsaw, Poland, September 17-20, 2023.\nAnnals of Computer Science and Information Systems, vol. 35, pp. 25–37 (2023).\ndoi: 10.15439/2023"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "ems, vol. 35, pp. 25–37 (2023).\ndoi: 10.15439/2023F2324\n49. Laurent, J., Platzer, A.: Learning to find proofs and theorems by learning to refine\nsearch strategies. In: Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho,\nK., Oh, A. (eds.) Advances in Neural Information Processing Systems. vol. 35, p.\n4843–4856. Curran Associates, Inc. (2022)\n50. LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W.,\nJackel, L.D.: Backpropagation applied to handwritten zip code recognition. Neural\nComputation 1(4), 541–551 (12 1989). doi: 10.1162/neco.1989.1.4.541\n51. Lee, E.A., Seshia, S.A.: Introduction to Embedded Systems — A Cyber-Physical\nSystems Approach. Lulu.com (2013)\n52. Lin, Q., Mitsch, S., Platzer, A., Dolan, J.M.: Safe and resilient practical waypoint-\nfollowing for a"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": " and resilient practical waypoint-\nfollowing for autonomous vehicles. IEEE Control. Syst. Lett. 6, 1574–1579 (2022).\ndoi: 10.1109/LCSYS.2021.3125717\n53.  Lukaszewicz, W.: Non-monotonic Reasoning. Ellis Horwood (1990)\n54. Martin, G.M., Frazier, D.T., Robert, C.P.: Computing Bayes: From then ‘til now.\nStatistical Science 39(1), 3 – 19 (2024). doi: 10.1214/22-STS876\n55. Martin-L¨ of, P.: Constructive mathematics and computer programming. In: Logic,\nMethodology and Philosophy of Science VI. pp. 153–175. North-Holland (1980)\n56. Marwedel, P.: Embedded System Design: Embedded Systems Foundations of\nCyber-Physical Systems, and the Internet of Things. Springer, 4 edn. (2021).\ndoi: 10.1007/978-3-030-60910-8\n57. McCune, W.: Solution of the Robbins problem. J. Autom. Reason. 19(3), 263–276\n(1997). do"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "oblem. J. Autom. Reason. 19(3), 263–276\n(1997). doi: 10.1023/A:1005843212881\nIntersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI 17\n58. Mitchell, T.: Machine Learning. McGraw-Hill (1997)\n59. Mitra, S.: Verifying Cyber-Physical Systems: A Path to Safe Autonomy. MIT\nPress (2021)\n60. Mitsch, S., Platzer, A.: ModelPlex: Verified runtime validation of verified cyber-\nphysical system models. Form. Methods Syst. Des. 49(1-2), 33–74 (2016). doi:\n10.1007/s10703-016-0241-z, special issue of selected papers from RV’14\n61. Mitsch, S., Platzer, A., Retschitzegger, W., Schwinger, W.: Logic-based modeling\napproaches for qualitative and hybrid reasoning in dynamic spatial systems. ACM\nComput. Surv. 48(1), 3:1–3:40 (2015). doi: 10.1145/2764901\n62. von Neumann, J., Morgenstern, O.: Theory of Games"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": " von Neumann, J., Morgenstern, O.: Theory of Games and Economic Behavior.\nPrinceton Univ. Press, 3rd edn. (1955)\n63. Newell, A., Shaw, J.C., Simon, H.A.: Chess-playing programs and the problem of\ncomplexity. IBM J. Res. Dev. 2(4), 320–335 (1958). doi: 10.1147/RD.24.0320\n64. Newell, A., Shaw, J.C., Simon, H.A.: Report on a general problem-solving pro-\ngram. In: Information Processing, Proceedings of the 1st International Conference\non Information Processing, UNESCO, Paris 15-20 June 1959. pp. 256–264. UN-\nESCO (Paris) (1959)\n65. Newell, A., Simon, H.A.: The logic theory machine-a complex information pro-\ncessing system. IRE Trans. Inf. Theory 2(3), 61–79 (1956). doi: 10.1109/TIT.\n1956.1056797\n66. Newell, A., Simon, H.A.: Computer science as empirical inquiry: Symbols and\nsearch. Commun. ACM"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "empirical inquiry: Symbols and\nsearch. Commun. ACM 19(3), 113–126 (1976). doi: 10.1145/360018.360022\n67. Nieuwenhuis, R., Oliveras, A., Tinelli, C.: Solving SAT and SAT modulo theories:\nFrom an abstract Davis–Putnam–Logemann–Loveland procedure to DPLL( t). J.\nACM 53(6), 937–977 (2006). doi: 10.1145/1217856.1217859\n68. Noppel, M., Peter, L., Wressnegger, C.: Disguising attacks with explanation-aware\nbackdoors. In: 44th IEEE Symposium on Security and Privacy, SP 2023, San\nFrancisco, CA, USA, May 21-25, 2023. pp. 664–681. IEEE (2023). doi: 10.1109/\nSP46215.2023.10179308\n69. Paliwal, A., Loos, S.M., Rabe, M.N., Bansal, K., Szegedy, C.: Graph represen-\ntations for higher-order logic and theorem proving. In: The Thirty-Fourth AAAI\nConference on Artificial Intelligence, AAAI 2020, The Thirty-Seco"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "rtificial Intelligence, AAAI 2020, The Thirty-Second Innovative\nApplications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI\nSymposium on Educational Advances in Artificial Intelligence, EAAI 2020, New\nYork, NY, USA, February 7-12, 2020. pp. 2967–2974. AAAI Press (2020). doi:\n10.1609/AAAI.V34I03.5689\n70. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,\nT., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., K¨ opf, A., Yang, E.Z.,\nDeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai,\nJ., Chintala, S.: PyTorch: An imperative style, high-performance deep learning\nlibrary. In: Wallach, H.M., Larochelle, H., Beygelzimer, A., d’Alch´ e-Buc, F., Fox,\nE.B., Garnett, R. (eds.) Advances in Neural Information Processing Syst"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "s.) Advances in Neural Information Processing Systems 32:\nAnnual Conference on Neural Information Processing Systems 2019, NeurIPS\n2019, December 8-14, 2019, Vancouver, BC, Canada. pp. 8024–8035 (2019)\n71. Pearl, J.: Heuristics - intelligent search strategies for computer problem solving.\nAddison-Wesley series in artificial intelligence, Addison-Wesley (1984)\n72. Pearl, J.: Probabilistic reasoning in intelligent systems - networks of plausible in-\nference. Morgan Kaufmann series in representation and reasoning, Morgan Kauf-\nmann (1989)\n73. Peters, U., Krauss, A., Braganza, O.: Generalization bias in science. Cogn. Sci.\n46(9) (2022). doi: 10.1111/COGS.13188\n18 Andr´ e Platzer\n74. Platzer, A.: Logical Analysis of Hybrid Systems: Proving Theorems for Complex\nDynamics. Springer, Heidelberg (20"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "ems for Complex\nDynamics. Springer, Heidelberg (2010). doi: 10.1007/978-3-642-14509-4\n75. Platzer, A.: Logics of dynamical systems. In: LICS. pp. 13–24. IEEE, Los Alamitos\n(2012). doi: 10.1109/LICS.2012.13\n76. Platzer, A.: Differential game logic. ACM Trans. Comput. Log. 17(1), 1:1–1:51\n(2015). doi: 10.1145/2817824\n77. Platzer, A.: Logical Foundations of Cyber-Physical Systems. Springer, Cham\n(2018). doi: 10.1007/978-3-319-63588-0\n78. Platzer, A.: The logical path to autonomous cyber-physical systems. In: Parker,\nD., Wolf, V. (eds.) QEST. LNCS, vol. 11785, pp. 25–33. Springer (2019). doi:\n10.1007/978-3-030-30281-8 2\n79. Platzer, A., Clarke, E.M.: The image computation problem in hybrid systems\nmodel checking. In: Bemporad, A., Bicchi, A., Buttazzo, G. (eds.) HSCC. LNCS,\nvol. 4416, pp. 473–"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "uttazzo, G. (eds.) HSCC. LNCS,\nvol. 4416, pp. 473–486. Springer, Berlin (2007). doi: 10.1007/978-3-540-71493-4\n37\n80. Platzer, A., Tan, Y.K.: Differential equation invariance axiomatization. J. ACM\n67(1), 6:1–6:66 (2020). doi: 10.1145/3380825\n81. Popper, K.R.: Conjectures and Refutations: The Growth of Scientific Knowledge.\nRoutledge, London, England (1962)\n82. Quine, W.V.: On natural deduction. J. Symb. Log. 15(2), 93–102 (1950)\n83. Reger, G., Tishkovsky, D., Voronkov, A.: Cooperating proof attempts. In: Felty,\nA.P., Middeldorp, A. (eds.) Automated Deduction - CADE-25 - 25th Inter-\nnational Conference on Automated Deduction, Berlin, Germany, August 1-7,\n2015, Proceedings. LNCS, vol. 9195, pp. 339–355. Springer (2015). doi: 10.1007/\n978-3-319-21401-6 23\n84. Reiter, R.: The frame problem in"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "19-21401-6 23\n84. Reiter, R.: The frame problem in the situation calculus: A simple solution (some-\ntimes) and a completeness result for goal regression. In: Lifschitz, V. (ed.) Arti-\nficial and Mathematical Theory of Computation, Papers in Honor of John Mc-\nCarthy on the occasion of his sixty-fourth birthday. pp. 359–380. Academic Press\n/ Elsevier (1991). doi: 10.1016/B978-0-12-450010-5.50026-8\n85. Reiter, R.: Natural actions, concurrency and continuous time in the situation cal-\nculus. In: Aiello, L.C., Doyle, J., Shapiro, S.C. (eds.) Proceedings of the Fifth\nInternational Conference on Principles of Knowledge Representation and Rea-\nsoning (KR’96), Cambridge, Massachusetts, USA, November 5-8, 1996. pp. 2–13.\nMorgan Kaufmann (1996)\n86. Renshaw, D.W., Loos, S.M., Platzer, A.: Distributed "
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "nshaw, D.W., Loos, S.M., Platzer, A.: Distributed theorem proving for dis-\ntributed hybrid systems. In: Qin, S., Qiu, Z. (eds.) ICFEM. LNCS, vol. 6991, pp.\n356–371. Springer (2011). doi: 10.1007/978-3-642-24559-6 25\n87. de Rezende, S.F., G¨ o¨ os, M., Nordstr¨ om, J., Pitassi, T., Robere, R., Sokolov, D.:\nAutomating algebraic proof systems is NP-hard. In: Khuller, S., Williams, V.V.\n(eds.) STOC ’21: 53rd Annual ACM SIGACT Symposium on Theory of Com-\nputing, Virtual Event, Italy, June 21-25, 2021. pp. 209–222. ACM (2021). doi:\n10.1145/3406325.3451080\n88. Robinson, J.A., Voronkov, A. (eds.): Handbook of Automated Reasoning. MIT\nPress (2001)\n89. Russel, S., Norvig, P.: Artificial Intelligence: a Modern Approach. Pearson, 4 edn.\n(2021)\n90. Schaeffer, J., Burch, N., Bj¨ ornsson, Y., Kishimoto, "
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "effer, J., Burch, N., Bj¨ ornsson, Y., Kishimoto, A., M¨ uller, M., Lake, R., Lu,\nP., Sutphen, S.: Checkers is solved. Science 317(5844), 1518–1522 (2007). doi:\n10.1126/science.1144079\nIntersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI 19\n91. Schreiber, D., Sanders, P.: Scalable SAT solving in the cloud. In: Li, C., Many` a,\nF. (eds.) Theory and Applications of Satisfiability Testing - SAT 2021 - 24th\nInternational Conference, Barcelona, Spain, July 5-9, 2021, Proceedings. LNCS,\nvol. 12831, pp. 518–534. Springer (2021). doi: 10.1007/978-3-030-80223-3 35\n92. Seisenberger, M., ter Beek, M.H., Fan, X., Ferrari, A., Haxthausen, A.E., James,\nP., Lawrence, A., Luttik, B., van de Pol, J., Wimmer, S.: Safe and secure fu-\nture AI-driven railway technologies: Challenges for formal methods"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "ailway technologies: Challenges for formal methods in railway. In:\nMargaria, T., Steffen, B. (eds.) Leveraging Applications of Formal Methods, Veri-\nfication and Validation. Practice, ISoLA. LNCS, vol. 13704, pp. 246–268. Springer\n(2022). doi: 10.1007/978-3-031-19762-8 20\n93. Shanahan, M.: Solving the frame problem - a mathematical investigation of the\ncommon sense law of inertia. MIT Press (1997)\n94. Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanc-\ntot, M., Sifre, L., Kumaran, D., Graepel, T., Lillicrap, T.P., Simonyan, K., Has-\nsabis, D.: Mastering Chess and Shogi by self-play with a general reinforcement\nlearning algorithm. CoRR abs/1712.01815 (2017)\n95. Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A.,\nHubert, T., Baker"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "noglou, I., Huang, A., Guez, A.,\nHubert, T., Baker, L., Lai, M., Bolton, A., Chen, Y., Lillicrap, T.P., Hui, F.,\nSifre, L., van den Driessche, G., Graepel, T., Hassabis, D.: Mastering the game\nof Go without human knowledge. Nat. 550(7676), 354–359 (2017). doi: 10.1038/\nNATURE24270\n96. Sogokon, A., Mitsch, S., Tan, Y.K., Cordwell, K., Platzer, A.: Pegasus: Sound\ncontinuous invariant generation. Form. Methods Syst. Des. 58(1), 5–41 (2022).\ndoi: 10.1007/s10703-020-00355-z, special issue for selected papers from FM’19\n97. Sowa, J.F.: Knowledge Representation: logical, philosophical, and computational\nfoundations. Brooks/Cole, Pacific Grove, CA (2000)\n98. Sukhobokov, A.A., Gapanyuk, Y.E., Chernenkiy, V.M.: Consciousness and sub-\nconsciousness as a means of AGI’s and narrow AI’s integration. In:"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": " a means of AGI’s and narrow AI’s integration. In: Samsonovich,\nA.V. (ed.) Biologically Inspired Cognitive Architectures 2019. Advances in In-\ntelligent Systems and Computing, vol. 948, pp. 515–520. Springer (2019). doi:\n10.1007/978-3-030-25719-4 66\n99. Sutton, R.S., Barto, A.G.: Reinforcement Learning: An Introduction. Bradford\nBooks, 2 edn. (2018)\n100. Teuber, S., Mitsch, S., Platzer, A.: Provably safe neural network controllers via\ndifferential dynamic logic. CoRR abs/2402.10998 (2024)\n101. Thielscher, M.: FLUX: A logic programming method for reasoning agents. Theory\nPract. Log. Program. 5(4-5), 533–565 (2005). doi: 10.1017/S1471068405002358\n102. Towell, G.G., Shavlik, J.W.: Knowledge-based artificial neural networks. Artificial\nIntelligence 70(1), 119–165 (1994). doi: 10.1016/0004-3702"
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "ence 70(1), 119–165 (1994). doi: 10.1016/0004-3702(94)90105-8\n103. Urquhart, A.: The complexity of propositional proofs. Bull. Symb. Log. 1(4),\n425–467 (1995). doi: 10.2307/421131, https://doi.org/10.2307/421131\n104. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,\nKaiser, L., Polosukhin, I.: Attention is all you need. In: Guyon, I., von Luxburg,\nU., Bengio, S., Wallach, H.M., Fergus, R., Vishwanathan, S.V.N., Garnett, R.\n(eds.) Advances in Neural Information Processing Systems 30. pp. 5998–6008\n(2017)\n105. Wenzel, M.: Shared-memory multiprocessing for interactive theorem proving.\nIn: Blazy, S., Paulin-Mohring, C., Pichardie, D. (eds.) Interactive Theorem\nProving - 4th International Conference, ITP 2013, Rennes, France, July 22-\n26, 2013. Proceedings. LNCS, vol."
  },
  {
    "arxiv_id": "2406.11563",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "chunk": "France, July 22-\n26, 2013. Proceedings. LNCS, vol. 7998, pp. 418–434. Springer (2013). doi:\n10.1007/978-3-642-39634-2 30\n"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "Overconfident and Unconfident AI Hinder\nHuman-AI Collaboration\nJingshu Li1†, Yitian Yang1†, Renwen Zhang2, Yi-Chieh Lee1*\n1*Department of Computer Science, National University of Singapore,\n21 Lower Kent Ridge Road, 119077, Singapore.\n2Department of Communications and New Media, National University\nof Singapore, 21 Lower Kent Ridge Road, 119077, Singapore.\n*Corresponding author(s). E-mail(s): ejli.uiuc@gmail.com;\nContributing authors: jingshu@u.nus.edu; t0931554@u.nus.edu;\nr.zhang@nus.edu.sg;\n†These authors contributed equally to this work.\nAbstract\nAI transparency is a central pillar of responsible AI deployment and effective\nhuman-AI collaboration. A critical approach is communicating uncertainty, such\nas displaying AI’s confidence level, or its correctness likelihood (CL), to users.\nHow"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": " or its correctness likelihood (CL), to users.\nHowever, these confidence levels are often uncalibrated, either overestimating or\nunderestimating actual CL, posing risks and harms to human-AI collaboration.\nThis study examines the effects of uncalibrated AI confidence on users’ trust\nin AI, AI advice adoption, and collaboration outcomes. We further examined\nthe impact of increased transparency, achieved through trust calibration sup-\nport, on these outcomes. Our results reveal that uncalibrated AI confidence leads\nto both the misuse of overconfident AI and disuse of unconfident AI, thereby\nhindering outcomes of human-AI collaboration. Deficiency of trust calibration\nsupport exacerbates this issue by making it harder to detect uncalibrated confi-\ndence, promoting misuse and disuse of AI. Con"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "nfi-\ndence, promoting misuse and disuse of AI. Conversely, trust calibration support\naids in recognizing uncalibration and reducing misuse, but it also fosters distrust\nand causes disuse of AI. Our findings highlight the importance of AI confidence\ncalibration for enhancing human-AI collaboration and suggest directions for AI\ndesign and regulation.\nKeywords: Human-AI Collaboration, AI Confidence Calibration, Trust Calibration\n1arXiv:2402.07632v3  [cs.AI]  17 Apr 2024\n1 Introduction\nArtificial intelligence (AI) has become a significant force in various domains, demon-\nstrating its capacity to extract valuable insights from data and opening new avenues\nfor human-AI collaboration. From everyday decisions, such as deciding daily outfits\n[1, 2], to high-risk sectors such as healthcare [3, 4, 5,"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": " to high-risk sectors such as healthcare [3, 4, 5, 6] and investment [7, 8], AI\ncan aid humans in accomplishing a myriad of tasks. However, recent studies found\nthat the effectiveness of human-AI teams often does not surpass that of AI operating\nalone, primarily because humans might follow AI’s incorrect advice, despite having\nthe capability to make superior judgments independently [9, 10, 11, 12, 13]. In fact,\nthe efficacy of human-AI collaboration is conditioned on many factors, one of which is\nthe human ability to accurately assess when AI’s guidance is dependable and when it\nmight falter, thus calibrating their trust accordingly [14, 15, 16, 17]. Trust calibration\nin human-AI collaboration refers to the process by which a person adjusts their trust\nto match the actual reliability and t"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": " their trust\nto match the actual reliability and trustworthiness of the AI system[16, 14]. Adequate\nsupport for trust calibration enables humans to utilize AI judiciously–leveraging it\nwhen it is reliable and refraining from using it when it is not. Conversely, excessive\ntrust in AI, or overtrust , can lead to its misuse by relying on it in unreliable scenarios.\nSimilarly, insufficient trust, or distrust , results in the disuse of AI, even in instances\nwhere AI could offer more reliable advice than human judgment [15, 16].\nRecent studies show that improving AI transparency by disclosing its uncertainty\nlevels can help humans calibrate trust towards AI and improve human decision-making\nabout AI use, fostering more effective human-AI collaboration [18, 19, 20, 21, 13].\nAn effective approach "
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "ation [18, 19, 20, 21, 13].\nAn effective approach to conveying AI uncertainty is expression of confidence , which\nestimate the probability of AI making correct decision, i.e., correctness likelihood [19].\nClassification models, for instance, can yield a percentage output as its confidence\nscore to denote uncertainty [21]. Large language models (LLMs), when prompted\nappropriately, can articulate their confidence using words or sentences [22]. Unlike\naccuracy , which evaluates AI’s overall performance and reliability, confidence levels\nquantify the uncertainty of each individual task [23, 24]. Thus, AI’s confidence may\nserve as a more powerful indicator of task-level uncertainty that influence human trust\nand use of AI. Higher AI confidence facilitates trust in AI and its usage, whereas\nlowe"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "acilitates trust in AI and its usage, whereas\nlower confidence may lead to preference for human judgment [17, 25]. Accurately\ncalibrated AI confidence can enhance not only trust calibration but also the efficiency\nand effectiveness of human-AI collaboration [23, 19, 26].\nWhile existing research often regards well-calibrated AI confidence as a founda-\ntional premise [25, 18, 19, 27], it overlooks a critical reality: achieving well-calibrated\nAI confidence is more challenging and less common than assumed. Despite numerous\nefforts to calibrate AI confidence [28, 23], many AI systems still exhibit misaligned\nconfidence levels, failing to accurately reflect their actual CL [23, 29, 30, 31]. This\ndiscrepancy is evident in AI systems that exhibit overconfidence (i.e., their expressed\nconfidence e"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "overconfidence (i.e., their expressed\nconfidence exceeds their actual CL), notably in some LLMs providing highly confident\nyet incorrect answers [21, 22]. Conversely, there are instances of unconfidence, where\nAI’s confidence falls short of their actual capability [31]. This misalignment highlights\nthe complexity and necessity of accurate confidence calibration in AI systems. Fur-\nthermore, the influences and risks of uncalibrated AI confidence are not sufficiently\n2\nFig. 1 (a) A flowchart illustrating the study procedure. Participants were required to sequentially\ncomplete these four phases. ( b) Conceptual map of variables: attitude is informed by perception\nand influences behavior. Human-AI collaboration outcome is the result of behavior. ( c) City photo\nrecognition task interface. Part"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": ". ( c) City photo\nrecognition task interface. Participants were tasked with identifying the origins of city photos from\nthree choices. In each task, participants made an initial decision, followed by the AI providing a\nsuggestion along with a percentage confidence score, after which participants made their final decision.\nunderstood. Encouraging AI to express uncertainty without recognizing its potential\ndownsides, especially in high-stake scenarios, could lead to negative outcomes. Thus,\nit is imperative to elucidate the impacts and risks of uncalibrated AI on humans’ trust\ncalibration and human-AI collaboration and to identify strategies for mitigating these\nissues.\nTherefore, this study examines the effects of uncalibrated AI confidence on trust\ntowards AI, adoption of AI’s advice, and "
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "on trust\ntowards AI, adoption of AI’s advice, and the outcomes of human-AI collaboration. We\n3\nalso examine to extent to which individuals are able to perceive uncalibrated AI con-\nfidence. We also explore the extent to which individuals can perceive uncalibrated AI\nconfidence. Prior research suggests that individual attitudes, shaped by perceptions\nand beliefs, influence behavior and actions [16, 32, 33]. Thus, we assume that percep-\ntion of an AI’s confidence calibration underpins trust calibration, which in turn affects\nthe adoption of AI advice and the success of human-AI collaboration (as shown in\nFig. 1 (b)). Exploring the effects of uncalibrated AI on trust calibration can provide\ndeeper insights into its negative impacts on human-AI collaboration, rather than just\nfocusing on colla"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": " collaboration, rather than just\nfocusing on collaboration outcomes.\nFurthermore, previous research has indicated that appropriately increasing trans-\nparency can aid individuals in better perceiving and understanding the boundaries\nof AI capabilities, facilitating complementary collaboration [34, 35]. This study also\nexamines whether increase transparency, i.e., trust calibration support, can improve\nparticipants’ perception of uncalibrated confidence, can enable a better understanding\nof AI’s strengths and weaknesses. Such improvement may facilitate rational comple-\nmentary collaboration, thereby mitigating the adverse effects of uncalibrated AI on\nhuman-AI collaboration. Explicitly addressing these questions is crucial for researchers\nand developers keen on mitigating the risks of discl"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "d developers keen on mitigating the risks of disclosing AI uncertainty, building\neffective and harmonious human-AI collaborative relationships.\nWe ran two behavioural experiments to answer these questions. In Experiment\n1(EXP1, N= 126), participants were randomly assigned to collaborate with AI with\ndifferent confidence calibration states: well-calibrated (EXP1-W, N=42), unconfident\n(EXP1-U, N=42), and overconfident (EXP1-O, N=42), to complete a city photo recog-\nnition task (experimental procedure as shown in Fig. 1 (a), task interface in Fig. 1\n(c), with more details in Sec. 4). Participants were expected to calibrate their trust\nin AI through tasks in Trust Calibration Phase and reported their perceptions of\nAI confidence calibration and trust in AI during the Survey Phase. In Collabora"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": " trust in AI during the Survey Phase. In Collaboration\nTask Phase, through formal collaborative tasks, we captured participants’ behaviors\nregarding AI use and assessed the outcomes of collaboration. Across all groups, the\nAI’s accuracy remained constant at 70%, which we used as an approximation ˆ pfor\nthe AI’s average true correctness likelihood. Based on the relationship between the\nAI’s average confidence score ¯ cand ˆp, the confidence calibration states of AI were\ndefined as: unconfident (¯ c= 60% <ˆp), confident (¯ c= 70% = ˆ p), and overconfident\n(¯c= 80% >ˆp). EXP1 investigated the effects of uncalibrated AI confidence without\nadditional support.\nSubsequently, we conducted Experiment 2 (EXP2, N= 126) with two objec-\ntives: firstly, to examine the effects of trust calibration suppor"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "to examine the effects of trust calibration support taking EXP1’s results\nas baselines, and secondly, to explore the effects of uncalibrated AI confidence when\ntrust calibration support is provided. Again, participants were randomly assigned to\ncollaborate with well-calibrated (EXP2-W, N=42), unconfident (EXP2-U, N=42), or\noverconfident (EXP2-O, N=42) AI. All participants received trust calibration support\nduring the Trust Calibration Phase, including disclosing AI’s confidence calibration\nstatus (e.g., overconfidence, unconfidence), providing immediate feedback on each\ntask’s correctness and presenting AI’s overall performance feedback after all tasks\n(more details in Sec. 4). Trust calibration support helped participants by increasing\n4\nFig. 2 A diagram demonstrates how uncalibrated AI i"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "ig. 2 A diagram demonstrates how uncalibrated AI influences individuals and hinders human-AI\ncollaboration. (a)EXP1-U: individuals are unable to detect AI’s unconfidence and overtrust AI’s\nconfidence scores. They mistakenly reject its correct advice, i.e. disuse it, and get poor collabora-\ntion outcomes. (b)EXP1-O: individuals are unable to detect AI’s overconfidence and overtrust AI’s\nconfidence scores. They mistakenly adopt its wrong advice, i.e. misuse it, and get poor collaboration\noutcomes. (c)EXP2-U: with trust calibration support, individuals are able to detect AI’s uncon-\nfidence. However, they distrust AI’s prediction accuracy, resulting in disuse. Finally they get poor\ncollaboration outcomes. (d)EXP2-O: with trust calibration support, individuals are able to detect\nAI’s overconfi"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "ort, individuals are able to detect\nAI’s overconfidence. However, they distrust AI’s prediction accuracy, resulting in disuse. Finally they\nget poor collaboration outcomes.\ntransparency during their trust calibration phase. The rest of the settings in EXP2\nremained the same as in EXP1.\n2 Results\nIn both experiments, uncalibrated AI consistently hindered human-AI collaboration,\nand participants were unable to effectively calibrate their trust towards uncalibrated\nAI. Increased transparency, i.e., trust calibration support, can enhance the perception\nof AI’s uncalibrated state by participants and reduce misuse caused by overconfident\n5\nFig. 3 Heat maps illustrating participants’ perception of AI’s confidence calibration state. Each row\nin a given map corresponds to different groups from EXP1"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "iven map corresponds to different groups from EXP1 and EXP2, categorized by the confidence\ncalibration states of the AI they collaborated with. Each column represents the AI’s confidence\ncalibration states perceived by participants within that group, which could be unconfident, well-\ncalibrated, or overconfident. The red boxes highlight instances where the perceived AI’s confidence\ncalibration state matched the AI’s confidence calibration state in that group, indicating participants\nwho correctly perceived the AI’s confidence calibration state. The numbers in each cell represent\nthe count of participants in the experimental group who perceived the corresponding AI’s confidence\ncalibration state for each column. Darker colors correspond to a greater number of participants. (a)\nis the result"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": " greater number of participants. (a)\nis the result of EXP1. (b)is the result of EXP2.\nAI; however, it introduces a distrust towards AI’s prediction accuracy as well as an\nincrease of disuse, thereby not significantly improving the outcome of human-AI col-\nlaboration. All statistical test result details, descriptive data, additional figures, tests\nrelated to control variables, and demographic data are provided in the supplementary\nmaterial.\n2.1 Overconfident and Unconfident AI Hinder Human-AI\nCollaboration\nThe results from EXP1 demonstrate that participants collaborating with uncalibrated\nAI struggled to perceive the uncalibration of AI confidence. Operating on this incorrect\ninformational basis, their trust in AI did not significantly differ from those collabo-\nrating with well-calibrated "
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "r from those collabo-\nrating with well-calibrated AI. Behaviorally, overconfident and unconfident AI led to\nmisuse and disuse, respectively, through which uncalibrated AI confidence correspond-\ningly increased or decreased participants’ adoption behaviors and resulted in poorer\ncollaboration outcomes (Fig. 2 (a) and (b)).\n6\nFig. 4 One-way ANOVA results of EXP1 are depicted in violin and box plots, about the effects of\nAI’s confidence calibration states on the dependent variables: participants’ Adoption Rate, Disuse\nRate, Misuse Rate, Accuracy Increase, Trust towards AI’s Confidence Score, Trust towards AI’s Pre-\ndiction Accuracy, and Trust towards AI’s Overall Capability. The center line represents the median.\nThe box limits represent upper and lower quartiles. The whiskers represent the 1"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": " and lower quartiles. The whiskers represent the 1.5x interquartile\nrange. The points represent the outliers. The width of each violin corresponds to the frequency of\nobservations at any given number on the y axis. The cut of kernel density estimation is set to 0. The\np-values of the AI confidence calibration states’ main effects from the one-way ANOVA are indicated\nbelow each figure, and the significance levels from post-hoc analyses are denoted within the figures\n(ns:p >0.05, *: p <0.05, **: p <0.01, ***: p <0.001). F-values with degrees of freedom are also\nreported below the figure. Differences between groups from post-hoc analyses, excluding comparisons\nbetween EXP1-U and EXP1-O, are marked in the figure. Further information, such as the verification\nof homogeneity assumptions, analyti"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "e verification\nof homogeneity assumptions, analytical methods, and effect sizes, can be found in the supplementary\nmaterials.\n2.1.1 Perceiving AI’s Overconfidence and Unconfidence is\nChallenging\nThe results of the groups EXP1-U and EXP1-O, as depicted on Fig. 3 (a), indicate\nthat, it is difficult for the majority of participants to perceive if the provided AI’s\nconfidence level aligns with its actual correctness likelihood. In EXP1-U, 28.571%\nof participants correctly perceived the AI’s unconfidence. In EXP1-O, only 26.190%\n7\nof participants correctly perceived the overconfidence. The majority of participants\ndeemed the AI’s confidence calibration state appropriate when the AI was either\noverconfident or unconfident, suggesting an overtrust in AI’s confidence.\nAdditionally, as shown in Fig"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": " in AI’s confidence.\nAdditionally, as shown in Fig. 4 (e)-(g), there were no significant differences\nbetween participants’ trust towards AI’s confidence, predictions, and overall capability\nwithin EXP1.\n2.1.2 Overconfident AI Leads to Misuse\nAs the main effects of AI confidence calibration states in one-way ANOVA are signif-\nicant, according to post-hoc analysis, as shown in Fig. 4 (c), participants in EXP1-O\nexhibited a significantly higher misuse rate ( M= 41.257%, s.d.= 10.835%, p <0.001)\ncompared to those in EXP1-W ( M= 28.207%, s.d.= 14.002%). Moreover, the adop-\ntion rate (see Fig. 4 (a)) for participants in EXP1-O ( M= 69.587%, s.d.= 19.246%,\np= 0.027) was significantly higher than for participants in EXP1-W ( M= 56.975%,\ns.d.= 21.868%). In terms of human-AI collaboration outcomes s"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "8%). In terms of human-AI collaboration outcomes shown in Fig. 4 (d),\nthe increase of accuracy in EXP1-O ( M= 7.221%, s.d. = 7.433%, p= 0.018)\nwas significantly lower than the increase observed in EXP1-W ( M= 11 .905%,\ns.d.= 7.967%).\n2.1.3 Unconfident AI Leads to Disuse\nOn the other hand, participants in EXP1-U showed a significantly higher disuse\nrate ( M= 83 .333%, s.d.= 11 .242%, p < 0.001) compared to those in EXP1-W\n(M= 73.016%, s.d.= 10.657%) as shown in Fig. 4 (b). Additionally, the adoption\nrate for participants in EXP1-U ( M= 40 .492%, s.d.= 24 .752%, p= 0.002) was\nsignificantly lower than that for participants in EXP1-W. From the perspective of\nHuman-AI collaboration outcomes, the increase of accuracy in EXP1-U ( M= 6.508%,\ns.d. = 7.859%, p= 0.018) was significantly lower than th"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": " 7.859%, p= 0.018) was significantly lower than the increase observed in\nEXP1-W.\n2.1.4 Misuse and Disuse Further Impair Human-AI Collaboration\nOutcomes\nThe results of the mediation analysis in Fig. 6 (a) and (b) further revealed the effects\nof uncalibrated AI confidence on accuracy increase, mediated by adoption rate and\nmisuse. Utilizing data from EXP1-W and EXP1-O, we found that the misuse rate ( βi\n(indirect effect) = −6.228,p <0.001) and adoption rate ( βi= 2.854,p= 0.007) fully\nmediated the influence of AI’s overconfidence on accuracy increase ( βd(direct effect) =\n−1.309,p= 0.231). With data from EXP1-U and EXP1-W, it was discovered that the\ndisuse rate ( βi=−9.081,p <0.001) and adoption rate ( βi= 3.196,p= 0.003) fully\nmediated the effect of AI’s unconfidence on accuracy increase ( "
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "ffect of AI’s unconfidence on accuracy increase ( βd= 0.487,p= 0.612).\n2.2 Trust Calibration Support Is A Double-Edged Sword\nCombining EXP1 and EXP2, our results indicate that trust calibration support can\naid participants in perceiving AI’s uncalibrated confidence and reduce misuse when\n8\nFig. 5 One-way ANOVA results of EXP2 are depicted in violin and box plots, about the effects of\nAI’s confidence calibration states on the dependent variables: participants’ Adoption Rate, Disuse\nRate, Misuse Rate, Accuracy Increase, Trust towards AI’s Confidence Score, Trust towards AI’s Pre-\ndiction Accuracy, and Trust towards AI’s Overall Capability. The center line represents the median.\nThe box limits represent upper and lower quartiles. The whiskers represent the 1.5x interquartile\nrange. The points"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "represent the 1.5x interquartile\nrange. The points represent the outliers. The width of each violin corresponds to the frequency of\nobservations at any given number on the y axis. The cut of kernel density estimation is set to 0. The\np-values of the AI confidence calibration states’ main effects from the one-way ANOVA are indicated\nbelow each figure, and the significance levels from post-hoc analyses are denoted within the figures\n(ns:p >0.05, *: p <0.05, **: p <0.01, ***: p <0.001). F-values with degrees of freedom are also\nreported below the figure. Differences between groups from post-hoc analyses, excluding comparisons\nbetween EXP2-U and EXP2-O, are marked in the figure. Further information, such as the verification\nof homogeneity assumptions, analytical methods, and effect sizes, can "
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "ptions, analytical methods, and effect sizes, can be found in the supplementary\nmaterials.\nthe AI is overconfident. While it improves participants’ trust calibration by reducing\ntheir trust towards AI confidence and overall capability, it concurrently lowers their\ntrust towards AI’s prediction accuracy. It not only fails to address the issue of disuse\nwhen the AI is unconfident, but also facilitates disuse in cases of AI overconfidence.\nUltimately, it failed to enhance the outcomes of human-AI collaboration.\n9\nFig. 6 The diagrams above illustrates the results of mediation analysis (ns: p >0.05, *: p <0.05, **:\np <0.01, ***: p <0.001). Arrows indicate the paths of effect, while numbers represent the coefficients\non each path. See more details in supplementary materials. (a) Mediation analys"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "s in supplementary materials. (a) Mediation analysis on EXP1-W and\nEXP1-O about the effects of overconfident AI confidence on accuracy increase, mediated by adoption\nrate and misuse. (b) Mediation analysis on EXP1-U and EXP1-W about the effects of unconfident\nAI confidence on accuracy increase, mediated by adoption rate and disuse. (c) Mediation analysis\non EXP2-W and EXP2-O about the effects of overconfident AI confidence on accuracy increase,\nmediated by adoption rate and disuse. (d) Mediation analysis on EXP2-U and EXP2-W about the\neffects of unconfident AI confidence on accuracy increase, mediated by adoption rate and disuse.\n2.2.1 Trust Calibration Helps Human Perceive AI’s Uncalibrated\nConfidence\nAccording to the findings in Fig. 3 (b), our results in EXP2-U and EXP2-O reveals\nthat m"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "), our results in EXP2-U and EXP2-O reveals\nthat most participants were able to perceive AI’s overconfidence or unconfidence when\nprovided with trust calibration support. In EXP2-U, 76.190% of participants correctly\nrecognized the AI’s unconfidence, which, according to the results of the χ2test, was\nsignificantly higher than in EXP1-U ( χ2= 20.202,p <0.001). In EXP2-O, 73.810% of\nparticipants correctly identified the overconfidence, significantly higher than in EXP1-\nO (χ2= 20.261,p <0.001). In EXP2-W, the distribution of participants’ perceptions\nof the AI’s confidence calibration state showed no significant difference from EXP1-W\n(χ2= 1,798,p= 0.407).\n2.2.2 Trust Calibration Reduces Humans’ Trust towards\nUncalibrated AI\nConsidering trust calibration support as a factor, the results of on"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "calibration support as a factor, the results of one-way ANOVA\nbetween EXP1-U and EXP2-U, as well as EXP1-O and EXP2-O, indicate that trust\ncalibration support reduced participants’ trust towards AI when AI expresses uncal-\nibrated confidence. In EXP2-O, participants’ trust towards the AI’s confidence score\n(MEXP 2−O= 4.182,s.d.EXP 2−O= 1.086,MEXP 1−O= 4.921,s.d.EXP 1−O= 0.933,\np= 0.001), trust in the AI’s predictions ( MEXP 2−O= 4.420,s.d.EXP 2−O= 0.857,\n10\nMEXP 1−O= 5.215, s.d.EXP 1−O= 0.721, p < 0.001), and trust towards the AI’s\noverall capability ( MEXP 2−O= 4.205, s.d.EXP 2−O= 1.218, MEXP 1−O= 5.222,\ns.d.EXP 1−O= 0.816, p < 0.001) were all significantly lower than those of partici-\npants in EXP1-O. In EXP2-U, participants’ trust towards the AI’s confidence score\n(MEXP 2−U= 4.262,s.d.E"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "s the AI’s confidence score\n(MEXP 2−U= 4.262,s.d.EXP 2−U= 1.110,MEXP 1−U= 4.912,s.d.EXP 1−U= 0.800,\np= 0.003), trust in the AI’s predictions ( MEXP 2−U= 4.421,s.d.EXP 2−U= 0.996,\nMEXP 1−U= 4.985, s.d.EXP 1−U= 0.691, p= 0.004), and trust towards the AI’s\noverall capability ( MEXP 2−U= 4.278, s.d.EXP 2−U= 1.043, MEXP 1−U= 4.928,\ns.d.EXP 1−U= 0.802,p= 0.002) were all significantly lower than those of participants\nin EXP1-U.\nAdditionally, in EXP2-W, participants’ trust towards the AI’s confidence score\n(MEXP 2−W= 4.920,s.d.EXP 2−W= 0.911,MEXP 1−W= 5.111,s.d.EXP 1−W= 0.895,\np= 0.336), trust towards the AI’s predictions ( MEXP 2−W= 5.008,s.d.EXP 2−W=\n0.681, MEXP 1−W= 5.166, s.d.EXP 1−W= 0.804, p= 0.334), and trust towards\nthe AI’s overall capability ( MEXP 2−W= 4.913,s.d.EXP 2−W= 0.887,MEXP 1−W="
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "ity ( MEXP 2−W= 4.913,s.d.EXP 2−W= 0.887,MEXP 1−W=\n5.190,s.d.EXP 1−W= 1.087,p= 0.203) showed no significant difference from those of\nparticipants in EXP1-W.\n2.2.3 Trust Calibration Support Does Not Improve Human-AI\nCollaboration Outcomes\nOur results indicate that trust calibration support does not enhance the outcomes of\nhuman-AI collaboration. According to the results from one-way ANOVA, in EXP2-\nO, the misuse rate ( M= 31 .560%, s.d.= 19 .112%, p= 0.006) and adoption rate\n(M= 46.137%, s.d.= 21.185%, p <0.001) significantly decreased compared to EXP1-\nO, yet the disuse rate in EXP2-O ( MEXP 2−O= 80.385%, s.d.EXP 2−O= 10.787%,\nMEXP 1−O= 73.243%, s.d.EXP 1−O= 10.723%, p= 0.003) was significantly higher\nthan in EXP1-O. The accuracy increase in EXP2-O ( M= 8.015%, s.d.= 7.326%,\np= 0.623) show"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "n EXP2-O ( M= 8.015%, s.d.= 7.326%,\np= 0.623) showed no significant difference from EXP1-O. In EXP2-U, the mis-\nuse rate ( MEXP 2−U= 32 .067%, s.d.EXP 2−U= 20 .598%, MEXP 1−U= 29 .632%,\ns.d.EXP 1−U= 20 .953%, p= 0.593), disuse rate ( M= 80 .272%, s.d.= 11 .355%,\np= 0.218), adoption rate ( M= 44.859%, s.d.= 21.896%, p= 0.394), and accuracy\nincrease ( M= 6.984%, s.d.= 7.715%, p= 0.780) all showed no significant differ-\nences from EXP1-U. For EXP2-W, the misuse rate ( M= 29.064%, s.d.= 14.806%,\np= 0.786), disuse rate ( M= 72 .676%, s.d.= 10 .254%, p= 0.882), adoption rate\n(M= 58.818%, s.d.= 17.967%, p= 0.674), and accuracy increase ( M= 12.540%,\ns.d.= 7.990%, p= 0.716) all showed no significant differences from EXP1-W.\n2.3 Overconfident and Unconfident AI Still Hinder Human-AI\nCollaboration w"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "confident AI Still Hinder Human-AI\nCollaboration with Trust Calibration Support\nQuantitative results in EXP2 indicate that, while AI’s overconfidence and unconfidence\ncould be perceived by the majority of participants with the help of trust calibration\nsupport, they still struggled to adequately calibrate their trust towards AI. Uncali-\nbrated AI confidence increased participants’ distrust towards AI prediction accuracy.\n11\nMoreover, uncalibrated AI caused significant higher disuse, through which it hinder\nhuman-AI collaboration outcomes.\n2.3.1 With Trust Calibration Support, Uncalibrated AI Lead to\nDistrust\nRegarding trust towards AI, as depicted in Fig. 5 (e)-(g), our findings indicate that\nparticipants in EXP2-U and EXP2-O decreased their trust towards the AI’s confidence\nscores and ove"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "r trust towards the AI’s confidence\nscores and overall capabilities when the AI exhibited either overconfidence or uncon-\nfidence. As the main effects of AI confidence calibration states in one-way ANOVA of\nEXP2 are significant, according to post-hoc analysis results, participants in EXP2-O\nhad significantly lower trust towards AI’s confidence score ( p= 0.004) compared to\nthose in EXP2-W. Similarly, participants in EXP2-U also showed significantly lower\ntrust towards AI’s confidence score ( p= 0.012) than those in EXP2-W. Additionally,\ntrust towards the overall capability of AI for participants in EXP2-O ( p= 0.009) was\nsignificantly lower compared to those in EXP2-W, and the same trend was observed\nin EXP2-U ( p= 0.010) compared to EXP2-W. These results were expected since the\nAI in EXP2"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": ". These results were expected since the\nAI in EXP2-U and EXP2-O presented uncalibrated confidence scores.\nHowever, as trust in AI’s confidence waned, trust towards AI’s predictions accuracy\nalso declined. Notably, the accuracy of AI’s predictions remained unchanged, yet par-\nticipants developed distrust in AI’s ability to make accurate forecasts. Trust towards\nAI’s predictions of participants in EXP2-O ( p= 0.002) and EXP2-U ( p= 0.007)\nwere both significantly lower than those in EXP2-W. The results of the linear regres-\nsion analysis (details in supplementary material) revealed a positive linear correlation\nbetween participants’ trust towards AI’s predictions and their trust towards AI’s\nconfidence ( R= 0.649,p <0.001).\n2.3.2 With Trust Calibration Support, Uncalibrated AI Lead to\nDisuse\n"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "libration Support, Uncalibrated AI Lead to\nDisuse\nRegardless of collaborating with overconfident or unconfident AI, with trust calibration\nsupport, participants exhibited a significant increase in disuse rate, and a significant\ndecrease in both adoption rate and final accuracy increase. As Fig.5 (a)-(d) show,\nparticipants in EXP2-O exhibited a significantly higher disuse rate ( p= 0.004) as did\nthose in EXP2-U ( p= 0.005), compared to those in EXP2-W. The adoption rate for\nparticipants in EXP2-O ( p= 0.014) and EXP2-U ( p= 0.006) was significantly lower\nthan for those in EXP2-W. In terms of Human-AI collaboration outcomes, the increase\nin accuracy in EXP2-O ( p= 0.021) and EXP2-U ( p= 0.003) was significantly lower\nthan EXP2-W’s. There was no significant difference in misuse rate among the"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "no significant difference in misuse rate among the three\ntreatments.\nLinear regression results (details in supplementary material) revealed a connection\nbetween participants’ distrust towards AI’s predictions and their disuse behaviors:\nthe disuse rate increased as trust towards AI’s predictions decreased ( R= 0.345,\np <0.001). Although the Rvalues for these linear regressions are low due to variation\nin the data, the probability of rejecting the null hypothesis is below 0.05, indicating a\nsignificant trend.\n12\n2.3.3 Disuse Further Worsen Human-AI Collaboration Outcomes\nThe results of the mediation analysis in EXP2 were shown in Fig. 6 (c) and (d) : Using\ndata from EXP2-W and EXP2-O, we found that disuse rate ( βi=−6.969,p <0.001)\nand adoption rate ( βi= 3.041,p= 0.005) fully mediated the "
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "ion rate ( βi= 3.041,p= 0.005) fully mediated the effect of AI overconfidence\non accuracy increase ( βd=−0.596,p= 0.459). With data from EXP2-U and EXP2-\nW, disuse rate ( βi=−6.218,p= 0.002) and adoption rate ( βi= 2.383,p= 0.008) fully\nmediated the effect of AI unconfidence on accuracy increase ( βd=−1.721,p= 0.082).\n3 Discussion\nThis study unveils the detrimental impacts of uncalibrated AI confidence on human-AI\ncollaboration and the mixed effect of AI transparency. Our findings indicate that under\nconditions of insufficient transparency, individuals struggle to accurately identify AI\noverconfidence and unconfidence, assuming AI possesses accurate confidence. In this\ncase, they were unable to calibrate their trust towards AI, exhibiting an overtrust on\nthe AI’s confidence scores. Uncalib"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "n overtrust on\nthe AI’s confidence scores. Uncalibrated AI confidence then misleads individuals and\nimpaires their ability to make informed decisions when heeding AI advice. Overconfi-\ndent AI misleads individuals into accepting AI advice when AI is wrong, significantly\nincreasing misuse and thus diminishing collaboration outcomes. Conversely, unconfi-\ndent AI, by displaying confidence levels lower than its actual correctness likelihood,\nmay lead individuals to disregard AI advice when AI is correct, significantly increas-\ning disuse and ultimately worsen collaboration outcomes. Unlike previous studies that\nsolely focused on the effects of different AI confidence levels [24], this research delves\ndeeper into the influence of the calibration states of AI’s confidence scores on trust\ncalibra"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": " states of AI’s confidence scores on trust\ncalibration and human-AI collaboration. Prior research has indicated that high AI con-\nfidence scores can enhance individuals’ belief in the correctness of AI suggestions [24].\nThis study corroborates similar findings and further elucidates the negative impacts\nof uncalibrated AI confidence scores on human behavior and collaborative outcomes.\nProviding additional trust calibration support aids individuals in accurately iden-\ntifying AI’s overconfidence and unconfidence, and effectively reduces misuse behaviors\nwhen collaborating with overconfident AI. However, this leads to a loss of trust in the\nAI’s expressed confidence scores, overall capability, and accuracy of predictions. Dis-\ntrust in AI’s confidence is reasonable, as the AI indeed presents"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "onfidence is reasonable, as the AI indeed presents confidence scores that\ndo not align with reality. The distrust in the predictions, however, is concerning, given\nthat the accuracy of AI predictions does not vary across different treatment setups.\nThis align with research suggesting more transparency may reduce individuals’ trust\n[34, 36]. The finding could be explained by prior studies showing that people tend to\napply the same social rules and expectations to interactions with computers as they\ndo with humans [37, 38, 39]. According to social cognitive theory, overconfidence in\ncollaborative settings can be perceived as arrogance, while lack of confidence may be\nseen as a lack of skill or motivation [40], both of which can erode trust.\nOur findings further uncover a nuanced impact of tr"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "ur findings further uncover a nuanced impact of trust calibration support on\ncollaboration. Increased transparency, when AI exhibits overconfidence, successfully\nreduces individuals’ misuse, but leads to greater disuse. In both overconfident and\nunconfident cases, it does not improve the collaboration outcomes, contrary to our\n13\nexpectation. Overall, our results show that both overconfident and unconfident AI\ninvariably hinder the outcomes of human-AI collaboration, with individuals’ misuse\nand disuse fully mediating the impact of AI’s uncalibrated confidence on outcomes.\nWe believe that the fundamental and effective solution to the aforementioned issues\nis to provide calibrated confidence scores that can accurately estimate the correctness\nlikelihood, necessitating further development in"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "s\nlikelihood, necessitating further development in AI model calibration techniques. This\ncontinues to require persistent efforts and investments in the field of trustworthiness,\nespecially for the calibration of currently prevalent and widely used LLMs [22]. We also\nadvocate for actions to be taken from policy and regulatory perspectives to advance\nthe calibration of AI.\nHowever, we must confront the reality that, at least for the present, we continue\nto collaborate with numerous uncalibrated AIs in various scenarios. In such cases, we\npropose that, firstly, individuals need to know whether AI confidence is calibrated, rec-\nognizing the potential inaccuracies in AI confidence rather than assuming AI possesses\nwell-calibrated confidence scores. Being explicitly aware of AI’s potential short"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "es. Being explicitly aware of AI’s potential shortcom-\nings, or its error boundaries, may reduce trust but is essential for complementary\nhuman-AI collaboration. Therefore, we argue that trust calibration support, which\naids users in perceiving whether AI is calibrated, is necessary. Only then can individ-\nuals calibrate their trust towards AI and estimate a true correctness likelihood based\non the provided AI confidence scores instead of blindly trusting AI. Indeed, in such\nscenarios, not displaying AI confidence might be one approach, but this not only for-\nfeits the benefits of presenting uncertainty [19] but also loses the potential to enhance\ncollaboration outcomes through altering human subjective perceptions.\nHowever, merely enhancing transparency as a form of trust calibration supp"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "g transparency as a form of trust calibration support is\ninsufficient. Once individuals have a correct understanding of uncalibrated AI con-\nfidence, it is necessary to further restore trust in AI predictions. This depends on\nenhancing human understanding of AI confidence, clarifying the distinction and con-\nnection between AI confidence and its predictive capabilities, and minimizing the\nimpact of distrust in AI confidence on the trust in other AI capabilities, which could\nbe achieved through quantifying and visualization methods [24, 41]. Importantly, not\nonly users but also researchers need to deepen their understanding of AI confidence.\nWe advocate for future research to increasingly view confidence as an aspect of AI’s\nself-evaluation capabilities, considering humans’ trust attitudes "
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "capabilities, considering humans’ trust attitudes towards it separately,\nrather than merely as a means to express uncertainty and increase transparency. This\nwill contribute to a better understanding and development of more efficient human-AI\ncollaboration.\nOur research also emphasizes the urgent issue of the potential risks of uncalibrated\nAI confidence on human-AI collaboration. In today’s era of prevalent LLMs, this raises\na consideration: Should uncalibrated AI models be allowed to express their confidence\nunregulated, given the known potential risks? Who should bear the responsibility for\nthe various risks that arise from this? These questions call for further deliberation\namong practitioners, regulators, and policymakers. Furthermore, this also raises con-\ncerns about the potential m"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": " this also raises con-\ncerns about the potential manipulation of AI confidence to influence user behavior.\nWhile such manipulation may have good intentions, such as improving the outcomes\n14\nof human-AI collaboration by adjusting AI confidence to align with human expecta-\ntions [42, 43], the risks associated with this manipulation should not be ignored. For\nexample, in situations involving high-stakes decisions or vested interests, adjusting AI\nconfidence based on user trust could manipulate users into making choices that are\ndetrimental to themselves. Should the manipulation of AI confidence be prohibited,\nwith strict alignment between confidence and accuracy? Or should well-intentioned\nmanipulation of AI confidence be allowed, encouraging developers to calibrate AI con-\nfidence in order "
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": " developers to calibrate AI con-\nfidence in order to optimize human-AI collaboration outcomes [43]? This paper calls\nfor extensive discussion on this matter and the establishment of a standardized indus-\ntry norm. Regardless of the future answer to this question, our study’s findings indicate\nthat malicious manipulation of AI confidence could lead to significant risks and harm.\n3.1 Limitations\nThere are several limitations of our work. Beyond the most basic probabilistic expres-\nsion of AI confidence, there are other forms of representing AI confidence, such as\ncategorizing AI confidence from high to low in discrete classes, and the verbalization\nmethods commonly used in LLM research [22]. It would be valuable to explore whether\nthe impact of AI uncalibrated confidence and individuals’ per"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "of AI uncalibrated confidence and individuals’ perception of AI confidence\nquality differ under these various representations compared to the probabilistic form.\nAdditionally, the generalizability of this study is limited to the category of generic\nAI-assisted decision-making scenarios represented by city recognition. In high-risk sce-\nnarios such as in healthcare and military applications, people’s decision-making may\nlean towards conservatism based on Loss Aversion and Prospect Theory [44, 45].\nIn such contexts, individuals may exhibit behaviors different from those observed in\nour experiments. AI uncalibrated confidence might pose greater risks, and its specific\nimpacts warrant further investigation.\n4 Method\nTo understand the potential impact of overconfident and unconfident AI on indi"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "impact of overconfident and unconfident AI on individ-\nuals’ trust calibration and human-AI collaboration, we designed and conducted an\nonline randomized behavioral experiment, EXP1. To investigate the effect of trust\ncalibration support in mitigating the harms brought by overconfident and unconfi-\ndent AI, we further carried out another online randomized behavioral experiment,\nEXP2. The design and procedure of EXP1 and EXP2 are fundamentally the same,\nwith differences only in Trust Calibration Phase, which will be detailed subsequently.\nParticipants were recruited from Prolific for both experiments.\n4.1 Task Description\nFor both EXP1 and EXP2, the human-AI collaborative task was city photo recognition\ntask. This task entailed classifying images from three U.S. cities: New York, Chicago,\na"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "mages from three U.S. cities: New York, Chicago,\nand San Francisco, which captured the architectural, cultural, or geographical features\nof each city. All images were derived from publicly available datasets [46] and online\nresources, and underwent preprocessing to ensure consistency in quality and format.\n15\nOn the image dataset of this experiment, the average accuracy of participants without\nAI assistance was 65.1%. For each task, participants initially assessed the city photos\nindependently, forming a preliminary decision about their origin. The AI collaborator\nthen provided its decision along with their confidence about the decision. The AI’s\nconfidence was visually represented by a percentage figure, supplemented by a color\ngradient bar with a pointer. The position of the pointer on t"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "r with a pointer. The position of the pointer on the gradient conveyed\nthe confidence, with positions further to the right denoting higher confidence. After\nreceiving this information, participants made their final decision on the origin of the\nimage.\n4.2 Human-AI Collaboration System Implication\nTo support the experiment, an online experimental system was implemented using\ntheJavaScript framework Vue.js , with the web interface shown in Fig. 1 (b). The\nsystem embedded pre-programmed rules to assist users in the city photo recognition\ntasks, with decisions and confidence scores for each task predefined. Participants were\nannounced that they would collaborate with an AI assistant during the tasks. For\neach task, the system displayed the evaluated image and the associated choices to\nthe part"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "uated image and the associated choices to\nthe participants, and after a preliminary decision was made by the participants, it\nfurther presented the AI’s recommended decision and confidence score. The system\nwas capable of capturing participants’ preliminary and final decisions, with backend\nstorage managed using MySQL .\n4.3 Experiment Procedure\nAs shown in Fig. 1 (a), the experiment procedure was organized into four sequential\nphases: Introduction Phase, Trust Calibration Phase, Survey Phase, and Collaboration\nTask Phase. For both EXP1 and EXP2, the procedures were similar and the only\ndifference was in Trust Calibration Phase.\n4.3.1 Introduction Phase\nParticipants were initially presented with the terms of the informed consent document.\nAfter agreeing to the study terms, participants ente"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "ter agreeing to the study terms, participants entered a pre-task survey designed\nto gather demographic information and control variables. Additionally, at this stage,\nparticipants were instructed to review the introduction to task details and guidance\non system usage.\n4.3.2 Trust Calibration Phase\nFollowing the Introduction Phase, all participants underwent Trust Calibration Phase,\nwhich acted as a trust calibration stage. This phase involved 10 city recognition tasks.\nFor participants in EXP1, they completed the 10 tasks in the sequence outlined in 4.1\nwith the assistance of AI, without receiving any feedback. At this point, participants\ncalibrated their trust in AI based on the consistency between AI and their own deci-\nsions. For participants in EXP2, they received trust calibration sup"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "pants in EXP2, they received trust calibration support in this phase,\nincluding statement about the AI’s confidence calibration state (i.e., whether the AI\n16\nwas unconfident, well-calibrated or overconfident), correctness feedback for each task\nand overall accuracy feedback after completing all tasks. Participants could use such\nadditional information to further calibrate their trust.\n4.3.3 Survey Phase\nAfter the Trust Calibration Phase, participants were required to complete a survey\naimed at capturing their trust in the AI collaborator and their perception of AI’s\nconfidence calibration states. Participants were to report their perception of the AI\ncollaborator’s confidence calibration state, that is, whether they perceived their AI col-\nlaborator as unconfident, well-calibrated, or ove"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "\nlaborator as unconfident, well-calibrated, or overconfident. Additionally, they were also\nasked to report their trust towards the AI’s prediction accuracy, their trust towards the\nAI’s expressed confidence scores, and their trust towards the AI’s overall capabilities.\n4.3.4 Collaboration Task Phase\nThe last phase was designed to observe the participants’ behavior and the outcomes\nof human-AI collaboration. Participants were tasked with completing 30 city photo\nrecognition tasks. This phase did not provide any correctness feedback for both EXP1\nand EXP2, as it aimed to simulate real-world human-AI collaborative recognition tasks\nwhere immediate feedback from ground truth is not available. The adoption rates,\nmisuse rates, disuse rates, and accuracy increase of the participants were compute"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "accuracy increase of the participants were computed\nafter this phase.\n4.4 AI Confidence Calibration States\nIn both EXP1 and EXP2, there are three different AI’s confidence calibration states:\nunconfident (EXP1-U and EXP2-U), well-calibrated (EXP1-W and EXP2-W), and\noverconfident (EXP1-O and EXP2-O). The definitions of different AI’s confidence\ncalibration states depend on the relationship between the AI’s confidence level and\nits true correctness likelihood: an AI is considered unconfident when its average con-\nfidence score is lower than its average true correctness likelihood; it is considered\nwell-calibrated when its average confidence score equals the average true correctness\nlikelihood; and it is considered overconfident when its average confidence score exceeds\nthe average true corre"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "ge confidence score exceeds\nthe average true correctness likelihood.\nGiven that this experiment utilized pre-programmed tasks, where each task was\nunequivocally right or wrong with predetermined correctness, and not probabilisti-\ncally correct, the experiment adopted AI’s accuracy as an approximation of the AI’s\naverage true correctness likelihood. The accuracy of AI across all groups in EXP1 and\nEXP2 was maintained at 70% during both the Trust Calibration Phase and Collabo-\nration Task Phase. In both phases, the average confidence score of the unconfident AI\nis 60%, 10% below the approximation of average true correctness likelihood for both\nphases; the well-calibrated AI’s average confidence score is 70%, aligning with the\napproximation of average true correctness likelihood; and the over"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": " average true correctness likelihood; and the overconfident AI’s aver-\nage confidence score is 80%, 10% above the approximation of average true correctness\nlikelihood. For each specific task, the confidence score is set to fluctuate within ±15%\n17\nof the AI’s average score, and our setup guarantees that the AI’s average confidence\nscore across all tasks in each phase equals the predetermined average.\n4.5 Trust Calibration Support\nIn EXP2, to investigate whether the enhancement of transparency mitigates uncali-\nbrated AI confidence’s influences, experiments were conducted in conditions with trust\ncalibration support. Participants in EXP2 were provided with statements about the\nAI’s confidence calibration state during the Trust Calibration Phase, along with AI’s\noverall accuracy feedback and"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "ase, along with AI’s\noverall accuracy feedback and task-specific correctness feedback. These interventions\naimed to aid participants in calibrating their trust in the AI collaborator and their\nperception of its confidence calibration state.\nSpecifically, for participants in the trust calibration support treatments, they were\npresented with statements regarding the AI’s confidence calibration state prior to\ninitiating the tasks in the Trust Calibration Phase:\n•For participants collaborating with an unconfident AI, they received the informa-\ntion: ”The AI you will collaborate with is unconfident. When the AI’s confidence\nlevel is lower than the accuracy of its predictions, we term it ’unconfident.’ For\ninstance, the AI might assert a 70% confidence in its predictions, yet in reality, it\nachi"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "idence in its predictions, yet in reality, it\nachieves accuracy 90% of the time.”\n•For participants working with a well-calibrated AI, they were informed: ”The AI you\nwill collaborate with is well-calibrated. When the AI’s confidence level aligns closely\nwith the accuracy of its predictions, we describe it as ’well-calibrated.’ For example,\nthe AI might show an 80% confidence in its predictions, and it indeed reflects an\n80% accuracy rate.”\n•For those engaging with an overconfident AI, the message was: ”The AI you will\ncollaborate with is overconfident. When the AI’s confidence level exceeds the accuracy\nof its predictions, we label this ’overconfident.’ This implies that the AI frequently\nexhibits high certainty in its outputs, yet these are not as dependable or accurate as\nthe AI’s confi"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "re not as dependable or accurate as\nthe AI’s confidence might suggest. For instance, the AI could claim a 90% confidence\nin its predictions, but in reality, it is correct merely 70% of the time.”\nTo ensure that participants absorbed and comprehended the statement, they were\nobliged to spend a minimum of 10 seconds on this information page and affirm their\nunderstanding by ticking a checkbox. Then, after each task, participants immediately\nreceived feedback on the correction of their final decision for that task. After complet-\ning all the tasks in the Trust Calibration Phase, they also received feedback on the\naccuracy of AI and their final decisions.\n4.6 Measurements\nEXP1 and EXP2 assessed control variables that could influence human-AI collab-\noration tasks and participant behaviors usin"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "llab-\noration tasks and participant behaviors using scales, and evaluated the dependent\nvariables of interest through both scales and behavioral measures, as specified below:\n18\n4.6.1 Control Variables\nControl variables comprise participants’ familiarity with the cities in the photo local-\nization tasks, their general attitudes towards AI, and their self-assessed level of\nconfidence, which could influence their willingness to adopt AI suggestions and the\ntask accuracy [47, 48].\n•City Familiarity was gauged using three Likert-scale questions, with a range from\n1 (completely unfamiliar) to 5 (very familiar), to evaluate participants’ familiarity\nwith the cities of New York, Chicago, and San Francisco, respectively.\n•General Attitudes towards AI was measured through a 5-point Likert scale, de"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "AI was measured through a 5-point Likert scale, derived\nfrom General Attitudes towards Artificial Intelligence Scale [48].\n•General Self-confidence was assessed through a 5-point Likert scale, derived from\nprior studies [49], which reflects the overall self-confidence level of the participants.\n4.6.2 Dependent Variables Measured by Survey\nIn human-AI collaboration process, human trust is defined as the attitude that an AI\nwill help humans complete the task appropriately in collaboration , which directly influ-\nences whether people depend on AI [16]. This study assessed participants’ perception\nof AI confidence calibration states and their trust towards AI’s prediction accuracy,\nAI’s confidence score, and AI’s overall capability, utilizing scales in Survey Phase.\n•Perception of AI Confidenc"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "cales in Survey Phase.\n•Perception of AI Confidence Calibration States collected participants’ self-reported\nperception about the confidence calibration state of the AI they collaborated\nwith, measured by a one-choice question with choices including unconfident ,\nwell-calibrated , and overconfident .\n•Trust towards AI Prediction Accuracy captured the extent of participants’ trust in\nthe correctness of AI’s predictions during the collaboration, measured on a 7-point\nLikert scale derived from previous studies [50, 51, 52].\n•Trust towards AI Confidence Score focused on the degree to which participants\nare willing to trust the confidence scores provided by AI during the collaboration,\nmeasured through a 7-point Likert scale derived from previous studies [50, 51, 52].\n•Trust towards AI Overall "
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "s studies [50, 51, 52].\n•Trust towards AI Overall Capability evaluated the level of trust participants have in\nthe overall performance of AI during the collaboration, using a 7-point Likert scale\nderived from previous studies [53].\n4.6.3 Dependent Variables from Behavioral Measurements\nIn addition to the previous measurements of perception and trust, the Collabora-\ntion Task Phase introduced behavioral measurements to assess the influences of AI’s\noverconfidence and unconfidence on individuals’ adoption behaviors and collaboration\noutcomes.\n•Adoption Rate is defined as the percentage of tasks when a participant switched\nits final decision to AI decision in Collaboration Task Phase [19]. It represents a\nbehavioral manifestation of the participant’s trust in the AI.\n19\n•Misuse Rate is define"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "ipant’s trust in the AI.\n19\n•Misuse Rate is defined as the percentage of tasks when a participant switched its\nfinal decision to AI incorrect decision in Collaboration Task Phase. It represents the\nextent to which the participants are misusing AI.\n•Disuse Rate is defined as the percentage of tasks when a participant did not switch\nits final decision to AI’s correct prediction in Collaboration Task Phase. It represents\nthe level of disuse of AI by the participants.\n•Accuracy Increase is the difference between the accuracy of the participants’ final\ndecision after combining the AI recommendations and the accuracy of the partici-\npants’ decision before seeing the AI recommendations in Collaboration Task Phase.\nIt represents the outcome improvements brought about by human-AI collaboration.\n4.7"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "ments brought about by human-AI collaboration.\n4.7 Participants\nParticipants were recruited from the Prolific platform to partake in our experiments\nanticipated to last 25 minutes, for which they would receive financial compensation.\nWe ensured a balance between male and female participants and pre-screened them to\nconfirm their proficiency in English as their first language. To ensure the validity of the\nresults, participants who erred on multiple attention check questions were excluded.\nFollowing exclusions, we had a total of 252 participants, with 126 in each experiment.\nThe sample size was predetermined prior to the experiment. Demographic information\non gender, age, and education level is available in the supplementary materials.\n4.8 Approvals\nThis research was reviewed and approved b"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "pprovals\nThis research was reviewed and approved by the NUS School of Computing Depart-\nmental Ethics Review Committee, protocol number SOC-23-27.\n4.9 Analysis\nIn statistical analysis strategy, statistal tests, including ANOVA for scale measure-\nments and χ2test for categorical measurements, were employed to capture the\ndifference between treatments. For ANOVA’s assumption check, given that each group\nin our study consisted of 42 participants, our sample size was sufficiently large ( >30)\nfor the normality assumption [54]. To examine the homogeneity of the variances, we\nused Levene’s test in different confidence calibration state groups for each dependent\nvariable [55]. When the assumption of equal variances was satisfied ( p-value >0.05),\nwe proceeded with a standard ANOVA [56] followed b"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "we proceeded with a standard ANOVA [56] followed by Tukey’s Honest Significant\nDifference (HSD) test [57] for post hoc comparisons. If the homogeneity assumption\nwas not met, Welch’s ANOVA was used [58], coupled with the Games-Howell [59] post\nhoc test.\nOur analysis strategy also includes linear regression (LR), which models the\nrelationship between a dependent variable and independent variables. It calculates\nregression coefficients to understand the direction and magnitude of these relation-\nships. The significance of these coefficients, assessed through p-values, determines the\nreliability of the observed relationships. The p-values are calculated based on the t-\nstatistic, which emerges from the regression coefficient divided by the standard error\n20\nof this coefficient. Furthermore, w"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "ndard error\n20\nof this coefficient. Furthermore, we employed mediation analysis to uncover poten-\ntial mediating effects. Mediation analysis, conducted using the principles of Structural\nEquation Modeling (SEM) [60], allows for the assessment of not only the direct effects\nof an independent variable on a dependent variable but also the indirect effects that\noperate through one or more mediators.\nReferences\n[1] Guan, C., Qin, S., Ling, W. & Ding, G. Apparel recommendation system\nevolution: an empirical review. International Journal of Clothing Science and\nTechnology 28, 854–879 (2016).\n[2] Kotouza, M. T., Tsarouchis, S.-F., Kyprianidis, A.-C., Chrysopoulos, A. C. &\nMitkas, P. A. Maglogiannis, I., Iliadis, L. & Pimenidis, E. (eds) Towards fash-\nion recommendation: An ai system for clothing d"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "h-\nion recommendation: An ai system for clothing data retrieval and analysis . (eds\nMaglogiannis, I., Iliadis, L. & Pimenidis, E.) Artificial Intelligence Applications\nand Innovations , 433–444 (Springer International Publishing, Cham, 2020).\n[3] Loftus, T. J. et al. Artificial intelligence and surgical decision-making. JAMA\nsurgery 155, 148–158 (2020).\n[4] Jussupow, E., Spohrer, K., Heinzl, A. & Gawlitza, J. Augmenting medical diag-\nnosis decisions? an investigation into physicians’ decision-making process with\nartificial intelligence. Information Systems Research 32, 713–735 (2021).\n[5] Bjerring, J. C. & Busch, J. Artificial intelligence and patient-centered decision-\nmaking. Philosophy & Technology 34, 349–371 (2021).\n[6] Kiani, A. et al. Impact of a deep learning assistant on the histo"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": ". Impact of a deep learning assistant on the histopathologic\nclassification of liver cancer. NPJ digital medicine 3, 23 (2020).\n[7] Shanmuganathan, M. Behavioural finance in an era of artificial intelligence:\nLongitudinal case study of robo-advisors in investment decisions. Journal of\nBehavioral and Experimental Finance 27, 100297 (2020).\n[8] Mhlanga, D. Industry 4.0 in finance: the impact of artificial intelligence (ai) on\ndigital financial inclusion. International Journal of Financial Studies 8, 45 (2020).\n[9] Bu¸ cinca, Z., Malaya, M. B. & Gajos, K. Z. To trust or to think: cognitive forcing\nfunctions can reduce overreliance on ai in ai-assisted decision-making. Proceedings\nof the ACM on Human-Computer Interaction 5, 1–21 (2021).\n[10] Bansal, G. et al. Does the whole exceed its parts? t"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "nsal, G. et al. Does the whole exceed its parts? the effect of ai explanations\non complementary team performance (2021).\n[11] Bu¸ cinca, Z., Lin, P., Gajos, K. Z. & Glassman, E. L. Proxy tasks and subjective\nmeasures can be misleading in evaluating explainable ai systems (2020).\n[12] Green, B. & Chen, Y. The principles and limits of algorithm-in-the-loop decision\nmaking. Proceedings of the ACM on Human-Computer Interaction 3, 1–24 (2019).\n[13] Bu¸ cinca, Z., Malaya, M. B. & Gajos, K. Z. To trust or to think: Cognitive\nforcing functions can reduce overreliance on ai in ai-assisted decision-making.\nProceedings of the ACM on Human-Computer Interaction 5, 1–21 (2021). URL\nhttp://dx.doi.org/10.1145/3449287.\n[14] Muir, B. M. Trust between humans and machines, and the design of decision\naids. Int"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "and machines, and the design of decision\naids. International journal of man-machine studies 27, 527–539 (1987).\n21\n[15] Parasuraman, R. & Riley, V. Humans and automation: Use, misuse, disuse, abuse.\nHuman factors 39, 230–253 (1997).\n[16] Lee, J. D. & See, K. A. Trust in automation: Designing for appropriate reliance.\nHuman factors 46, 50–80 (2004).\n[17] Lee, J. D. & Moray, N. Trust, self-confidence, and operators’ adaptation\nto automation. International journal of human-computer studies 40, 153–184\n(1994).\n[18] Lai, V., Chen, C., Liao, Q. V., Smith-Renner, A. & Tan, C. Towards a sci-\nence of human-ai decision making: a survey of empirical studies. arXiv preprint\narXiv:2112.11471 (2021).\n[19] Zhang, Y., Liao, Q. V. & Bellamy, R. K. Effect of confidence and explanation on\naccuracy and trust "
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": " confidence and explanation on\naccuracy and trust calibration in ai-assisted decision making (2020).\n[20] Bhatt, U. et al. Uncertainty as a form of transparency: Measuring, communicat-\ning, and using uncertainty (2021). 2011.07586.\n[21] Liao, Q. V. & Vaughan, J. W. Ai transparency in the age of llms: A human-\ncentered research roadmap (2023). 2306.01941.\n[22] Xiong, M. et al. Can llms express their uncertainty? an empirical evaluation of\nconfidence elicitation in llms (2023). 2306.13063.\n[23] Guo, C., Pleiss, G., Sun, Y. & Weinberger, K. Q. On calibration of modern neural\nnetworks (2017).\n[24] Rechkemmer, A. & Yin, M. When confidence meets accuracy: Exploring the\neffects of multiple performance indicators on trust in machine learning models\n(2022).\n[25] Ma, S. et al. Who should i trust: Ai"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "\n(2022).\n[25] Ma, S. et al. Who should i trust: Ai or myself? leveraging human and ai cor-\nrectness likelihood to promote appropriate trust in ai-assisted decision-making\n(2023).\n[26] Ghosh, S. et al. Uncertainty quantification 360: A holistic toolkit for quantify-\ning and communicating the uncertainty of ai. arXiv preprint arXiv:2106.01410\n(2021).\n[27] Feng, S. & Boyd-Graber, J. What can ai do for me: Evaluating machine learning\ninterpretations in cooperative play (2019). 1810.09648.\n[28] Jiang, Z., Araki, J., Ding, H. & Neubig, G. How can we know when language mod-\nels know? on the calibration of language models for question answering (2021).\n2012.00955.\n[29] Zhang, J., Kailkhura, B. & Han, T. Y.-J. Mix-n-match: Ensemble and composi-\ntional methods for uncertainty calibration in deep lea"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "al methods for uncertainty calibration in deep learning (2020).\n[30] Kumar, A., Sarawagi, S. & Jain, U. Trainable calibration measures for neural\nnetworks from kernel mean embeddings (2018).\n[31] Wang, X., Liu, H., Shi, C. & Yang, C. Be confident! towards trustworthy graph\nneural networks via confidence calibration. Advances in Neural Information\nProcessing Systems 34, 23768–23779 (2021).\n[32] Fishbein, M. & Ajzen, I. Belief, attitude, intention, and behavior: An introduction\nto theory and research (1977).\n[33] Ajzen, I. & Fishbein, M. Understanding attitudes and predicting social behavior\n(p. hall, ed.) (1980).\n22\n[34] Kizilcec, R. F. How much information? effects of transparency on trust in an\nalgorithmic interface (2016).\n[35] Endsley, M. R. Supporting human-ai teams: Transparency, expl"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": ". R. Supporting human-ai teams: Transparency, explainability, and\nsituation awareness. Computers in Human Behavior 140, 107574 (2023).\n[36] Turner, A., Kaushik, M., Huang, M.-T. & Varanasi, S. Calibrating trust in\nai-assisted decision making. URL https://api.semanticscholar.org/CorpusID:\n235679554.\n[37] Nass, C., Steuer, J. & Tauber, E. R. Computers are social actors (1994). URL\nhttps://doi.org/10.1145/191666.191703.\n[38] Nass, C. & Moon, Y. Machines and mindlessness: Social responses to computers.\nJournal of Social Issues 56, 81–103 (2000). URL https://spssi.onlinelibrary.wiley.\ncom/doi/abs/10.1111/0022-4537.00153.\n[39] Reeves, B. & Nass, C. The media equation: How people treat computers,\ntelevision, and new media like real people. Cambridge, UK 10(1996).\n[40] Bandura, A. Social foundatio"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "ge, UK 10(1996).\n[40] Bandura, A. Social foundations of thought and action. Englewood Cliffs, NJ\n1986 (1986).\n[41] Fernandes, M., Walls, L., Munson, S., Hullman, J. & Kay, M. Uncertainty displays\nusing quantile dotplots or cdfs improve transit decision-making (2018).\n[42] Vodrahalli, K., Gerstenberg, T. & Zou, J. Uncalibrated models can improve\nhuman-ai collaboration (2022). 2202.05983.\n[43] Benz, N. L. C. & Rodriguez, M. G. Human-aligned calibration for ai-assisted\ndecision making (2023). 2306.00074.\n[44] Tversky, A. & Kahneman, D. Loss aversion in riskless choice: A reference-\ndependent model. The quarterly journal of economics 106, 1039–1061 (1991).\n[45] Kahneman, D. & Tversky, A. Prospect theory: An analysis of decision under risk\n(2013).\n[46] Vodrahalli, K., Daneshjou, R., Gerstenberg"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": ").\n[46] Vodrahalli, K., Daneshjou, R., Gerstenberg, T. & Zou, J. Do humans trust advice\nmore if it comes from ai? an analysis of human-ai interactions (2022).\n[47] Chong, L., Zhang, G., Goucher-Lambert, K., Kotovsky, K. & Cagan, J. Human\nconfidence in artificial intelligence and in themselves: The evolution and impact of\nconfidence on adoption of ai advice. Computers in Human Behavior 127, 107018\n(2022).\n[48] Schepman, A. & Rodway, P. The general attitudes towards artificial intelli-\ngence scale (gaais): Confirmatory validation and associations with personality,\ncorporate distrust, and general trust. International Journal of Human–Computer\nInteraction 39, 2724–2741 (2023).\n[49] Schwarzer, R. & Jerusalem, M. Generalized self-efficacy scale. J. Weinman, S.\nWright, & M. Johnston, Measures in "
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "J. Weinman, S.\nWright, & M. Johnston, Measures in health psychology: A user’s portfolio. Causal\nand control beliefs 35, 37 (1995).\n[50] Yin, M., Wortman Vaughan, J. & Wallach, H. Understanding the effect of\naccuracy on trust in machine learning models (2019).\n[51] McKnight, D. H., Choudhury, V. & Kacmar, C. Developing and validating trust\nmeasures for e-commerce: An integrative typology. Information systems research\n13, 334–359 (2002).\n[52] Madsen, M. & Gregor, S. Measuring human-computer trust (2000).\n23\n[53] Jian, J.-Y., Bisantz, A. M. & Drury, C. G. Foundations for an empirically deter-\nmined scale of trust in automated systems. International journal of cognitive\nergonomics 4, 53–71 (2000).\n[54] Ghasemi, A. & Zahediasl, S. Normality tests for statistical analysis: a guide for\nnon-statis"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "s for statistical analysis: a guide for\nnon-statisticians. International journal of endocrinology and metabolism 10, 486\n(2012).\n[55] Brown, M. B. & Forsythe, A. B. Robust tests for the equality of variances.\nJournal of the American Statistical Association 69, 364–367 (1974). URL http:\n//www.jstor.org/stable/2285659.\n[56] Fisher, R. A. Statistical methods for research workers (1970).\n[57] Tukey, J. W. Comparing individual means in the analysis of variance. Biometrics\n99–114 (1949).\n[58] Welch, B. L. On the comparison of several mean values: an alternative approach.\nBiometrika 38, 330–336 (1951).\n[59] Games, P. A. & Howell, J. F. Pairwise multiple comparison procedures with\nunequal n’s and/or variances: a monte carlo study. Journal of Educational\nStatistics 1, 113–125 (1976).\n[60] Kline, R."
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "ional\nStatistics 1, 113–125 (1976).\n[60] Kline, R. B. Principles and practice of structural equation modeling (Guilford\npublications, 2023).\n24\nAppendix A Supplementary Information\nA.1 Survey Items\nA.1.1 City Familiarity\nParticipants were asked their familiarity with the cities of New York, Chicago, and\nSan Francisco. The following items were on a Likert scale from 1 (Not familiar at all)\nto 5 (Extremely familiar).\n1.Please indicate your familiarity with New York .\n2.Please indicate your familiarity with Chicago .\n3.Please indicate your familiarity with San Francisco .\nA.1.2 General Attitudes towards AI\nThe survey probed participants’ general attitudes towards AI. Questions were derived\nfrom the General Attitudes towards Artificial Intelligence Scale (GAAIS), as developed\nby Schepman and R"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "ence Scale (GAAIS), as developed\nby Schepman and Rodway [48]. The following items were on a Likert scale from 1\n(Strongly disagree) to 5 (strongly agree).\n1. I am interested in using A systems in my daily life.\n2. Al systems can perform better than humans.\n3. Al systems can help me make a better decision.\n4. Al systems can be basically honest.\n5. Al systems can be trustworthy.\nA.1.3 General Self-confidence\nParticipants’ general self-confidence was measured by a 5-point Likert scale including\nthe following items, ranging from 1 (Strongly disagree) to 5 (strongly agree). The\nquestions were partly derived from the General Self-Efficacy Scale, as established by\nSchwarzer and Jerusalem [49].\n1.If I am in trouble, I can usually think of a solution.\n2.I can usually handle whatever comes my way.\n3"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "n.\n2.I can usually handle whatever comes my way.\n3.I can solve most problems if I invest the necessary effort.\n4.Thanks to my resourcefulness, I know how to handle unforeseen Situations.\n5.It is easy for me to stick to my aims and accomplish my goals.\nA.1.4 Perception of AI Confidence Calibration State\nParticipants were asked to report their perception of the AI’s confidence calibration\nstate in previous tasks, choosing between Unconfident ,Well-calibrated , orOverconfi-\ndent. When answering questions, participants first received descriptions of three AI\nconfidence types:\n•When the AI’s confidence level is lower than the accuracy of its predictions, we\nterm it ’unconfident.’ For instance, the AI might assert a 70% confidence in its\npredictions, yet in reality, it achieves accuracy 90% of t"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "ons, yet in reality, it achieves accuracy 90% of the time.\n25\n•When the AI’s confidence level aligns closely with the accuracy of its predictions, we\ndescribe it as ’well-calibrated.’ For example, the AI might profess an 80% confidence\nin its predictions, and it indeed reflects an 80% accuracy rate.\n•When the AI’s confidence level exceeds the accuracy of its predictions, we label this\n’overconfident.’ This implies that the AI frequently exhibits high certainty in its\noutputs, yet these are not as dependable or accurate as the AI’s confidence might\nsuggest. For instance, the AI could claim a 90% confidence in its predictions, but in\nreality, it is correct merely 70% of the time.\nThen, they were asked to assess the AI’s confidence calibration state:\n•Please assess the AI’s confidence calibra"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": " state:\n•Please assess the AI’s confidence calibration state in the previous tasks.\nA.1.5 Trust towards AI’s Prediction Accuracy\nParticipants’ trust towards AI’s prediction accuracy was measured by a 7-point Likert\nscale ranging from 1 (Strongly disagree) to 7 (strongly agree). The scale qustions are\nderived from previous studies [50, 51, 52]. Here are the questions:\n1.I trust the AI’s predictions.\n2.I am comfortable adopting the AI’s predictions.\n3.I believe the AI uses appropriate methods to reach predictions.\nA.1.6 Trust towards AI’s Confidence Scores\nParticipants’ trust towards AI’s confidence scores was measured by a 7-point Likert\nscale ranging from 1 (Strongly disagree) to 7 (strongly agree). The scale qustions are\nderived from previous studies [50, 51, 52]. Here are the questions:\n"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "ous studies [50, 51, 52]. Here are the questions:\n1.I trust the AI’s confidence scores in its predictions.\n2.I am comfortable referring to the AI’s confidence scores.\n3.I believe the AI uses reasonable methods to evaluate confidence scores.\nA.1.7 Trust towards AI’s Overall Capability\nParticipants’ trust towards AI overall capability was measured by a 7-point Likert\nscale ranging from 1 (Strongly disagree) to 7 (strongly agree). The scale qustions are\nderived from a previous study [53]. Here are the questions:\n1.I can trust the Al’s overall capability.\n2.I believe the Al is dependable.\n3.I believe the Al is reliable.\n26\nA.2 Statistical Results\nFig. A1 Demographics of the participants in different groups from EXP1 and EXP2. Values are\nproportions.\n27\nFig. A2 One-way ANOVA results for control"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": "ions.\n27\nFig. A2 One-way ANOVA results for control variables in EXP1 and EXP2. The symbols used to\ndenote significance levels are as follows: * ( p <0.05), ** ( p <0.01), and *** ( p <0.001).\n28\nFig. A3 One-way ANOVA results of EXP1 in Fig. 4 about the effects of AI confidence calibration\nstates. The symbols used to denote significance levels are as follows: * ( p <0.05), ** ( p <0.01), and\n*** (p <0.001).\n29\nFig. A4 One-way ANOVA results of EXP2 in Fig. 5 about the effects of AI confidence calibration\nstates. The symbols used to denote significance levels are as follows: * ( p <0.05), ** ( p <0.01), and\n*** (p <0.001).\n30\nFig. A5 One-way ANOVA results between EXP1 and EXP2 in Sec. 2.2.2 and Sec. 2.2.3 about the\neffects of trust calibration support. The symbols used to denote significance "
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": " support. The symbols used to denote significance levels are as follows: *\n(p <0.05), ** ( p <0.01), and *** ( p <0.001).\n31\nFig. A6 Post-hoc analysis results of EXP1 in Fig. 4 about the effects of AI confidence calibration\nstates. The symbols used to denote significance levels are as follows: * ( p <0.05), ** ( p <0.01), and\n*** (p <0.001).\nFig. A7 Post-hoc analysis results of EXP2 in Fig. 5 about the effects of AI confidence calibration\nstates. The symbols used to denote significance levels are as follows: * ( p <0.05), ** ( p <0.01), and\n*** (p <0.001).\n32\nFig. A8 McDonald’s ωreliance of scales used in the study.\nFig. A9 Mediation analysis results in Fig. 6 about the effects of uncalibrated AI confidence on\naccuracy increase, mediated by misuse/disuse rate and adoption rate. AI confiden"
  },
  {
    "arxiv_id": "2402.07632",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "chunk": " misuse/disuse rate and adoption rate. AI confidence type. The symbols\nused to denote significance levels are as follows: * ( p <0.05), ** ( p <0.01), and *** ( p <0.001).\n33\nFig. A10 Linear regression results of EXP2 in Sec. 2.3.1 and Sec. 2.3.2. The error bands represent\na 95% confidence interval. The symbols used to denote significance levels are as follows: * ( p <0.05),\n** (p <0.01), and *** ( p <0.001).\nFig. A11 χ2test results about participants’ perception of AI confidence calibration state of EXP1-\nU&EXP2-U, EXP1-W&EXP2-W, and EXP1-O&EXP2-O in Sec. 2.2.1.\n34\n"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "Supporting AI/ML Security\nWorkers through an\nAdversarial Techniques, Tools,\nand Common Knowledge\n(AI/ML ATT&CK) Framework\nMohamad Fazelnia\nGlobal Cybersecurity Institute (GCI), Rochester Institute of Technology (RIT), USA\nAhmet Okutan\nGlobal Cybersecurity Institute (GCI), Rochester Institute of Technology (RIT), USA\nMehdi Mirakhorli\nGlobal Cybersecurity Institute (GCI), Rochester Institute of Technology (RIT), USA\nAbstract —This paper focuses on supporting AI/ML Security Workers— professionals involved in\nthe development and deployment of secure AI-enabled software systems. It presents AI/ML\nAdversarial Techniques, Tools, and Common Knowledge (AI/ML ATT&CK) framework to enable\nAI/ML Security Workers intuitively to explore offensive and defensive tactics.\nINTRODUCTION In recent years, there"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "nsive tactics.\nINTRODUCTION In recent years, there has\nbeen a signiﬁcant increase in the use of artiﬁcial\nintelligence and machine learning (AI/ML) in a\nvariety of application domains, ranging from au-\ntonomous driving cars to medical diagnosis tools.\nSuch increased usage of AI/ML techniques in\nreal-world applications attracted adversaries and\ncyberattackers to exploit the weaknesses in these\ntechniques to achieve malicious goals. AI/ML\ncapability builders need to employ appropriate\nmitigation techniques during the development and\nmaintenance stages to minimize the attack surface\nof the AI-software products [1]. This is a chal-\nlenging task considering the variety of AI/ML\ntechniques, their unique vulnerabilities, and the\nemergence of new attack vectors.\nWhile the shortage of skilled infor"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "ttack vectors.\nWhile the shortage of skilled information se-\ncurity workers continues to grow, this problem isexacerbated when engineering AI-Enabled soft-\nware systems — for the simple reason that it\nrequires security workers who are AI/ML subject\nmatter experts, and demand for such expertise\ncontinues to exceed the supply. This paper aims\nto provide a framework that can be used by AI\nengineers to transform them into AI/ML Security\nWorkers .\nTo develop AI/ML-enabled systems resilient\nagainst cyberattacks, AI engineers must be aware\nof the existing attacks and their distinctive char-\nacteristics, such as the underlying techniques,\nevolving scenarios, and goals. For classical soft-\nware systems, there are extensive resources such\nas MITRE’s CWE, CAPEC and ATTCK frame-\nworks that enumerate s"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "CWE, CAPEC and ATTCK frame-\nworks that enumerate software weaknesses and\ncommon attacks [2], [3], however these frame-\nworks do not cover the security issues of AI/ML\nIEEE Security & Privacy © 2023 IEEE1arXiv:2211.05075v1  [cs.CR]  9 Nov 2022\nsystems. Having similar resources for AI en-\ngineers would enable them to become AI/ML\nSecurity Workers by guiding them in investigat-\ning potential attack scenarios against their AI\nproducts and choosing appropriate defensive mea-\nsures. While existing resources such as NIST\nTaxonomy and Terminology of Adversarial Ma-\nchine Learning [1] and other similar work [4],\n[5] provide a common language to discuss var-\nious types of attacks using an upper taxonomy,\nthey often lack details and concrete attack/de-\nfense techniques necessary to make actionable\nde"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "-\nfense techniques necessary to make actionable\ndecisions for a given AI system [6]. To ﬁll this\ngap we need to support the training of the next\ngeneration of AI/ML Security Workers, and ease\ntheir day-to-day tasks. To this end, we argue that\nit is necessary to have an intuitive framework that\nguides AI/ML Security Workers, particularly the\nnovices to ensure that cybersecurity thinking is\nexplicitly integrated in their daily AI engineering\nand decision-making activities.\nThis paper, therefore, discusses AI/ML Ad-\nversarial Techniques, Tools, and Common\nKnowledge (AI/ML ATT&CK framework) , an\nintuitive and comprehensive framework based on\na systematic literature review (SLR) that char-\nacterizes the offensive anddefensive techniques,\ntactics, and tools related to the security of AI/ML-\nenab"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": ", and tools related to the security of AI/ML-\nenabled systems. AI engineers can employ the\nAI/ML ATT&CK framework during software de-\nsign and implementation as an easy-to-use deci-\nsion support system to identify the weaknesses\nassociated with the AI models, explore viable\nattacks, and consider appropriate mitigation tech-\nniques and security measures. Our contributions\nare four-fold:\n\u000fWe provide a deﬁnition for AI/ML Security\nWorkers , describing their role as AI cyberse-\ncurity professionals and advocate individuals\nin such role on taking both offensive and\ndefensive measures.\n\u000fWe provide an extensive knowledge base of\n102attacks , 65mitigation techniques , and 105\ntools related to the security of AI/ML-enabled\nsoftware systems. This catalog is created based\non an SLR to categorize and "
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "alog is created based\non an SLR to categorize and characterize\ncurrently available attack scenarios, mitigation\ntechniques, and tools discussed in 860 papers.\n\u000fWe deﬁne an AI/ML ATT&CK framework thataims to support both offensive and defensive\nAI/ML Security Workers. The Offensive View\nis organized as a decision tree, which enumer-\nates 26 attack scenarios , and is mapped to 102\nconcrete attack techniques and existing tools\ncapable of implementing the attacks. The De-\nfensive View enumerations 25 defense scenar-\niosand is mapped to 65 mitigation techniques.\n\u000fAn online interactive version of the framework\nis provided to support AI/ML Security Work-\ners, especially less-experienced ones. A limited\nscope user study has demonstrated promising\nresults on the effectiveness of such framework.\nAll"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "esults on the effectiveness of such framework.\nAll research materials, the generated AI/ML\nATT&CK framework, developed attack mitiga-\ntion, datasets, and investigated tools are shared\nwith the community at www.aimlattack.com.\nAI/ML Security Workers\nWho are AI/ML Security Workers? We deﬁne\nthem as cybersecurity professionals [7], [8], [9]\nwho contribute to the development of secure AI-\nenabled software systems through monitoring the\ntraining and deployment of AI systems, as well\nas formulating new adversarial attack scenarios,\nmeasuring their severity, and devising novel de-\nfense strategies for the robustness and integrity\nof AI-enabled systems in different application\ndomains. AI/ML security workers are responsi-\nble for implementing, validating, justifying, and\nadvocating the required de"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "dating, justifying, and\nadvocating the required defenses and mitigation\nstrategies. While this role is not yet deﬁned in\nstandardized Work Roles such as NIST’s Work-\nforce Framework for Cybersecurity (NICE) [10],\nrecent job posts demonstrate a growth in the\ndemand for AI security engineers ,Adversarial AI\nexperts, and many other roles with similar respon-\nsibilities. According to a recent publication [11],\njust about every industry needs workers with AI\nskills as they focus on technologies to give com-\nputers the capability to think, learn, and adapt. To\nutilize AI at its optimal potential, AI engineers\nneed to have programming skills and knowledge\nin statistical learning, data modeling, data eval-\nuation, software engineering and system design,\ndistributed computing, and in general, the a"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "sign,\ndistributed computing, and in general, the ability\nfor conceptual thinking to understand how a prod-\nuct is used and how it can be used more effec-\ntively. We argue that AI/ML Security Workers, in\n2IEEE Security & Privacy\nTable 1: Review Protocol\nResearch\nQuestions(1) What are the AI/ML-speciﬁc weaknesses and vulnerabilities and how do attackers use them to exploit\nthe system and execute organized attacks?\n(2) What are the potential mitigation techniques that can be leveraged to prevent and mitigate the\nvulnerabilities and improve the system’s robustness?\nDates 2000-2021\nDatabases IEEE Xplore Digital Library, ACM Digital Library,\nSearch Criteria English, Search in title, abstract and keywords.\nSearch Keywords f(Artiﬁcial Intelligence ORMachine Learning ORDeep Learning ORNeural Networ"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "ORMachine Learning ORDeep Learning ORNeural Networks )AND (threat\nORmitigation ORadversary )g.\nInclusion/Exclusion\nCriteriaInclusion: Full Paper, Focus on Attacks and Weaknesses on AI/ML Models, Focus on Improving the\nRobustness Against Attacks; Exclusion: Not Written in English; Reports, Abstract, Ideas, Summaries\nand Discussions; Duplicated Studies.\nNumber of Papers Initial results 20321 ; After First Review Step: 3329 ; Fully synthesized papers: 860papers .\naddition to the above skills, need to be equipped\nwith offensive anddefensive thinking so they can\nidentify potential attack scenarios ,abuse cases ,\nas well as mitigation techniques as they engineer\nAI Software. AI/ML Security Workers will deal\nwith complex tasks that require creativity, critical\nthinking, complex information proces"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "ity, critical\nthinking, complex information processing and\ndecision-making. It has been demonstrated that\nsuch tasks need higher cognitive skills [11]. Un-\nlike software security workers who have access to\nmany resource such as OWASP Top 101, Security\nChecklists, Secure Coding Practices, IEEE Top\n10 Design Flaws2and more, AI/ML Security\nWorkers currently lack comprehensive resources\nto support them in decision-making. In particular,\nthere are no checklists or other frameworks that\nenumerate AI/ML weaknesses and attack scenar-\nios that can support both novices and experts\nin systematically identifying known ﬂaws in AI\nSoftware. Despite the signiﬁcance of security in\nAI software, there is a lack of academic research,\nespecially regarding any holistic guidelines to\nsupport AI/ML Security Work"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "holistic guidelines to\nsupport AI/ML Security Workers. Our work aims\nto leverage SLR with the objective of integrating\nand synthesizing existing knowledge to provide\nnew insights for AI/ML Security Workers. This\nwork aims to characterize AI/ML cybersecurity\ntechniques ,tactics , and tools in a way that can\nhelp AI/ML Security Workers better understand\nthe attack surface of the AI-powered systems,\nreason about the AI-speciﬁc vulnerabilities in the\nsystem, and employ the best defense techniques to\nmitigate the vulnerabilities. In particular, we aim\nto support two groups of AI/ML Security workers\n1OWASP Top 10: https://owasp.org/Top10/\n2IEEE Top 10 Software Security Design Flaws:\nhttps://cybersecurity.ieee.org/blog/2015/11/13/avoiding-the-\ntop-10-security-ﬂaws/who investigate the following qu"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "-10-security-ﬂaws/who investigate the following questions in their\nday-to-day activities.\nOffensive AI/ML Security Workers: What\nare the AI/ML-speciﬁc weaknesses and vulnera-\nbilities, and how do attackers use these to exploit\nthe system and execute organized attacks?\nDefensive AI/ML Security Workers: What\nare the potential mitigation techniques that can be\nleveraged to prevent and mitigate the vulnerabil-\nities and improve the system’s robustness?\nMethodology\nTo create such a comprehensive AI/ML\nATT&CK framework to serve as a guideline,\nwe performed an SLR following the guidelines\nprovided by Kitchenman [12].\nReview Protocol\nTable 1 summarizes our SLR protocol. We\nperformed the SLR in two of the most popu-\nlar computer science archives; IEEE Xplore and\nACM Digital Library.\nFirst, to ﬁnd t"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "EE Xplore and\nACM Digital Library.\nFirst, to ﬁnd the appropriate search keywords,\nwe collected an initial set of relevant works by\nmanual exploration and then searched for the\ncommon and relevant words in their titles and\nabstracts. Then, as we found new relevant papers,\nwe added them to the collection and reﬁned the\nset of keywords. Based on this procedure, we\nformed the following search query: f(Artiﬁcial\nIntelligence OR Machine Learning OR Deep\nLearning ORNeural Networks ) AND ( threat OR\nmitigation ORadversary )g. We also used other\nqueries that relied on the variations of the terms.\nThe variations consist of the words with and\nwithout the possible preﬁx and sufﬁx for each\nkeyword (e.g., adverse, adversarial, adversary,\netc.) This review was conducted on peer-reviewed\nJanuary 20233\npap"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "w was conducted on peer-reviewed\nJanuary 20233\npapers published between the years 2000 and\n2021.\nAfter extracting the papers from IEEE Xplore\nand ACM digital libraries, we documented the\nresults to prepare them for the review process.\nFrom the initial search, we extracted 20321 pa-\npers. Then, we checked the paper’s titles and\nﬁltered out the papers based on the inclusion and\nexclusion criteria represented in Table 1. After the\nﬁrst ﬁltration process, we gathered 3329 papers.\nThen, we applied the inclusion and exclusion\ncriteria on the abstracts of the remaining papers.\nAt the end of this step, we gathered 860 papers .\nFinally, we went through the papers and started\nthe detailed literature review.\nAfter reviewing the ﬁrst 10% randomly se-\nlected papers, the initial skeleton of the AI/ML\nAT"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "ected papers, the initial skeleton of the AI/ML\nATT&CK framework was formulated, consisting\nof the framework structure and the proposed\nattributes to model the cyber adversaries and se-\ncurity techniques [13], as well as the related tools\nand toolchains. Along with reviewing the papers\nand adding new techniques to the framework, we\nupdated the framework skeleton, until all the pa-\npers were reviewed. Moreover, during reviewing\neach paper, we performed a snowballing process\nto review other related works cited in the paper.\nAs a result, we present a unifying taxonomy of\nAI/ML-speciﬁc cyberattacks and cyberdefenses\nthat provides the information that can help AI/ML\ncapability builders to deploy appropriate cyberse-\ncurity tactics during system design to deliver a\nreliable and secure AI-based s"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "design to deliver a\nreliable and secure AI-based system.\nAI/ML Adversarial Techniques, Tools &\nCommon Knowledge (AI/ML ATT&CK)\nFramework\nBased on the conducted literature review and\nanalyzing the papers described in the previ-\nous section, we developed our AI/ML ATT&CK\nFramework. We organized the framework around\nthree main components: Attack ,Mitigation ,Tool,\nand two views: Offensive and Defensive . The\nthree components of our framework and their\nintended usage by AI/ML security workers is\nshown in Figure 1. The Offensive andDefensive\nviews provides the AI/ML Security Workers with\nscenario-based decision trees to identify appro-\npriate techniques and tools for their offensive/de-\nfensive tasks.\nAttacks\nMitigation\nToolsAI/ML Security Worker\nAI/ML ATT&CK\nOffensive View\nDefensive ViewInvest"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "r\nAI/ML ATT&CK\nOffensive View\nDefensive ViewInvestigate Attack Scenarios \nFind Tools to implementAttack Scenarios Investigate Defense Scenarios AI/ML Security WorkerFind Tools/Methods to Mitigate Attacks Figure 1: The framework consists of three main\ncomponents, Attacks ,Mitigations , and Tools . The\nframework connects and unites these three com-\nponents to help AI/ML Security Workers identify\nrelated cybersecurity techniques. The Offensive\nand Defensive scenarios help Security Workers\nidentify and select appropriate cybersecurity tech-\nniques.\nThe three main components of the framework\nare explained as follows:\n1)Attack: Represents a comprehensive analysis\nof the offensive techniques against AI/ML-\nenabled systems, as well as the adversary’s\ngoal, assumptions, and capabilities.\n2)Mitigati"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "’s\ngoal, assumptions, and capabilities.\n2)Mitigation: Provides a detailed analysis of\nthe defensive techniques, their effects, ap-\nproach, and applicability of offensive tech-\nniques.\n3)Tool: Illustrates tools and toolchains capable\nof providing the offensive and defensive\ntechniques for AI/ML systems.\nNext section explores each component and\ndescribes the features in detail. Current version\nof AI/ML ATT&CK framework contains more\nthan 100 attack techniques, 65 mitigation tech-\nniques, and 105 tools related to the security of\nAI/ML techniques. Each entry in this framework\nprovides a descriptive analysis of the technique as\nwell as the corresponding offensive or defensive\ntechniques. Moreover, the online framework pro-\nvides information about the dataset used in each\ntechnique and its chara"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "t the dataset used in each\ntechnique and its characteristics.\nAI/ML ATT&CK Components\nThis section provides a description of compo-\nnents, their features, and relationships.\n4IEEE Security & Privacy\n ATTACK Awareness White-Box\n Grey-Box\n Black-Box\n Tactic Evasion Source Misclassification\n Targeted Misclassification\n Source-Targeted Misclassification\n Confidence Reduction\n Exploratory Membership\n  Inference\n Model \n Inversion Pattern in \n Training Data\n Regularization\n  Coefficient\n Attribute \n Inference\n Model \n Extraction Decision \n Boundaries\n Model \n Architecture\n Model \n Functionality\n Poisoning Data Manipulation\n Label Flipping random\n intelligent\n Data Injection\n Data Reordering\n Type of \n Violation Confidentiality\n Integrity\n Availability\n Target Affected Models\n Affected Dataset\n A"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "ility\n Target Affected Models\n Affected Dataset\n Affected Application\n Domain Text\n Visual\n Audio\n Graph\n NominalFigure 2: Characterization of AI/ML-speciﬁc of-\nfensive techniques. Each leaf node of this tree,\nin the online framework, expands to concrete\noffensive techniques for further investigation.\nAttacks represents a detailed analysis of the\noffensive techniques against AI/ML-powered sys-\ntems, as shown in Figure 2. Each part of this\ncomponent is described as follows:\nAwareness: Some cyberattacks require infor-\nmation such as training data, models, and ar-\nchitecture, while other attacks can be executed\nwithout such knowledge. This feature represents\nthe level of information that is accessible to the\nadversary as follows:\n1)White-Box: The attacker has full knowledge\nof the target mode"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "The attacker has full knowledge\nof the target model and the training data.\n2)Grey-Box: The attacker has partial knowl-\nedge about the model and the training data.\n3)Black-Box: The adversary has no knowledge\nabout the model or the data.\nTactic: This component represents adver-sary’s approach, which is categorized as follows:\n1)Poisoning: The adversary tries to undermine\nthe learning process during model training\nto manipulate the system’s behavior during\ninference time. In this attack, the attacker\npoisons the dataset through the following\napproaches:\n\u000fData Manipulation: The attacker manip-\nulates the existing training samples that\nresults in wrong output during inference\ntime.\n\u000fLabel Flipping: By adjusting the label of\nthe training samples, the attacker aims to\ninterrupt the learning proce"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": " the attacker aims to\ninterrupt the learning process in two ways\nof1)random and 2)intelligent .\n\u000fData Injection: The attacker injects ma-\nlicious samples in to the training dataset\nthat corrupts the learning process.\n\u000fData Reordering: The attacker changes\nthe order of the data feeding in training\nto corrupt the learning process.\n2)Evasion: The attacker aims to fool the ML\nsystem to misbehave by modifying the le-\ngitimate input to result in wrong predictions\nduring inference time. Depending on the\ninput’s original label and the target class, this\nattack is categorized as follows [14]:\n\u000fConﬁdence Reduction: The attacker tries\nto reduced the conﬁdence scores of the\npredicted classes.\n\u000fSource Misclassiﬁcation: The adversary\ntries to change the prediction to any label\nother than the correct one"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "prediction to any label\nother than the correct one.\n\u000fTargeted Misclassiﬁcation: The attacker\naims to force the model to misclassify any\ngiven input to the speciﬁed class.\n\u000fSource-Targeted Misclassiﬁcation: The\nattacker forces the model to misclassify\na certain set of inputs to a speciﬁc class.\n3)Exploratory : The adversary aims to explore\nthe system’s private information, such as\ntraining data, learning algorithm, and the\ndecision boundaries. There are three types\nof attacks based on the stolen information:\n\u000fMembership Inference: The adversary\naims to determine whether a speciﬁc sam-\nple has contributed to the training process.\nJanuary 20235\n\u000fModel Inversion: The adversary aims to\nreconstruct the training data and the sam-\nples representing each class of the training\ndataset.\n\u000fModel Extrac"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": " each class of the training\ndataset.\n\u000fModel Extraction: The adversary aims to\nextract the model, the decision bound-\naries, or build a functionally similar\nmodel to the target model.\nEach of the above tactics is further classiﬁed\ninto more granular classes, as shown in Figure 2,\nand is accessible to AI/ML Security Workers for\ndeeper investigations available at online AI/ML\nATT&CK framework.\nType of Violation: This feature represents\nthe type of violation caused by the adversary,\nwhich can be one or combination of the following\ngroups:\n1)Conﬁdentiality: The adversary aims to ex-\ntract system’s private information.\n2)Integrity: The adversary forces the system\nto generate inaccurate results by processing\nmalicious samples.\n3)Availability: The adversary aims to corrupt\nthe normal behavior of t"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "adversary aims to corrupt\nthe normal behavior of the system for legit-\nimate inputs.\nTarget: Represents the entities that are vul-\nnerable to the attack as follows:\n1)Affected Models: Represents the ML and\nDeep Learning (DL) models that are vulner-\nable to the adversary.\n2)Affected Applications : Illustrates the real\nworld applications, services, software and\ntools that can be affected by the correspond-\ning adversary.\n3)Affected Dataset : Represents the dataset on\nwhich the attack is carried out.\n4)Domain : Represents the type of the data on\nwhich the attack is carried out, which can be\nin the form of text, visual, audio, graph, or\nnominal.\nMitigation represents a detailed analysis of the\nAI/ML-speciﬁc defensive techniques, as shown in\nFigure 3. Each component of this framework is\ndescrib"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "ure 3. Each component of this framework is\ndescribed as follows:\nTarget: Represents the corresponding attack\nand the affected data andmodel entities.\nTactic: This component classiﬁes the mitiga-\ntion technique based on the approach as follows:\n Mitigation Target Attack's type Poisoning\n Evasion\n Exploratory\n Entity Data\n Model\n Tactic Adversarial Detection Statistical-Based\n Clustering-Based\n Reject-On-\n Negative-Impact\n Inconsistent \n Prediction\n Neuron \n Activation \n Pattern\n Adversarial Removal Data Reconstruction\n Relabling\n Feature Squeezing\n Training Modification Adversarial Training\n Ensemble of Methods\n Feature Masking\n Distillation\n Gradient Masking\n Randomization\n Differential Privacy\n Homomorphic Encryption\n Timing Proactive\n ReactiveFigure 3: Categorization of the Mitigation te"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "ctiveFigure 3: Categorization of the Mitigation tech-\nniques. Each leaf node of this tree on the website\nexpands to the concrete techniques identiﬁed in\nthe literature. It can be used to explore defensive\ntechniques and understand their characteristics.\n1)Adversarial Detection: Detects malicious\nsamples to prevent evading into the system.\n2)Adversarial Removal: Removes the pertur-\nbations from the data and generates clean\ndata.\n3)Training Modiﬁcation: Modiﬁes the training\nprocess to make the model more robust\nagainst attacks.\n4)Differential Privacy: Protects the privacy of\nthe system by sharing patterns of the training\ndataset while withholding information about\nthe individuals in the training dataset.\n5)Homomorphic Encryption: Provides an en-\ncrypted form of the data for training the\nmode"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "en-\ncrypted form of the data for training the\nmodel.\nEach of the above tactics is further categorized\ninto more granular classes, as shown in Figure 3,\nand is accessible to AI/ML security Workers for\ndeeper investigations available at online AI/ML\nATT&CK framework.\n6IEEE Security & Privacy\n Tool Type Offensive Confidentiality\n Availability\n Integrity\n Defensive Respond\n Prevent\n Detect\n Availability Public\n Private\n Target Domain Text\n Audio\n Nominal\n Visual\n Graph\n Model Classical ML\n Deep Neural \n NetworksFigure 4: Tool component. It characterizes tools\nand toolchains capable of implementing the of-\nfensive and defensive techniques. On the website,\neach leaf node of this tree expands to show the\nconcrete tools and its code repository as identiﬁed\nin the literature.\nTiming: There are two "
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "dentiﬁed\nin the literature.\nTiming: There are two stages when the miti-\ngation techniques can be deployed:\n1)Proactive: The mitigation technique should\nbe deployed before or during the model\ntraining.\n2)Reactive: The mitigation technique should\nbe deployed after model training.\nTool characterizes the tools and toolchains ca-\npable of deploying offensive and defensive tech-\nniques related to the security of AI/ML systems\nas shown in Figure 4. Each part of this component\nis explained as follows:\nType: Represents whether the tool is offensive\nor defensive.\nAvailability: Illustrates whether the tool is\npublicly available or private.\nTarget: Explains the data and model entities\nthat are required to deploy the tool.\nThe AI/ML ATT&CK provides features to\ncharacterize the offensive and defensive m"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "ures to\ncharacterize the offensive and defensive method-\nologies related to the security of AI/ML tech-\nniques. The framework enables AI/ML Security\nWorkers to understand the attack surface of dif-\nferent AI/ML techniques. Moreover, it representsappropriate cyberdefense techniques to build ro-\nbust systems that withstand various threats. To\nour knowledge, this is the ﬁrst study that thor-\noughly characterizes tools and toolchains related\nto AI/ML security and can practically guide the\nAI/ML Security Workers to choose the best tools\ncapable of implementing offensive and defensive\ntactics.\nSupporting AI/ML Security Workers\nSimply providing access to a large number of\nAI/ML robustness techniques cannot fully support\nthe users in choosing the best techniques to de-\nploy. They need easy-to-use "
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "est techniques to de-\nploy. They need easy-to-use guidance to explore\nvarious techniques, identify the best approach,\nand reason about their choices. To this end, two\nadditional views, Offensive View and Defensive\nView, have been compiled in the AI/ML ATT&CK\nframework. These views provide decision trees\nrepresenting Offensive and Defensive Scenarios\nthat can guide the AI/ML Security Workers to\ninvestigate the system’s robustness from the at-\ntacker and defender perspectives.\nHow to Use AI/ML ATT&CK Framework,\nWalk-through Examples\nTo facilitate using the guidelines of AI/ML\nATT&CK framework, it is delivered as a publicly\naccessible web solution at http://www.aimlattack.\ncom.\nUsing this web solution, users can explore\nthe framework from various perspectives. The\nfollowing describes how it c"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "ous perspectives. The\nfollowing describes how it can be used to support\nOffensive and Defensive AI/ML Security Work-\ners.\nUse Case#1: AI/ML ATT&CK for Offensive\nSecurity Workers\nAn offensive AI/ML Security Worker can use\nAI/ML ATT&CK framework to determine the at-\ntack surface of an AI-enabled system and deploy\noffensive techniques to assess the system’s robust-\nness. The framework represents attack scenarios\nwhich have been developed with respect to the\ngoals of the cyberattack ,level of access to the\ntarget system , and the application domain of the\ntarget system. By following the provided compre-\nhensive step-by-step decision tree style guidance,\nas shown in Figure 5, AI/ML Security Work-\ners can identify the related offensive techniques.\nJanuary 20237\nFigure 5: Offensive View of AI/ML "
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": ".\nJanuary 20237\nFigure 5: Offensive View of AI/ML ATT&CK Framework. It provides an overview of the attack\nscenarios from the attacker’s perspective. Offensive AI/ML Security Workers follow the provided step-\nby-step guidance to identify the related attacks and the tools. In the website, the leaf nodes of this\nview are connected to the concrete attack techniques for further investigation.\nFurthermore, on the website, by selecting each\noffensive technique, the user will be directed to a\nnew page that thoroughly explains the technique’s\ndetails and represents tools capable of imple-\nmenting the technique to deploy and observe the\nsystem’s performance under such attack.Example Offensive Scenario: For a given\nsystem that utilizes AI algorithms and has been\ntrained on sensitive data, an offensiv"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "nd has been\ntrained on sensitive data, an offensive AI/ML Se-\ncurity Worker wants to determine to what extent\nthe system is robust against data breaching. Thus,\nas the ﬁrst step, by following the offensive view\n8IEEE Security & Privacy\nFigure 6: Defensive View of AI/ML ATT&CK Frame-\nwork. Defensive AI/ML Security Workers can explore\nmitigation scenarios to ﬁnd the appropriate techniques to\npractice secure AI/ML design. On the website, the leaf\nnodes of this view are connected to the concrete mitigation\ntechniques.\nJanuary 20239\nof the framework (Figure 5), the offensive AI/ML\nSecurity Worker selects the second box, “Steal\nSystem’s Private Information” . Then, in the next\nlayer, considering the attacker’s goal, which is as-\nsessing the system’s security against data breach-\ning, the AI/ML S"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "m’s security against data breach-\ning, the AI/ML Security Worker chooses “Steal-\ning Private Information about the Training Data”.\nNext, with respect to the goal, “Identify the\nParticipation of a Sample in Training Model”\ncan potentially be chosen. Finally, considering\nthe attack’s setting, the AI/ML Security Worker\ncan choose any of the nodes of interest (e.g.,\n“With access to the Model and Training Data”).\nBy selecting the leaf node, the framework will\nexpand all the related concrete techniques and\ntools.\nUse Case #2: AI/ML ATT&CK for Defensive\nAI/ML Security Workers\nThe AI/ML ATT&CK framework empowers\ndefensive AI/ML Security Workers to identify\nthe system’s attack surface, reason about the sys-\ntem vulnerabilities, and select the best mitigation\ntechniques to improve the system’s robus"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "itigation\ntechniques to improve the system’s robustness.\nConsidering the attacks on the system that AI/ML\nSecurity Workers want to mitigate, they can se-\nlect the best potential mitigation techniques and\nutilize the provided defensive tools to deploy and\nimprove the system’s robustness.\nExample Defensive Scenario: For a given\nsystem that utilizes AI algorithms and has been\ntrained on sensitive data, it has been shown that\nthe system is vulnerable to attacks that attempt to\nidentify the participation of individual samples in\nthe training process. Thus, it is required to deploy\nappropriate mitigation techniques to improve the\nrobustness against such attacks. To this end, by\nfollowing the defensive view represented in the\nframework, as shown in Figure 6, the user can\nchoose the last box, “App"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "n Figure 6, the user can\nchoose the last box, “Apply Conﬁgurations to\nthe Data and Model to Improve the Privacy” . In\nthe next layer, the user can select the ﬁrst box,\n“Protect the Privacy of the Individual Samples\nin the Training Dataset by Modifying the Data\nwhile it Provides Useful Information to Train the\nModel. ” Finally, by selecting the leaf node in the\nwebsite, the framework expands all the appro-\npriate mitigation techniques. Also, by selecting\neach technique, the user will be directed to a new\npage that thoroughly explains the details of themitigation technique, and represents tools capable\nof implementing the technique.\nUser Evaluation\nA small user study was conducted to show\nour framework’s effectiveness. First, six subject\nmatter experts (SMEs) with prior work experience\nas AI"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "er experts (SMEs) with prior work experience\nas AI engineers who were not cybersecurity pro-\nfessionals were recruited. Then, these SMEs were\ndivided into two groups, where the AI experience\nlevel of the groups was balanced based on the\nyears of experience. The groups were provided\nwith the same case study of an Object Detec-\ntion and Recognition system for an Autonomous\nVehicle. Both groups were asked to identify po-\ntential attacks and mitigation techniques across\nthe development cycle of this project. This study\nhad two parts: each user was asked to once\nplay the role of an Offensive AI/ML Security\nWorker and another time as a Defensive AI/ML\nSecurity Worker. Group #1 had access to existing\ntaxonomies in the literature ([1], [4], [5]) as\nwell as access to the internet, while Group #2\nwa"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "\nwell as access to the internet, while Group #2\nwas requested to use only the AI/ML ATT&CK\nframework. For evaluation, we calculated the\nnumber of correct answers reported by each user\ndivided by the total number of correct answers\n(ground-truth).\nThe results show that Group #2 identiﬁed not\nonly abstract attack classes (types of attacks) but\nalso concrete techniques. The response of Group\n#2 was speciﬁc, enumerating attack classes, sub-\nclasses, and then listing speciﬁc methods from\na paper. In contrast, Group #1 only identiﬁed\nvery high-level types of attacks (e.g., poisoning\nattack) and mitigation techniques and could not\nbe speciﬁc about sub-types and concrete methods.\nIn comparison, the responses from Group #1\nlacked information about a concrete method to\nimplement the attack or mitiga"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": " concrete method to\nimplement the attack or mitigate it. Group #1\nonly identiﬁed 3% of concrete attack methods\nand 5% of mitigation techniques, while Group\n#2 identiﬁed 39% of all correct and concrete\nattack methods and 50% of correct mitigation\ntechniques. Even at the abstract high-level attack\nclasses (types of attacks), Group #1 identiﬁed\n75% of attack types and 69% of mitigation types\ncorrectly, while Group #2 identiﬁed 100% of\nboth. Overall, Group #2 outperformed Group #1\nin terms of retrieved correct attacks and miti-\n10IEEE Security & Privacy\ngation. While the current results are promising,\ngiven the sample size, further studies are needed\nto understand the effectiveness of each part of this\nframework.\nLimitations and Future Work\nAll represented information in this framework\nhas bee"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": " represented information in this framework\nhas been extracted and characterized through an\nSLR of the related techniques and tools. However,\ndespite the extensive effort to curate and develop\nthe framework, it is not guaranteed that these\ntechniques and tactics are the only viable ones\nthat can be explored by AI/ML Security Workers,\nespecially with the emergence of new AI/ML\ncyberattacks.\nWe have only carried out a limited-scope\nstudy involving six AI engineers. Further work\nis required to evaluate the claims related to the\nframework’s effectiveness in supporting AI engi-\nneers with limited security experience to improve\nAI systems’ security.\nEthics\nOffensive cyberwarfare raises serious ethical\nproblems for societies. This topic is not fully ad-\ndressed by policies and regulations. Our pri"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "y ad-\ndressed by policies and regulations. Our primary\nintention in formulating the AI/ML ATT&CK\nframework and the associated offensive and de-\nfensive tools is purely educational and for en-\nhancing the robustness of AI/ML systems. We do\nnot endorse the unethical use of this framework\nand curated tools/software. Using offensive tech-\nniques requires extreme legal and ethical consid-\neration, careful determination of the perpetrators\nand victims, and reduction of collateral damages.\nREFERENCES\n1. E. Tabassi, K. J. Burns, M. Hadjimichael, A. D. Molina-\nMarkham, and J. T. Sexton, “A taxonomy and termi-\nnology of adversarial machine learning,” https://nvlpubs.\nnist.gov/nistpubs/ir/2019/NIST.IR.8269-draft.pdf, pp. 1–\n29, 2019.\n2. M. ATT&CK, “MITRE ATT&CK,” https://attack.mitre.org/,\n2021.\n3. M"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "TRE ATT&CK,” https://attack.mitre.org/,\n2021.\n3. MITRE, “Common weakness enumeration,” https://cwe.\nmitre.org/, 2021.\n4. L. Huang, A. D. Joseph, B. Nelson, B. I. Rubinstein,\nand J. D. Tygar, “Adversarial machine learning,” in\nProceedings of the 4th ACM Workshop on Security\nand Artiﬁcial Intelligence , ser. AISec ’11. New Y ork,NY , USA: Association for Computing Machinery, 2011,\np. 43–58. [Online]. Available: https://doi.org/10.1145/\n2046684.2046692\n5. K. Sadeghi, A. Banerjee, and S. K. S. Gupta, “A system-\ndriven taxonomy of attacks and defenses in adversar-\nial machine learning,” IEEE Transactions on Emerging\nTopics in Computational Intelligence , vol. 4, no. 4, pp.\n450–467, 2020.\n6. J. M. Spring, A. Galyardt, A. D. Householder, and\nN. VanHoudnos, “On managing vulnerabilities in ai/ml\nsy"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "nHoudnos, “On managing vulnerabilities in ai/ml\nsystems,” in New Security Paradigms Workshop 2020 ,\nser. NSPW ’20. New Y ork, NY , USA: Association\nfor Computing Machinery, 2020, p. 111–126. [Online].\nAvailable: https://doi.org/10.1145/3442167.3442177\n7. J. Haney and W. Lutters, “Cybersecurity advocates: Dis-\ncovering the characteristics and skills of an emergent\nrole,” Information & Computer Security , vol. ahead-of-\nprint, 03 2021.\n8. S. Jia, X. Liu, P . Zhao, C. Liu, L. Sun, and T. Peng,\n“Representation of job-skill in artiﬁcial intelligence with\nknowledge graph analysis,” in 2018 IEEE Symposium\non Product Compliance Engineering - Asia (ISPCE-\nCN), 2018, pp. 1–6.\n9. S. Amershi, A. Begel, C. Bird, R. DeLine, H. Gall,\nE. Kamar, N. Nagappan, B. Nushi, and T. Zimmermann,\n“Software engineeri"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": ", B. Nushi, and T. Zimmermann,\n“Software engineering for machine learning: A case\nstudy,” in 2019 IEEE/ACM 41st International Confer-\nence on Software Engineering: Software Engineering in\nPractice (ICSE-SEIP) , 2019, pp. 291–300.\n10. R. Petersen, D. Santos, M. Smith, K. Wetzel,\nand G. Witte, “Nist special publication 800-181\nrevision 1: Workforce framework for cybersecu-\nrity (nice framework),” https://nvlpubs.nist.gov/nistpubs/\nSpecialPublications/NIST.SP .800-181r1.pdf, 2020.\n11. J. Bughin, E. Hazan, S. Lund, P . Dahlstr ¨om,\nA. Wiesinger, and A. Subramaniam, “Skill shift:\nAutomation and the future of the workforce,” McKinsey\nGlobal Institute , vol. 1, pp. 3–84, 2018.\n12. B. Kitchenham, “Procedures for performing systematic\nreviews,” Keele, UK, Keele University , vol. 33, no. 2004,\npp. 1"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "e, UK, Keele University , vol. 33, no. 2004,\npp. 1–26, 2004.\n13. M. Fazelnia, I. Khokhlov, and M. Mirakhorli, “The\nlimitations of deep learning in adversarial settings,”\ninIn 9th International Conference on Learning\nRepresentation, RobustML work- shop, (ICLR), Vienna,\nAustria (Virtual) , 2021. [Online]. Available: https:\n//arxiv.org/abs/2202.09465\n14. N. Papernot, P . McDaniel, S. Jha, M. Fredrikson, Z. B.\nCelik, and A. Swami, “The limitations of deep learning\nin adversarial settings,” in 2016 IEEE European Sym-\nJanuary 202311\nposium on Security and Privacy (EuroS&P) , 2016, pp.\n372–387.\nMohamad Fazelnia is currently a Ph.D. student\nat Global Cybersecurity Institute (GCI), Rochester\nInstitute of Technology. His research interest is in the\narea of AI Security, Human Aspects of Cybersecurity"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "rea of AI Security, Human Aspects of Cybersecurity\nand Software Development. He is a member of the\nIEEE and the IEEE Computer Society.\nAhmet Okutan is a Senior AI Engineer at Global\nCybersecurity Institute (GCI), Rochester Institute of\nTechnology. He hold a Ph.D. in Computer Science\nfrom Is ¸ık ¨Universitesi, Turkey. He has over 20 years\nexperience designing, developing and deploying com-\nplex software systems. His research and development\ninterest is on the overlap of AI/ML and cybersecurity.\nMehdi Mirakhorli, (Member, IEEE) is currently an\nassociate professor and Kodak Endowed Chair at\nGlobal Cybersecurity Institute (GCI) and Software\nEngineering Department at Rochester Institute of\nTechnology. He received his Ph.D. degree in Com-\nputer Science from DePaul University, in Chicago. His\nres"
  },
  {
    "arxiv_id": "2211.05075",
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques,\n  Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "chunk": "cience from DePaul University, in Chicago. His\nresearch interests are in the area of cybersecurity,\nsoftware assurance and artiﬁcial intelligence. He is\nthe Associate Editor for IEEE Transaction on Software\nEngineering (TSE) and International Journal on Em-\npirical Software Engineering (EMSE). Contact him at\nmehdi.mirakhorli@rit.edu.\n12IEEE Security & Privacy\n"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "Noname manuscript No.\n(will be inserted by the editor)\nNavigating Fairness: Practitioners’ Understanding,\nChallenges, and Strategies in AI/ML Development\nAastha Pant* ·Rashina Hoda ·Chakkrit\nTantithamthavorn ·Burak Turhan\nReceived: date / Accepted: date\nAbstract The rise in the use of AI/ML applications across industries has\nsparked more discussions about the fairness of AI/ML in recent times. While\nprior research on the fairness of AI/ML exists, there is a lack of empirical\nstudies focused on understanding the perspectives and experiences of AI prac-\ntitioners in developing a fair AI/ML system. Understanding AI practitioners’\nperspectives and experiences on the fairness of AI/ML systems is important\nbecause they are directly involved in its development and deployment and their\ninsights ca"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "s development and deployment and their\ninsights can offer valuable real-world perspectives on the challenges associated\nwith ensuring fairness in AI/ML systems. We conducted semi-structured inter-\nviews with 22 AI practitioners to investigate their understanding of what a ‘fair\nAI/ML’ is, the challenges they face in developing a fair AI/ML system, the\nconsequences of developing an unfair AI/ML system, and the strategies they\nemploy to ensure AI/ML system fairness. We developed a framework showcas-\ning the relationship between AI practitioners’ understanding of ‘fair AI/ML’\nsystem and (i) their challenges in its development, (ii) the consequences of de-\nA. Pant\nDepartment of Software Systems and Cybersecurity, Monash University, Melbourne, Aus-\ntralia\nE-mail: aastha.pant@monash.edu\nR. Hoda\n"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "us-\ntralia\nE-mail: aastha.pant@monash.edu\nR. Hoda\nDepartment of Software Systems and Cybersecurity, Monash University, Melbourne, Aus-\ntralia\nE-mail: rashina.hoda@monash.edu\nC. Tantithamthavorn\nDepartment of Software Systems and Cybersecurity, Monash University, Melbourne, Aus-\ntralia\nE-mail: chakkrit@monash.edu\nB. Turhan\nFaculty of Information Technology and Electrical Engineering, University of Oulu, Oulu,\nFinland\nE-mail: burak.turhan@oulu.fiarXiv:2403.15481v2  [cs.CY]  31 Jul 2024\n2\nveloping an unfair AI/ML system, and (iii) strategies used to ensure AI/ML\nsystem fairness. By exploring AI practitioners’ perspectives and experiences,\nthis study provides actionable insights to enhance AI/ML fairness, which may\npromote fairer systems, reduce bias, and foster public trust in AI technologies"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "e bias, and foster public trust in AI technologies.\nAdditionally, we also identify areas for further investigation and offer recom-\nmendations to aid AI practitioners and AI companies in navigating fairness.\nKeywords artificial intelligence ·machine learning ·AI fairness ·AI\npractitioners ·interviews\n1 Introduction\nIn recent years, the use of AI/ML systems has become widespread across var-\nious domains, including recruitment, legal proceedings, credit risk forecasting,\nand admission processes (Mehrabi et al., 2021). ‘Fairness’ has been a subject of\nstudy in Software Engineering (SE) research for some time, predating the re-\ncent surge in AI/ML applications (Finkelstein et al., 2008). At the same time,\nthe importance of ‘fairness’ of AI/ML systems has been highlighted by several\nreal-world "
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "ystems has been highlighted by several\nreal-world incidents in recent years (Majumder et al., 2023). For example, there\nhave been fairness issues in AI/ML systems such as Google’s ML algorithm\nexhibiting gender bias against women by more frequently associating men with\nScience, Technology, Engineering, and Mathematics (STEM) careers (Prates\net al., 2020); Amazon’s AI-powered recruitment tool that was gender-biased\nas it preferred male candidates over female candidates based on their resumes\n(Martin, 2018); a risk score predicting algorithm exhibiting significant bias\nagainst African Americans, revealing a higher error rate in predicting future\ncriminals (Angwin et al., 2016); gender bias in Google (Caliskan et al., 2017)\nandBing translators (Johnson and Brun, 2022). Widespread cases of sof"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": " (Johnson and Brun, 2022). Widespread cases of software\ndisplaying unfair behavior, particularly regarding protected attributes such\nas gender (Caliskan et al., 2017) and race (Angwin et al., 2016), underscore\nthe necessity of prioritising ‘fairness’ in the development of AI/ML systems, as\nthese instances lead to unacceptable consequences disproportionately affecting\nusers in minority or historically disadvantaged groups.\nThe widespread adoption of AI/ML systems across different domains has\nraised concerns about fairness, leading to increased research and the develop-\nment of guidelines and policies. Major tech companies like Google (Google,\n2022), Microsoft (Microsoft, 2024a), IBM (IBM, 2022), and various countries/\ncontinents, including Australia (Australia, 2019) and Europe (Group, 2019"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "ustralia (Australia, 2019) and Europe (Group, 2019),\nhave defined ‘fairness’ as a guiding principle for AI practitioners in developing\na fair AI/ML system. The essence of the ‘fairness’ principle for these coun-\ntries/continents and tech companies is centered around developing an inclu-\nsive AI/ML system that does not discriminate against any specific individuals,\ngroups, or communities. Along with that, several software and tools have also\nbeen developed such as IBM’s AI Fairness 360 (IBM, 2024b), LinkedIn’s Fair-\nness Toolkit (LiFT) (Vasudevan and Kenthapadi, 2020), and fairness checklists\nlike Deon (DrivenData, 2024), Microsoft’s AI Fairness Checklist (Microsoft,\n3\n2024b), IBM’s AI FactSheets (IBM, 2024a) and many more to aid AI practi-\ntioners in developing a fair AI/ML system. The ext"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "tioners in developing a fair AI/ML system. The extensive research in the field\nof AI/ML system fairness covers various aspects, including the proposal of\nmethods and frameworks (Johnson and Brun, 2022; Zhang et al., 2023), aimed\nat aiding AI practitioners in the design and development of a fair AI/ML sys-\ntem or mitigating fairness-related issues in them. Despite the development of\nnumerous tools, frameworks, guidelines, and policies for AI/ML fairness, is-\nsues persist. Our recent survey study also showed that most AI practitioners\ndiscussed facing challenges in developing fairAI/ML systems because of their\nown biased nature (Pant et al., 2023).\nThe predominant focus has been on introducing guidelines, and policies,\nand developing tools for AI practitioners to enhance the development of a"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "r AI practitioners to enhance the development of a fair\nAI/ML system. Given that human society is diverse in terms of cultures, ex-\nperiences, and viewpoints, AI teams must reflect this diversity to effectively\ncreate fair and impactful technologies (Xavier, 2024). Therefore, understand-\ning the perspectives and experiences of these practitioners who are actively\ninvolved in AI/ML system development is equally crucial. This deeper un-\nderstanding can play a pivotal role in uncovering real-world challenges en-\ncountered during the development process. Such awareness can help to devise\nsolutions that can directly address practical needs and concerns identified by\npractitioners, thereby aiding in the development of fair AI/ML systems and\nmitigating societal inequalities (Holstein et al., 2019"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "ating societal inequalities (Holstein et al., 2019). Understanding what\n‘fairness’ means in the context of AI/ML from practitioners’ perspectives may\nhelp policymakers create better regulations that tackle real-world issues and\npromote ethical AI deployment. This approach may enhance inclusivity, re-\nduce discrimination risks, and boost public trust in AI systems (Dankloff et al.,\n2024). Ultimately, it creates a digital environment where AI enhances societal\nwell-being. A recent study has also reported that most studies on AI/ML sys-\ntem fairness are conceptual and focused on technical aspects, highlighting the\nimportance and need for research on the social/human aspects of AI (Xivuri\nand Twinomurinzi, 2021).\nTherefore, considering the importance of understanding the overall per-\nspectives"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "rtance of understanding the overall per-\nspectives and experiences of AI practitioners in the development of a fair\nAI/ML system, as emphasised in the literature, and taking into account the\nidentified research gap (Xivuri and Twinomurinzi, 2021), we were interested\nin addressing this gap by conducting an empirical study with AI practition-\ners1. We conducted semi-structured interviews with 22 AI practitioners to\nexplore four aspects: (i) AI practitioners’ understanding of ‘fair AI/ML’, (ii)\ntheir challenges in fair AI/ML development, (iii) consequences of developing\n1The term ‘AI practitioners’ in our study includes AI/ML developers, AI engineers,\nAI/ML experts, and AI/ML/ data scientists involved in the design and development ac-\ntivities of AI/ML systems. The terms ‘AI practitioners’ an"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": " of AI/ML systems. The terms ‘AI practitioners’ and ‘practitioners’ are used inter-\nchangeably throughout our study.\n4\nan unfair AI/ML system, and (iv) their strategies2to ensure the fairness of an\nAI/ML system. The study aims to answer the following four research questions\n(RQs):\nRQ1. What do AI practitioners understand by ‘fair AI/ML’?\nTo address RQ1, we explicitly asked AI practitioners about their understand-\ning of ‘fair AI/ML’. This approach was chosen to investigate how ‘fairness’ is\nunderstood by AI practitioners in the context of AI/ML.\nRQ2. What challenges do AI practitioners face in developing a\nfair AI/ML system and what are the factors that lead to those\nchallenges?\nTo address RQ2, we inquired with AI practitioners about the overall challenges\nthey encounter in developing a fa"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "erall challenges\nthey encounter in developing a fair AI/ML system, drawing insights from their\nexperiences. Additionally, we explored the underlying factors contributing to\nthose challenges.\nRQ3. What do AI practitioners perceive as the consequences of\ndeveloping an unfair AI/ML system?\nTo address RQ3, we asked AI practitioners to share their perceptions of the\nconsequences associated with developing an unfair AI/ML system. The ques-\ntion went beyond inquiring about their experiences, and also seeking their\noverall perspective on the consequences of developing an unfair AI/ML sys-\ntem.\nRQ4. What strategies do AI practitioners use in ensuring the\nfairness of an AI/ML system?\nTo address RQ4, we asked AI practitioners about their practical, day-to-day\napproaches derived from their experience "
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "y-to-day\napproaches derived from their experience in ensuring the fairness of the AI/ML\nsystem they develop.\nWe used Socio-Technical Grounded Theory (STGT) for data analysis (Hoda,\n2021) to analyse the qualitative data. The main contributions of this study\nare:\n–We investigated what AI practitioners understand by ‘fair AI/ML’.\n–We identified the challenges faced by AI practitioners in developing a fair\nAI/ML system and the factors leading to those challenges.\n–We identified the consequences of developing an unfair AI/ML system per-\nceived by AI practitioners.\n–We explored the strategies used by AI practitioners to ensure the fairness\nof the AI/ML system they developed.\n–We developed a framework illustrating the relationship between AI prac-\ntitioners’ understanding of ‘fair AI/ML’ and (i) "
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "\ntitioners’ understanding of ‘fair AI/ML’ and (i) challenges faced in devel-\noping a fair AI/ML system, (ii) the consequences of developing an unfair\nAI/ML system, and (iii) strategies for ensuring the fairness of an AI/ML\nsystem.\n2The term ‘strategy’ in our study refers to practical, day-to-day approaches aimed at\nensuring the fairness of AI/ML, rather than encompassing a broader, overarching plan or\napproach intended for achieving long-term goals.\n5\n–We formulated a set of recommendations for AI practitioners and AI com-\npanies to assist them in the development of fair AI/ML systems based on\nthe empirical findings.\n2 Background and Motivation\n2.1 Definition and Approaches on ‘AI/ML fairness’\nIn recent years, the concept of ‘fairness’ in AI has gained significant atten-\ntion. Leading soft"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "I has gained significant atten-\ntion. Leading software companies such as Microsoft, Google, and IBM have\neither outlined principles or recommended practices to guide practitioners in\ndeveloping fair AI systems. For instance, Microsoft has defined ‘fairness’ as\n“AI systems should treat all people fairly” (Microsoft, 2024a). Likewise, IBM\nemphasises the importance of minimising bias and promoting inclusive rep-\nresentation in AI development (IBM, 2022). Meanwhile, Google recommends\nconcrete steps for fair AI, including setting clear goals for fairness, using rep-\nresentative datasets, checking systems for unfair biases, and analysing sys-\ntem performance (Google, 2022). In addition to companies, various countries\nand continents have their definitions of the term ‘fairness’ in the context of\n"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "initions of the term ‘fairness’ in the context of\nAI. For example, Australia’s AI Ethics Principles defined the ‘fairness’ prin-\nciple as “AI systems should be inclusive and accessible, and should not in-\nvolve or result in unfair discrimination against individuals, communities or\ngroups” (Australia, 2019). Similarly, the European Commission defined ‘Di-\nversity, non-discrimination and fairness’ in AI as, “Unfair bias must be avoided,\nas it could have multiple negative implications, from the marginalisation of vul-\nnerable groups to the exacerbation of prejudice and discrimination. Fostering\ndiversity, AI systems should be accessible to all, regardless of any disability,\nand involve relevant stakeholders throughout their entire life circle” (Group,\n2019).\nAlong with that, over the last 13 "
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": " (Group,\n2019).\nAlong with that, over the last 13 years, there has been extensive research\non AI/ML fairness (Friedler et al., 2019), and different tools, techniques, and\nmethods to measure and mitigate fairness issues in AI/ML systems have been\ndeveloped and evaluated. Major tech companies like Microsoft, Google, and\nIBM have developed software tools and techniques to enhance the development\nof fair AI/ML systems such as AI Fairness 360 (IBM, 2024b), LinkedIn’s Fair-\nness Toolkit (LiFT) (Vasudevan and Kenthapadi, 2020), and fairness checklists\nlike Deon (DrivenData, 2024), Microsoft’s AI Fairness Checklist (Microsoft,\n2024b), IBM’s AI FactSheets (IBM, 2024a). Furthermore, researchers have\ndeveloped a variety of methods and frameworks intending to enhance the de-\nvelopment of fair AI/ML sy"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "ding to enhance the de-\nvelopment of fair AI/ML systems, including fairness checklists (Madaio et al.,\n2020), frameworks (Vasudevan and Kenthapadi, 2020; D’Amour et al., 2020),\nand fairness evaluation and comparison toolkit (Johnson and Brun, 2022).\n6\n2.2 Review studies on AI/ML fairness\nIn addition to defining the concept of AI fairness , several review studies have\nbeen conducted in the area of fairness of the AI/ML systems. For example,\nstudies have been conducted to explore and review the definition of fairness\nfocused on various aspects such as ML algorithmic classification (Verma and\nRubin, 2018), the widely used definition in ML (Mehrabi et al., 2021; Choulde-\nchova and Roth, 2018) and political philosophy (Binns, 2018). Likewise, stud-\nies have also been conducted to compare the hi"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "ud-\nies have also been conducted to compare the historical and current perspec-\ntives of fairness in ML (Hutchinson and Mitchell, 2019). Studies have also\nfocused on reviewing the challenges and methodologies related to AI fairness.\nFor example, Chen et al. (2023) conducted a literature review of 59 articles\nto explore the challenges in ensuring AI fairness and the strategies to improve\nfairness in AI systems. Likewise, Xivuri and Twinomurinzi (2021) performed\na systematic literature review (SLR) with 47 articles, examining AI algorithm\nfairness across research methods, practices, sectors, and locations. Their find-\nings revealed a predominance of conceptual research, primarily emphasising\nthe technical aspects of narrow AI, and highlighted a notable gap in research,\nspecifically the lack "
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": " a notable gap in research,\nspecifically the lack of research on the social and human aspects of AI. Pes-\nsach and Shmueli (2022) conducted a review study on ML fairness focusing\non exploring the causes of algorithmic bias, common definition, and measures\nof fairness. Caton and Haas (2020) conducted a review study to provide an\noverview of different approaches used to increase the fairness of ML systems.\nPagano et al. (2023) conducted a systematic review to explore various aspects\nlike datasets, fairness metrics, tools, and identification and mitigation meth-\nods of mitigating bias and unfairness in ML systems. Likewise, Bacelar (2021)\nprovided an overview of various measurement methods of bias and fairness\nin ML models, in their review study. Wan et al. (2023) provided a review of\nthe cur"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "dy. Wan et al. (2023) provided a review of\nthe currently available mitigation techniques of in-procession fairness issues\nin ML models. Review studies have also been conducted to address the fair-\nness issues, the causes of biases in AI, and their consequences in the medical\ndomain (Ueda et al., 2024). Likewise, Wang et al. (2023) conducted a review\nof 95 articles to explore similarities, and differences in the understanding of\nfairness, influencing factors, and potential solutions for fairness integration in\nmedical AI.\nThe emphasis of major tech companies and nations has largely been on\nworking to define the concept of ‘fairness’ and develop diverse tools and tech-\nniques to assist AI practitioners in enhancing the development of fair AI/ML\nsystems. Despite these efforts, our recent surv"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "ML\nsystems. Despite these efforts, our recent survey study showed that most par-\nticipants reported the challenges in developing fair AI/ML systems due to\ntheir own biased nature (Pant et al., 2023). Given that most studies on AI/ML\nfairness are conceptual and focused on technical aspects, and considering the\nhighlighted importance and need for research on the social/human aspects of\nAI in the literature (Xivuri and Twinomurinzi, 2021), we were interested in\nexploring the perceptions and experiences of AI practitioners regarding fair\nAI/ML systems. Investigating AI practitioners’ perceptions and experiences\n7\nin developing a fair AI/ML system can assist in understanding the real-world\nchallenges associated with fair AI/ML system development. Furthermore, it\ncan aid in devising solutions to"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": ". Furthermore, it\ncan aid in devising solutions to address their practical needs and concerns in\ndeveloping a fair AI/ML system.\n3 Research Methodology\nOur study aimed to investigate the perspectives and experiences of AI prac-\ntitioners in developing a fair AI/ML system. Figure 1 shows the overview of\nthe research methodology of our study.\n3.1 Study Design\nFig. 1 Overview of the research methodology of our study\nWe conducted a semi-structured interview-based study which commonly\nallows researchers to study the complexities of human behavior such as mo-\ntivation, communication, and understanding to obtain rich and informative\nresults (Seaman, 1999). We conducted semi-structured interviews focusing on\nAI/ML fairness and the findings are divided into two primary categories. A\nsmaller part of"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "ded into two primary categories. A\nsmaller part of the findings which revolves around AI practitioners’ perspec-\ntives and experiences on ‘AI/ML bias’, has been accepted for publication in\nIEEE Software (Pant et al., 2024). The larger part (core part) of the findings\nrelated to ‘fair AI/ML’, is presented in this paper. The complete interview\nprotocol is provided in Appendix A.\nBasically, we gathered AI practitioners’ insights on their understanding of\n‘fair AI/ML’, the challenges they face in developing a fair AI/ML system, con-\nsequences of developing an unfair AI/ML system perceived by them, and the\nstrategies they take to ensure fairness of an AI/ML system. Interview plan-\nning spanned from July 2023 to October 2023. Throughout this period, tasks\nincluded defining interview objectives, "
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "od, tasks\nincluded defining interview objectives, refining the interview protocol through\niterative processes, and prioritising crucial interview questions. Consequently,\na semi-structured interview protocol with two sections was developed.\n8\n3.1.1 Participant Information\nThe first section of the interview protocol was formulated to gather partic-\nipants’ demographic information, including their name, email, gender, age,\ncountry of residence, and educational qualifications. Employment details such\nas job titles, and involvement in AI/ML system development activities were\nalso collected. We used a pre-interview questionnaire to gather the partici-\npants’ demographic information. Participants were also asked to provide de-\ntails of their work experience in the area of AI/ML system developmen"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": " experience in the area of AI/ML system development, and\nthose without experience were not included in the study. Each participant in-\ncluded in our study has at least some experience in the area of AI/ML system\ndevelopment. Using the Qualtrics platform, we created the pre-interview ques-\ntionnaire and advertised it as an anonymous survey link following the receipt\nof necessary ethics approval (Reference Number: 38991). The pre-interview\nquestionnaire can be found in Appendix A - Section A.\n3.1.2 Understanding Participants’ Perspectives and Experiences in\nDeveloping a Fair AI/ML System\nThe second section of the interview protocol was designed to gather insights\ninto participants’ perspectives and experiences in the development of a fair\nAI/ML system. At the start of the interview, we asked"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "ML system. At the start of the interview, we asked participants if they\nwere familiar with the term ‘fair AI/ML’, and if they had encountered any\nfairness-related cases while developing AI/ML systems. Only those who had\nexperience with fairness-related cases in their professional work were recruited\nfor the interview.\nOur focus was specifically on investigating AI practitioners’ understanding\nof ‘fair AI/ML’. We did not provide a predefined definition of ‘fairness’ to par-\nticipants and explicitly inquired about their understanding, aiming to assess\ntheir perspectives independently. The two key reasons for this design choice\ninclude: (i) as mentioned in section 2, there is no universal definition of ‘fair-\nness’ in AI—different countries and tech companies have their own definitions\nof ‘fa"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "d tech companies have their own definitions\nof ‘fairness’ and (ii) this approach aimed to evaluate participants’ natural\ninterpretations, avoiding influence from a predetermined definition. We also\naimed at identifying their challenges in developing fair AI/ML and the factors\nleading to those challenges, understanding the consequences of developing an\nunfair AI/ML system from the participants’ perspective, and exploring the\nstrategies they employ to ensure fairness of an AI/ML system. To ensure we\ncaptured real-world experiences, we asked participants for real-world examples\nand experiences during the interview. The interview questions can be found\nin Appendix A - Section B.\n3.1.3 Pilot study\nAfter designing the interview protocol, we executed a pilot study, engaging two\nAI practitioners—o"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "ted a pilot study, engaging two\nAI practitioners—one from industry and another from academia—identified\n9\nthrough our professional networks. The purpose was to confirm the clarity\nand understandability of the interview questions, assess the time required to\ncomplete the study and gather feedback for enhancing the interview process.\nBoth participants possessed expertise in AI/ML system development. Taking\ninto account their feedback, we made slight modifications to the interview\nquestions to enhance clarity, ultimately finalising the interview protocol.\n3.2 Interview Sampling and Data Collection\nWe used purposive sampling in our study to select the participants (Baltes\nand Ralph, 2022). By using this method, we were able to specifically target\nour desired group of participants, namely AI pr"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "et\nour desired group of participants, namely AI practitioners involved in AI/ML\nsystem development activities.\nWe conducted data collection in two rounds. In the first round, participa-\ntion was voluntary. After we got the ethics approval, we advertised our study\non social media platforms such as LinkedIn and Twitter, as well as within\nour professional networks. We specifically targeted AI practitioners engaged\nin AI/ML system development activities. In the first round, we received inter-\nest from only 3 candidates for participating in our study. So, after obtaining\nethics approval, we decided to conduct a second round of data collection and\nintroduced a reward— an AUD 50 gift card voucher— to incentivise partici-\npation. The second round, advertised again on social media like LinkedIn and"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "advertised again on social media like LinkedIn and\nTwitter with mention of the reward, resulted in responses from 19 suitable can-\ndidates, bringing the total number of participants to 22. Since our goal was to\nrecruit participants with some experience in AI/ML system development, we\nincorporated two employment-related questions, inquiring about their years\nof experience in the field and their level of involvement in various job respon-\nsibilities. Participants received a reward of AUD 50 upon the completion of\ndata collection. Since we advertised our study on social media, we obtained\nresponses from various countries worldwide, as illustrated in Table 1. Since we\ndid not favour specific countries, the responses were spread out across different\nregions. We obtained a majority of the respon"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "rent\nregions. We obtained a majority of the responses from Australia (13), followed\nby the responses from other countries like Nepal (3), Israel (1), Japan (1), USA\n(1) etc. We present an in-depth analysis of the participants’ demographics in\nSection 4.1.\nWe gathered qualitative data through semi-structured interviews with 22\nAI practitioners experienced in AI/ML system development. All interviews\nwere conducted online using Zoom and were audio-recorded. Each interview\nlasted between 40 and 45 minutes.\n3.3 Data Analysis\nIn our study, qualitative data was gathered via semi-structured interviews, and\nconsequently, a qualitative approach was employed for data analysis. Socio-\nTechnical Grounded Theory (STGT) for data analysis was used to analyse the\n10\ndata, as it is particularly suitable for"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "se the\n10\ndata, as it is particularly suitable for analysing open-ended data and gaining\ninsights within socio-technical contexts (Hoda, 2021). After obtaining consent\nfrom each participant, we transcribed the data. The data collection and anal-\nysis phases involved an iterative process as shown in Figure 1. Initially, we\nanalysed the data from 13 participants using open coding approach to develop\nconcepts and categories, involving constant comparison of diverse open-text\nresponses (Hoda, 2021). We performed inductive open coding within the RQs.\nFor example, to answer our RQ2 which is, What challenges do AI practitioners\nface in developing a fair AI/ML system and what are the factors that lead to\nthose challenges? , initially, we gathered qualitative data from 13 participants\nby asking the"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "ualitative data from 13 participants\nby asking them, “Based on your professional experience, do you face any chal-\nlenges in developing a fair AI/ML system? (If yes), what challenges do you\nface? What do you think are the factors leading to those challenges?” We de-\nveloped codes using the open-coding approach in open-text answers as shown\nin Figure 2. For instance, codes like ‘access to limited data’ and‘lack of data\naccess’ were identified through open coding. Subsequently, we engaged in con-\nstant comparison of these codes to continually compare them, leading to the\nrecognition of patterns among them. For instance, upon reviewing the codes\nmentioned above, we identified a common pattern related to the challenge of\naccessing datasets required in the development of a fair AI/ML system. We"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "ired in the development of a fair AI/ML system. We\ncombined these two codes to develop a concept of ‘gaining access to datasets’ .\nFig. 2 Examples of STGT analysis (Hoda, 2021) applied to qualitative data on the chal-\nlenges in developing a fair AI/ML system.\nUsing the same constant comparison approach for other codes, we derived\nconcepts such as ‘balancing ideal vs real’ ,‘handling data-related issues’ , and\n‘following policies and regulations’ . We again constantly compared these con-\ncepts with one another and developed distinct categories. In this context, these\nfour concepts shared a challenge associated with the process of developing a\nfair AI/ML system, leading us to establish a category known as ‘process-related\nchallenges’ .\nLikewise, we identified multiple codes and concepts addr"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "se, we identified multiple codes and concepts addressing the chal-\nlenges associated with the resources required for developing a fair AI/ML\nsystem. This process led to the development of another high-level category,\n11\nnamely, ‘resource-related challenges’ . In this way, we established a total of three\ncategories encapsulating the challenges faced by AI practitioners in develop-\ning a fair AI/ML system, namely, process-related challenges ,resource-\nrelated challenges , and team-related challenges . Detailed information on\nthese challenges is provided in Section 4.\nBuilding on the primary findings from the initial analysis, we collected\ndata from the remaining 9 participants, focusing on key insights from the first\nround. This data was analysed using targeted coding , which involves genera"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "ysed using targeted coding , which involves generating\ncodes that align with the concepts and categories identified in the initial stage,\nfollowing STGT guidelines (Hoda, 2021).\nAll four authors were involved in designing the interview questionnaire.\nHowever, the first author led the data analysis with detailed feedback from\nthe second author and regular feedback from the third and fourth authors.\nAfter the qualitative data were analysed, the results, including codes, concepts,\nand categories, were shared and discussed among all authors, who collectively\ncontributed to presenting the findings.\nThe STGT for data analysis encompasses steps of open coding, targeted\ncoding, constant comparison, and memoing. “Basic memoing is the process\nof documenting the researcher’s thoughts, ideas, and refl"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "menting the researcher’s thoughts, ideas, and reflections on emerging\nconcepts and (sub)categories and evidence-based conjectures on possible links\nbetween them” (Hoda, 2021). Consequently, we wrote memos to record signif-\nicant insights and reflections discovered during the open coding and targeted\ncoding activities. An illustration of a memo created for AI practitioners’ de-\nscription of ‘fair AI/ML’, specifically ‘in terms of the absence of bias’ and\n‘in terms of the presence of desirable attributes ’, is provided in Figure 3. The\ndiscussion on the key insights derived from memoing is presented in Section\n5.5.\nFig. 3 An example of a memo on AI practitioners’ understanding of ‘fair AI/ML’\n12\n4 Findings\n4.1 Participants’ Demographics\nWe present the demographic information of the participa"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "esent the demographic information of the participants in this section.\nTable 1 presents an overview of the participants’ demographics based on their\nage, gender, country, education, work experience in AI/ML system develop-\nment activities, and job title. We used identifiers such as P1, P2, P3, and so\nforth to represent the participants in our study.\nTable 1 Demographics of the Interview Participants\nPId Age\nRange\n(years)Gender Country Education AI/ML\nExp.\n(years)Job Title\nP1 20-25 Man Nepal Bachelor 1-2 AI Engineer\nP2 20-25 Man Australia Bachelor 0-1 AI Engineer\nP3 31-35 Woman Thailand Ph.D. or higher 2-5 AI Research\nScientist\nP4 26-30 Man India Bachelor 5+ Data Scientist\nP5 26-30 Man Australia Master 5+ AI Engineer\nP6 26-30 Man Australia Master 1-2 Data Scientist\nP7 31-35 Woman Australia "
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "aster 1-2 Data Scientist\nP7 31-35 Woman Australia Master 1-2 AI Engineer\nP8 26-30 Man Nepal Master 1-2 ML Engineer\nP9 31-35 Man Australia Ph.D. or higher 5+ Data Scientist\nP10 46-50 Man Australia Ph.D. or higher 2-5 ML Engineer\nP11 26-30 Man Australia Master 1-2 ML Engineer\nP12 20-25 Woman Australia Master 1-2 ML Engineer\nP13 26-30 Man Australia Master 2-5 ML Engineer\nP14 31-35 Man Australia Ph.D. or higher 2-5 Data Scientist\nP15 46-50 Man Japan Master 2-5 AI Engineer\nP16 31-35 Man Australia Master 2-5 ML Engineer\nP17 26-30 Man Australia Bachelor 1-2 ML Engineer\nP18 31-35 Woman Australia Master 0-1 ML Engineer\nP19 31-35 Man Vietnam Master 2-5 AI Engineer\nP20 31-35 Man Israel Master 5+ ML Expert\nP21 20-25 Man Nepal Bachelor 2-5 ML Engineer\nP22 26-30 Woman USA Master 2-5 Data Scientist\nA tot"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "22 26-30 Woman USA Master 2-5 Data Scientist\nA total of 22 AI practitioners took part in our study including 17 men and\n5 women. Moreover, the majority of participants (8 out of 22) fell into the\nage group of 26-30 years and 31-35 years each, while only 2 belonged to the\nage group of 46 to 50 years. In terms of experience, the majority (13 partici-\npants) had more than 2 years of experience whereas 9 participants had up to\n2 years of experience in AI/ML system development. The geographical distri-\nbution indicated that the majority were from Australia (13 participants), with\n3 participants from Nepal and 1 each from India, Japan, USA, Israel, Thai-\nland, and Vietnam. Similarly, we inquired about the participants’ job titles\nor roles within their companies. The majority of participants held"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "their companies. The majority of participants held the title of\n‘ML Engineer’ (9 out of 22), followed by ‘AI Engineer’ (6 out of 22), ‘Data\n13\nScientist’ (5 out of 22), and one participant each for ‘AI Research Scientist’\nand ‘ML Expert’. As our target interview participants were practitioners in-\nvolved in AI/ML system development activities, we wanted to know the major\nAI/ML system development-related activities they were involved in. Among\nthe 22 participants, the majority engaged in ‘Data cleaning’ (19 participants),\nfollowed by ‘Model requirements’, ‘Data collection’, ‘Model training’, ‘Model\nevaluation’, and ‘Model deployment’ activities, each having 17 participants\nout of 22. 5 out of 22 participants chose the ‘Other’ option and elaborated on\nactivities they engaged in through open-"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "orated on\nactivities they engaged in through open-ended answers— activities that were\nnot initially listed in the pre-interview questionnaire. Some of the mentioned\nactivities included ‘system design’, ‘data pipelines’, ‘business benefit monitor-\ning and reporting’, ‘model integration developed by the research team into the\npipelines’, and ‘pipeline deployment’.\n4.2 RQ 1- What do AI practitioners understand by ‘fair AI/ML’?\nBased on the responses, we grouped the participants’ understanding of ‘fair\nAI/ML’ into two categories including, (i) In terms of absence of bias and\n(ii) In terms of presence of desirable attributes , which are explained in detail\nbelow. Figure 4 shows the overview of the participant’s understanding of ‘fair\nAI/ML’.\nFig. 4 Overview of the participant’s understanding of"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": ". 4 Overview of the participant’s understanding of ‘fair AI/ML’\n4.2.1 In terms of absence of bias\nWhen a person is tasked with comprehending the concept of ‘fairness’, it is\nquite probable that their understanding will revolve around the absence of\nbiases as defined by tech companies such as Google (Google, 2022), IBM (IBM,\n2022), and countries like Australia (Australia, 2019). When the participants\nwere asked to share their understanding of ‘fair AI/ML’, [P2, P9, P10, P11,\nP13, P14, P15, P18, P19, and P20] described ‘fair AI/ML’ in terms of absence\nofbias in the AI/ML system. For example, participants [P9] and [P11] said:\n14\n“A fair model is a model which is not skewed and not biased.” - [P9]\n“So in my opinion, I guess like a fair model should be something which decreases\nthe bias, as I m"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "ould be something which decreases\nthe bias, as I mean, there should be very less bias.” - [P11]\n4.2.2 In terms of the presence of desirable attributes:\n[P1, P3, P4, P5, P6, P7, P8, P12, P14, P16, P17, P21 and P22] described ‘fair\nAI/ML’ in terms of its features or attributes. The participants framed it as the\nnecessary elements an AI/ML system must possess to be considered as fair. For\nexample, the participants said that the AI/ML system should be reproducible\n[P3], transparent and explainable [P4, P12, P17, P21], interpretable [P21],\nand accurate [P1, P7, P8, P16, P22]. Some participants also mentioned that\na fair AI/ML system should use a good amount of data [P6] and have proper\nalgorithms [P14]. For example, [P3], [P4], [P7], and [P14] said,\n“But to me fairness is more about whether or "
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "aid,\n“But to me fairness is more about whether or not it is reproducible. It’s some-\nthing that can be tested and checked and improved from that again.” - [P3]\n“There should be transparency in any fair model that you build, right? So it\nshould explain why it throws a certain outcome.” - [P4]\n“Fair model my understanding that could work with different data. It should still\nlike give proper accurate results. There shouldn’t be a huge difference between the\nseen data and unseen data.” - [P7]\n“A fair model should have proper algorithms which support you to treat your\ngroups fairly and maybe post-processing stage where you when you’re applying busi-\nness logic.” - [P14]\n4.3 RQ 2- What challenges do AI practitioners face in developing a fair\nAI/ML system and what are the factors that lead to tho"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "L system and what are the factors that lead to those challenges?\nFig. 5 Overview of the participants’ challenges in developing a fair AI/ML system\nWe also asked the participants about the challenges they face in developing\na fair AI/ML system through an open-ended question. We categorised the chal-\nlenges of AI practitioners into three categories which are, (i) Process-related\nchallenges, (ii) Resource-related challenges, and (iii) Team-related challenges,\nbased on the responses of the participants. Each category is underpinned by\nmultiple concepts and codes which are explained in detail below. Figure 5\n15\nshows the overview of the challenges faced by AI practitioners in developing a\nfair AI/ML system.\nOnce we inquired with the participants regarding the challenges they en-\ncountered in de"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": " regarding the challenges they en-\ncountered in developing a fair AI/ML system, we delved further to understand\nthefactors leading to those challenges. Gaining insights into the factors lead-\ning to the challenges could contribute to devising more effective strategies to\nassist AI practitioners in overcoming those challenges. Additionally, we have\nhighlighted the factors leading to each challenge within the quotes of the par-\nticipants.\n4.3.1 Process-related challenges\nThe participants shared the challenges they encountered in developing fair\nAI/ML systems, specifically about the process of developing such systems.\nHere, the term ‘process-related challenges’ refers to the challenges faced during\nthe development phase of the AI/ML systems. The participants reported four\nkey challenges (conc"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "he participants reported four\nkey challenges (concepts) under this category which include, (i) Gaining access\nto datasets, (ii) Balancing ideal vs real, (iii) Handling data-related issues, and\n(iv) Following policies and regulations. Each of these concepts is underpinned\nby multiple codes which are discussed below.\nGaining access to datasets\nAI practitioners involved in the development of AI/ML systems might face\nlimitations in accessing vital resources for their work. Factors like adherence to\ncompany rules and regulations can impede their access to necessary resources,\nleading to challenges that, in turn, may contribute to the development of\nan unfair AI/ML system. In our study, [P4, P8, P12, and P14] reported the\nchallenge of gaining access to the datasets they require to train an AI/ML"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "ess to the datasets they require to train an AI/ML\nmodel. The factors that led to the challenge of gaining access to the datasets\ninclude the size of the organisation and the data confidentiality policy. For\nexample, participants [P4] and [P8] said,\n“Like as a data scientist, I would have access to a very certain amount of data\nwhich I can pick. For example, some of the data would be from the external side or\nsomething you would not have access to that team data. Sometimes when you work\ndue to data confidentiality policy, you will not have access to most of your data\npoints.” - [P4]\n“Data is the one thing which models are built on but they are not available for\npublic access, right? Like the open AI is things.. that language models are trained\non the data sets, but data that are not availa"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "ned\non the data sets, but data that are not available for our cases (small companies) .”\n- [P8]\nBalancing ideal vs real\n#Real-world data vs training data: The desire to develop a perfect AI/ML\nsystem is different from the ability to develop it. Training an AI/ML model\nwith an extensive array of real-world data can be impractical. AI practitioners\n16\nmust rely on initial training data, and the system subsequently interacts with\nreal-world data after deployment. Consequently, achieving fairness in AI/ML\nsystems requires AI practitioners to navigate a balance between the training\ndata and real-world data. This equilibrium can prove challenging at times due\nto a variety of limiting factors. In our study, [P8, P9, P10, P12, P15, P16, and\nP19] reported the challenge of striking a balance between"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "ported the challenge of striking a balance between the real-world data\nand the training data that they use in the development of AI/ML systems. The\nfactors that led to the challenge of striking a balance between the real-world\ndata and the training datasets include gaps existing between the real-world\ndata and training data and negligence of the AI practitioners. For example,\nparticipants [P10] and [P15] said,\n“Because the collected data is only a subset of the data in real world . So,\nthe distribution of collected data is not identical to the real-world distribution, even\nif we do some data augmentation such as oversampling or other generative tech-\nniques, we cannot ensure the data distribution of augmented data is identical to the\ndistribution in the real world. So we can just assume it"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "bution in the real world. So we can just assume it is approximately identical,\nbut they are not perfectly, identical. So it can be still a huge, huge challenge.” -\n[P10]\n“Originally, in the project we made, we were trying to do some stuff on human\nphotos. And so we had to augment our data, but the way that the data was created\nwas not correct , it didn’t actually match the real data, like real photos. And so\nthe AI actually learned to tell the difference between the synthetic data and the real\ndata and could tell the difference between a fake photo and a real photo. And so\nthat was like a kind of an eye opener for us that there was a bias from introduced\nfrom the synthetic data.” - [P15]\n#Fairness requirements vs technical constraints: Developing an AI/ML sys-\ntem may appear straightforwar"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "loping an AI/ML sys-\ntem may appear straightforward, but it is a highly intricate undertaking. AI\npractitioners may face considerable challenges when trying to balance their\nenvisioned ideal AI/ML system with the challenges of the real world. In our\nstudy, [P4, P11, P14, P16, P17, and P19] mentioned the challenge they face in\nmaintaining a balance between their requirements for developing a fair AI/ML\nsystem with the technical constraints they encounter. They discussed factors\nsuch as the complex nature of AI and lack of time as contributing to this\nchallenge. For example, [P16] and [P19] said:\n“And actually, at first, we do not talk about the machine learning bias. At first,\nwe talk about the production because we are practitioners and it is more important,\nI mean, the working version is "
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "is more important,\nI mean, the working version is the most important .”- [P16]\n“So as I mentioned, we tend to have like the model that works first, then we’ll\nlook at the virus later. In the industry here, every project has a deadline and\nlifetime . So if we don’t launch the products, the stakeholders might not be happy\nand then they could find some other people who can do that.” - [P19]\nHandling data-related issues\n#Detecting data bias: One of the main aspects of developing any AI/ML sys-\ntem is data, presenting a significant challenge for AI practitioners in addressing\ndata-related issues during the development of fair AI/ML system. Effectively\nmanaging biases in the data necessitates the initial step of detection, and cor-\nrective measures can only be taken once the biases are identifie"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "es can only be taken once the biases are identified. The detection\n17\nof data bias may pose a formidable challenge for AI practitioners, influenced\nby various factors. In our study, [P3, P6, P9, P10, P13, P14, and P15] reported\nthe challenge of detecting data bias during the development of a fair AI/ML\nsystem. This challenge was led by factors like the nature of data bias, lack of\ntime, and lack of tools/techniques. For example, [P9] and [P13] said:\n“You’re working on a limited timeline project . So your priority is to have a\nworking model. And sometimes you might not be able to discover such data biases.”\n- [P9]\nIf we have data bias checking tool to detect biases automatically, it would be\ngreat.” - [P13]\n#Addressing data bias: Simply identifying data bias is not sufficient for\ndeveloping"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "tifying data bias is not sufficient for\ndeveloping a fair AI/ML system; it is crucial to actively address and rectify\nthese biases. Dealing with identified bias in the data becomes the subsequent\nstep and it can be challenging for AI practitioners due to several factors. In\nour study, [P9, P11, P14, and P15] discussed that they feel it is challenging to\naddress (mitigate and/or remove) biases from the data during the development\nof AI/ML systems. The factors leading to the challenge of addressing data bias\ninclude a lack of tools/techniques and the biased nature of team members. For\nexample, [P11] said,\n“If there was some kind of tool which can let the person who is training the\nmodel know, maybe you need to remove this data, or else maybe you need to do these\nkinds of operations on your d"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "you need to do these\nkinds of operations on your data, or maybe you need to do something, obviously,\nanyone wants that kind of tool as removing bias from the data is hard.” - [P11]\nFollowing policies and regulations\nAI practitioners are required to adhere to various policies and regulations,\nwhich serve as guiding principles in developing AI/ML systems. Nevertheless,\nif these policies and regulations fall short, they can pose challenges to AI prac-\ntitioners and can impede the development of a fair AI/ML system. Likewise, in\nour study, [P4, P5, P7, P9, and P12] expressed challenges in adhering to poli-\ncies and regulations concerning AI ethics while working on the development\nof AI/ML systems. This challenge arose due to factors like lack of policies and\nlack of implementation of the polic"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "f policies and\nlack of implementation of the policies. For example, [P5] and [P9] quoted:\n“I don’t think Australia has any updated AI ethics policies and stuff, maybe,\nthey need to update the policies based on how to follow the current trend and to\nfollow the current technologies, something like that.” - [P5]\n“The European Alliance introduced a responsible AI framework but not sure\nif Australia has adapted such things in organisations. I know that companies\nlike Facebook, are currently adapting. When it comes to small companies, small\norganisations, I’m not quite sure.” - [P9]\n4.3.2 Resource-related challenges\nParticipants encountering challenges in developing a fair AI/ML system face\nissues primarily associated with the resources used in the development process,\nconstituting the second ca"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "he development process,\nconstituting the second category of challenges. The participants reported two\n18\nkey challenges (concepts) under this category which include, (i) Obtaining\nrequired datasets and (ii) Obtaining other resources. Each of these concepts\nis underpinned by multiple codes which are discussed below.\nObtaining required datasets\nAs previously mentioned, data plays a crucial role in the development of\nAI/ML systems. In our study, the majority of the participants [P1, P3, P4,\nP5, P8, P9, P11, P13, P14, P15, P18, P20, P21, and P22] pointed out that\nobtaining the required datasets to develop a fair AI/ML system is challenging.\nThe participants noted that they often acquire imperfect (non-representative)\ndatasets or incomplete datasets for developing AI/ML systems, presenting\na ch"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "sets for developing AI/ML systems, presenting\na challenge in ensuring the fairness of such systems. The factors leading to\nthis challenge include a lack of representative datasets, lack of cost, lack of\ntools/techniques, lack of control over data collection, and negligence of AI\npractitioners. For example, [P3], [P11] and [P20] said:\n“I think bias could happen since the first step because sometimes we have other\nteams to collect the data , yes, we don’t know what they actually provide to us.” -\n[P3]\n“It has got something to do with the data, but then from the very beginning,\nlike black people are discriminated in the world. So, I feel like the reason might\nbe because, in the world, we have more white people images than black people\nimages . So the data itself is less and it is a proven thi"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": " So the data itself is less and it is a proven thing.” - [P11]\n“Should I buy this data? Or not? It’s money. In most cases, you don’t use data\nfor free. You know, sometimes you need to actually pay for this in some way .”-\n[P20]\nObtaining other resources\n#Technological requirements: In addition to datasets, AI practitioners have\ndiverse technological needs, including high-quality hardware, to augment the\ndevelopment of a fair AI/ML system. Technology plays a vital role in assisting\nAI practitioners in the development process. However, acquiring the necessary\ntechnology can present a challenge, potentially impeding the development of\na fair AI/ML system. Only [P8, P11, and P12] indicated that they encounter\nchallenges in developing fair AI/ML systems because they lack the necessary\ntechnolog"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": " systems because they lack the necessary\ntechnology needed for the development process. The participants discussed the\nlack of cost as a factor leading to this challenge. For example, [P11] said:\n“And resources because some huge models require huge GPUs. So in our com-\npany, we do not have GPUs because they are expensive . So yeah, lack of such\nresources of course is a challenge.” - [P11]\n#Human-related requirements: Developing any software is a collaborative\neffort, involving multiple teams within a company dedicated to specific tasks.\nCollaborating with various team members offers advantages, such as diverse\nassistance in different aspects. Nonetheless, not all companies may incorpo-\nrate multiple members in their development teams, posing a challenge for AI\npractitioners striving to dev"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "g a challenge for AI\npractitioners striving to develop a fair AI/ML system. In our study, [P12 and\n19\nP15] reported that not having multiple people on the team is a challenge for\nthem in developing a fair AI/ML system, and the lack of cost is the factor\ncontributing to this challenge. For example, [P15] said:\n“So even an individual (AI practitioner) can introduce bias into their model.\nAnd what we try to do is have the evaluation code be performed by a different\nperson than the trainer. But, usually, companies can’t really afford to do that\nwhich is challenging.” - [P15]\n4.3.3 Team-related challenges\nThe majority of participants also encountered challenges pertaining to their\nknowledge and understanding of different aspects, which we have classified as\nteam -related challenges in developin"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "lassified as\nteam -related challenges in developing a fair AI/ML system. Here, the term\n‘team-related challenges’ refers to the challenges AI practitioners encounter as\na result of their own limitations or shortcomings. The participants discussed\ntwo key challenges under this category which are the challenges in (i) Having\nknowledge of bias/fairness and (ii) Having knowledge of AI. These two key\nconcepts are underpinned by multiple codes which are explained in detail\nbelow:\nHaving knowledge of bias/fairness\nIt is important that AI practitioners possess good knowledge and understand-\ning of key concepts such as ‘bias’ and ‘fairness’ when aiming to develop a fair\nAI/ML system. However, AI practitioners might face difficulties in grasping\nthe concepts of ‘bias’ and ‘fairness’ due to reasons s"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "concepts of ‘bias’ and ‘fairness’ due to reasons such as the subjective na-\nture of these concepts. In our study, [P1, P2, P4, P5, P7, P9, P10, P14, P15,\nand P19] reported that they are unable to understand the concept of ‘bias’\nor ‘fairness’ which poses a challenge in developing a fair AI/ML system. The\nfactors discussed by the participants that contributed to this challenge include\na lack of domain knowledge, a lack of AI practitioners’ common approach, and\na lack of awareness. For example, [P4], [P9] and [P19] said,\n“And again, now all humans’ thoughts and the way they approach a problem\nwould be different . So that is one more reason for not understanding the bias\nproblems in the systems we develop.” - [P4]\n“Most of our data is from sensors. So we might not have like a very clear view\n"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "sors. So we might not have like a very clear view\nof biases like the people who deal with NLP and stuff.” - [P9]\n“It’s hard to understand fairness. I think I’m not intellectual enough to be in\na position to define fairness. My definition of fairness could be different from\nother people’s point of view.” - [P19]\nHaving knowledge of AI\nThe rapid growth of AI is making it increasingly challenging for everyone to\nkeep updated with its advancements and understand its outcomes (Pant et al.,\n2023). AI practitioners might face such challenges that can negatively impact\nthe development of a fair AI/ML system. In our study, only [P11 and P15]\n20\nreported that understanding AI outcomes is challenging due to its complex\nnature, negatively affecting the development of fair systems. For example,\n[P15] s"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": " development of fair systems. For example,\n[P15] said,\n“And then AI models, sometimes you don’t know what it is actually deciding\non and what it is actually measuring as it is too complex . So we had other cases\nwhere the AI kind of learns funny things that you don’t anticipate.” - [P15]\n4.4 RQ 3- What do AI practitioners perceive as the consequences of\ndeveloping an unfair AI/ML system?\nFig. 6 Overview of the consequences of developing an unfair AI/ML system\nWe posed an open-ended query to the participants regarding the conse-\nquences of developing an unfair AI/ML system. The participants explored\nthree distinct categories of negative consequences: (i) Impact on organisa-\ntions, (ii) Impact on users, and (iii) Impact on practitioners. Each of these\nthree categories is underpinned by multi"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": " of these\nthree categories is underpinned by multiple concepts and codes explained in\ndetail below. Figure 6 shows an overview of the consequences of developing an\nunfair AI/ML system.\n4.4.1 Impact on organisations\nThe majority of participants delved into the adverse impacts on organisations\nresulting from the failure to develop a fair AI/ML system. The participants\ndelved into two key facets (concepts) of the impacts on organisations namely:\n(i) Financial losses and (ii) Reputational repercussions.\nFinancial losses\nDeveloping AI/ML systems is a complex process that demands various re-\nsources like time and money (Pant et al., 2023). If the systems turn out to\nbe unfair or fail to meet goals, it not only affects the project but also leads\nto financial setbacks for the organisation. Many pa"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "o financial setbacks for the organisation. Many participants [P1, P4, P6, P8,\nP9, P11, P12, P14, P17, P19, and P21] in our study reported that the devel-\nopment of an unfair AI/ML system leads to financial losses to organisations.\nFor example, participants [P4] and [P6] said:\n21\n“Because the one that you’re going to deploy would definitely have an impact on\nyour business and in such cases, any small bias can lead to a huge financial loss.”\n- [P4]\n“Yeah, it also constitutes money loss to the organisation.” - [P6]\nReputational repercussions\nIn today’s tech-driven era, organisations are in fierce competition to enhance\ntheir software systems (Hua and Belfield, 2020). The constant race for im-\nprovement means even a minor flaw can tarnish an organisation’s reputation.\nDeveloping an unfair AI/M"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "ganisation’s reputation.\nDeveloping an unfair AI/ML system poses a significant risk, as it can lead to\nsevere reputational repercussions for organisations in this highly competitive\nlandscape. In our study, only [P5, P7, and P9] provided insights into the conse-\nquences associated with the organisation’s reputation when an unfair AI/ML\nsystem is developed. For example, participants [P5] and [P9] said:\n“If they are collecting the data, then there shouldn’t be, any bias in the data.\nIf bias is there, different kinds of controversies will rise in, and legal issues will\narise.” - [P5]\n“But to give you another perspective, like Twitter, such models, are exposed\nto a large number of people and a large number of datasets and are heavily used\nworldwide. Such organisations need to ensure they do no"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "wide. Such organisations need to ensure they do not have such biases in their\ndatasets, or in finally at their models, because then it would create controversies,\nand then it could finally tarnish the image of these organisations as well.” - [P9]\n4.4.2 Impact on users\nMany participants mentioned the negative impacts on users due to the de-\nvelopment of an unfair AI/ML system. The participants discussed two key\naspects (concepts) of the impacts on people including (i) Obtaining flawed\nproduct and (ii) Emotional distress and discrimination.\nObtaining flawed product\nThe primary objective of developing any AI/ML system is to aid users in\nvarious domains, be it healthcare, technology, education, etc. When AI/ML\nsystems are developed unfairly, users receive flawed products, which under-\nmines th"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "ers receive flawed products, which under-\nmines their core purpose. In our study, [P1, P2, P3, P8, P9, P10, P11, P15,\nP16, and P18] emphasised that the primary detriment to users resulting from\nthe development of an unfair AI/ML system is the receipt of defective prod-\nucts, leading to inaccurate predictions. For example, [P1] and [P2] said:\n“It is important to create a fair ML model because the main reason is that if\nthe ML model is biased, users won’t be able to achieve our end goal. If the model is\nbiased, it won’t give the result that it is required to give.” - [P1]\n“Users will be getting a great product from the company if it is fair, otherwise\nnot.” - [P2]\n22\nEmotional distress and discrimination\nWhen an unfair AI/ML system is developed, most likely the users who are us-\ning those sy"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "ed, most likely the users who are us-\ning those systems are impacted negatively (Martin, 2018; Prates et al., 2020).\nThe primary impact could manifest as emotional distress and discrimination\ntowards these users. Only [P1, P7, P20, and P22] highlighted that users experi-\nence emotional distress and discrimination when using unfair AI/ML systems,\nleading to hurt sentiments. For example, a participant [P1] said:\n“The second reason is that the sentiments of the people can be hurt if the model\nis biased.” - [P1]\n4.4.3 Impact on practitioners\nA very few participants mentioned the negative impacts of developing an unfair\nAI/ML system on the practitioners responsible for their development. We\nclassified these negative consequences into a specific aspect: (i) No professional\nempowerment.\nNo profes"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "aspect: (i) No professional\nempowerment.\nNo professional empowerment\nAI practitioners gain valuable insights through hands-on experience in devel-\noping AI/ML systems, complementing their theoretical knowledge. The de-\nvelopment of unfair systems, however, could have a negative impact on these\npractitioners, affecting their learning experiences in the field. Only [P11 and\nP13] highlighted that the development of unfair AI/ML systems hinders the\nprofessional empowerment of AI practitioners, causing a decline in confidence\nand knowledge. For example, participants [P11] and [P13] said,\n“When they develop an unfair model, they will not learn something new from\nthat.. like, in terms of data augmentation, or terms of algorithmic change, or terms\nof data collection.” - [P11]\n“Yeah, if the researc"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "of data collection.” - [P11]\n“Yeah, if the research team or the model training team make the unfair model\nand we lack the confidence to use the model directly in our product.” - [P13]\n4.5 RQ 4- What strategies do AI practitioners use in ensuring the fairness of\nan AI/ML system?\nWe asked the participants about the strategies that they use to ensure the fair-\nness of AI/ML systems. The participants discussed two categories of strategies\nthat they use to ensure the fairness of AI/ML systems which are (i) Bias-\nrelated strategies and (ii) Performance-related strategies. Each of these two\ncategories is underpinned by multiple concepts and codes explained in detail\nbelow. Figure 7 shows the overview of AI practitioners’ strategies to ensure\nthe fairness of the AI/ML systems they develop.\n23\nFig."
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "airness of the AI/ML systems they develop.\n23\nFig. 7 Overview of the participants’ strategies in ensuring the fairness of an AI/ML system\n4.5.1 Bias-related strategies\nThe majority of the participants discussed the strategies they used to address\nthe bias-related issues when developing a fair AI/ML system. The participants\nreported two key strategies (concepts) they used to address bias-related issues\nincluding (i) Detecting bias and (ii) Mitigating bias. Each of these concepts is\ndiscussed in detail below.\nDetecting bias\nAddressing any concern starts with its detection; without identification, mit-\nigation is impossible. In our study also, participants outlined their strategies\nfor ensuring the fairness of the AI/ML systems they developed, emphasising\nthe initial step of detecting bias. ["
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": " emphasising\nthe initial step of detecting bias. [P1, P3, P4, P5, P6, P7, P8, P9, P11, and\nP18] mentioned that they rely on testing the system with test datasets as their\nstrategy for identifying biases in the system. For example, participants [P1]\nand [P8] said:\n“ So in the beginning, we categorise our whole dataset as train, test, and val-\nidation dataset and use the testing data to test the model. The testing phase is\nmandatory, otherwise, we won’t know if the model we created has biases.” - [P1]\n“After building the model, we use some test case scenarios and we have been\ndoing this post process like how well the model is doing in the test data set to find\nout any biases.” - [P8]\nMitigating bias\n#By balancing datasets: Data holds significance in the development of AI/ML\nsystems. Using we"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "ance in the development of AI/ML\nsystems. Using well-balanced datasets for training is important to ensure the\nsystem generates an unbiased prediction. Likewise, in our study, the majority\nof the participants [P1, P3, P5, P7, P8, P9, P10, P11, P13, P14, P15, P16, P18,\nP19, P20, P21, and P22] mentioned using the data augmentation technique to\nbalance training datasets during the development phase to mitigate biases in\ntheir systems. For example, participants [P14] and [P22] said:\n24\n“I try to identify whether there are any segments where these metrics are up\nor down, and then I would go back to the data and see if it is because we don’t\nhave enough data for those regions. And then if that is the case, I’ll try to maybe\naugment the data.” - [P14]\nI tried to increase for those that did not ha"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "P14]\nI tried to increase for those that did not have enough by strategically doing some\nkind of artificial, like stretching the existing data or compressing data because I was\nworking with audio data.” - [P22]\n#By involving multiple people: Collaborating in a team with multiple\nmembers offers numerous advantages in software development, fostering the\nexchange of knowledge and facilitating mutual learning (Augustin et al., 2002).\nThis collaborative dynamic can also prove particularly beneficial in the con-\ntext of developing AI/ML systems. In our study, [P2, P7, and P12] mentioned\nthat they get input from multiple people and this collaborative approach aids\nthem in mitigating data biases within the system they develop. For example,\n[P7] said:\n“I don’t have any medical background. Sometimes,"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": ":\n“I don’t have any medical background. Sometimes, some factors or features I\nnever thought about could cause bias. So we have other advisors, like from another\nuniversity, doctors, and professors. So yeah, we have that on the system to ask them\n(domain experts) if we have any doubts.” - [P7]\n#By focusing on practices: In addition to securing the necessary resources,\nadhering to best practices can be important in the development of an AI/ML\nsystem. In our study, [P1, P2, P4, P5, and P12] reported that focusing on\nthe practices of developing a fair AI/ML system is the strategy they take in\nmitigating biases from the system. For example, [P2] and [P4] quoted:\n“I just become conscious about unconscious biases during the development pro-\ncess.” - [P2]\n“I would also believe in the feedback mech"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": " - [P2]\n“I would also believe in the feedback mechanism out there, not just seeing your\nresults on the test set and then going and deploying it, but rather enabling a feed-\nback mechanism. And whenever the system goes a little off in terms of prediction,\nimmediately, the feedback loop is getting connected there. So that is one way that I\ngenerally rectify my bias.” - [P4]\n4.5.2 Performance-related strategies\nA few participants elaborated on strategies for the performance of the AI/ML\nsystem they developed to ensure fairness. Within this category, participants\ndeliberated on a specific strategy (concept), namely, (i) Detecting inaccuracy,\nwhich is explained below.\nDetecting inaccuracy\nIn our study, [P5, P7, P8, P9, P10, P15, P17, and P21] mentioned that they\ndetected the inaccuracy of the A"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "ntioned that they\ndetected the inaccuracy of the AI/ML system by using evaluation metrics.\nThis helps them gauge the system’s performance and ensure its fairness. For\nexample, [P8] said:\n“Fair model should be 100% accurate. 100% accuracy is good for our scenario,\nbut we also have other case scenarios like the loss and other things to notice in the\nevaluation metrics. When we have to check what is the loss of the models we have\nto go for the minimum loss.” - [P8]\n25\n4.6 Summary of Key Findings\nThis study focuses on exploring AI practitioners’ understanding of ‘fair AI/ML’,\nexploring the challenges they encounter during the development of a fair\nAI/ML system, understanding the consequences of developing an unfair AI/ML\nsystem perceived by them and investigating the strategies they employ to "
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "m and investigating the strategies they employ to en-\nsure fairness in the AI/ML systems they develop. Table 2 shows the summary\nof the key findings of our study.\nTable 2 Key Findings (KF) of the study.\nKey Findings (KF) Section\nKF1 AI practitioners’ understanding of ‘fair AI/ML’ 4.2\n(i) In terms of the absence of bias\n(ii) In terms of the presence of desirable attributes (transparency, accuracy,\ninterpretability etc.)\nKF2 AI practitioners’ challenges in developing a fair AI/ML system 4.3\n(i) Process-related challenges: gaining access to datasets, balancing ideal vs real,\nhandling data-related issues, and following policies and regulations\n(ii) Resource-related challenges: obtaining required datasets and obtaining other\nresources\n(iii) Team-related challenges: having knowledge of bias/fair"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "-related challenges: having knowledge of bias/fairness and having\nknowledge of AI\nKF3 Consequences of developing an unfair AI/ML system perceived by AI\npractitioners4.4\n(i) Impact on organisations: financial losses and reputational repercussions\n(ii) Impact on users: obtaining flawed product and emotional distress and\ndiscrimination\n(iii) Impact on practitioners: no professional empowerment\nKF4 AI practitioners’ strategies to ensure the fairness of an AI/ML system 4.5\n(i) Bias-related strategies: detecting bias and mitigating bias\n(ii) Performance-related strategies: detecting inaccuracy\nKF5 Despite differing understandings of ‘fair AI/ML’ among practitioners, a common\nchallenge they report facing is obtaining the required datasets for model training\nduring AI/ML system development.4.7.1\nK"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "l training\nduring AI/ML system development.4.7.1\nKF6 Some AI practitioners describing ‘fair AI/ML’ in terms of absence of bias seemed to\nhave a broader understanding of negative consequences on practitioners due to the\ndevelopment of unfair AI/ML systems compared to those emphasising the presence\nof desirable attributes .4.7.2\nKF7 Despite differing understandings of ‘fair AI/ML’ among practitioners, a common\nstrategy they report using to ensure fairness in AI/ML systems is implementing\nbias-mitigation strategies.4.7.3\n4.7 Framework showing the relationship between the aspects- understanding ,\nchallenges ,consequences andstrategies\nIn this section, we discuss the relationship between AI practitioners’ under-\nstanding of ‘fair AI/ML’ with three other aspects including, (i) the challenges\nenc"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "ee other aspects including, (i) the challenges\nencountered in the development of a fair AI/ML system, (ii) the consequences\nof developing an unfair AI/ML system, and (iii) strategies used to ensure the\nfairness of an AI/ML system. Every participant [P1 to P22] in our study de-\nscribed ‘fair AI/ML’ either in terms of the absence of bias or in terms of the\n26\npresence of desirable attributes in AI/ML systems. The exception was the par-\nticipant [P14], who described it in terms of the absence of bias, as well as in\nterms of the presence of desirable attributes in AI/ML systems. To illustrate\nthe relationship between these aspects, we developed a framework, which is\nshown in Figure 8.\nFig. 8 A framework showing the relationship between AI practitioners’ understanding of\n‘fair AI/ML’, with thre"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "itioners’ understanding of\n‘fair AI/ML’, with three aspects including, (i) their challenges in its development, (ii) con-\nsequences of developing unfair AI/ML system perceived by them, and (iii) their strategies\nin ensuring fairness of AI/ML systems\n4.7.1 Relationship between AI practitioners’ understanding and their\nchallenges\nAI practitioners’ understanding – in terms of the absence of bias\nvschallenges\nThe interview indicates that the participants who described ‘fair AI/ML’ in\nterms of the absence of bias in an AI/ML system reported challenges in all\nthree categories: (i) process-related, (ii) resource-related, and (iii) team-related\nchallenges that they face during AI/ML system development, as shown in Fig-\nure 8. Specifically, the majority of the participants reported facing challenge"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "rity of the participants reported facing challenges\nin obtaining required datasets (resource-related challenge), followed by the\nchallenge of handling data-related issues (process-related challenge). We see\nthat AI practitioners who described ‘fair AI/ML’ in terms of the absence of\nbias in AI/ML systems also reported facing substantial challenges related to\nthe datasets used in development. These challenges primarily revolve around\n27\nresource availability, specifically in obtaining the necessary datasets and han-\ndling data-related issues.\nAI practitioners’ understanding – in terms of the presence of de-\nsirable attributes vs challenges\nThe interview shows that the participants who described ‘fair AI/ML’ in terms\nof the presence of desirable attributes reported almost all the challenges a"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "le attributes reported almost all the challenges across\nall three categories— (i) process-related, (ii) resource-related, and (iii) team-\nrelated challenges that they encounter during the development of an AI/ML\nsystem, as illustrated in Figure 8. Notably, none of the participants reported\nthe challenge of lacking knowledge of AI (team-related challenge), whereas,\nmost of the participants reported facing the challenge of obtaining required\ndatasets (resource-related challenge) during AI/ML system development.\nIn summary, regardless of how AI practitioners described ‘fair AI/ML’, a\ncommon challenge they faced was obtaining the required datasets to train a\nmodel during AI/ML system development.\n4.7.2 Relationship between AI practitioners’ understanding and the\nconsequences\nAI practitioners’ "
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "erstanding and the\nconsequences\nAI practitioners’ understanding – in terms of the absence of bias\nvsconsequences\nThe interview indicates that the participants in our study, describing ‘fair\nAI/ML’ in terms of the absence of bias in AI/ML systems, perceived the\nnegative consequences of developing an unfair AI/ML system across all three\ncategories including, (i) impact on organisations, (ii) impact of users and (iii)\nimpact on practitioners, as shown in Figure 8. Notably, the majority of par-\nticipants perceived the acquisition of flawed products by users as a negative\nconsequence of developing an unfair AI/ML system. However, the majority did\nnot explicitly mention emotional distress and discrimination for users, as well\nas reputational repercussions for organisations, as significant negati"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "rcussions for organisations, as significant negative conse-\nquences. These two specific concerns were expressed by only one participant\neach.\nAI practitioners’ understanding – in terms of the presence of de-\nsirable attributes vsconsequences\nThe interview shows that the participants in our study, describing ‘fair AI/ML’\nin terms of the presence of desirable attributes perceived the negative conse-\nquences of developing an unfair AI/ML system across only two categories\nincluding, (i) impact on organisations, and (ii) impact on users. According to\nthe data, most participants talked about the financial loss to organisations as\na negative consequence of developing an unfair AI/ML system, as shown in\n28\nFigure 8. Notably, no participants in our study mentioned any negative conse-\nquences of dev"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "study mentioned any negative conse-\nquences of developing an unfair AI/ML system for the practitioners engaged\nin their development.\nIn summary, of the ones who described ‘fair AI/ML’ in terms of absence of\nbias, two of them discussed the negative consequence of developing an unfair\nAI/ML system on practitioners involved in the development. While those who\ndescribed it in terms of the presence of desirable attributes did not discuss the\nnegative consequences of developing an unfair AI/ML system for practitioners\nat all. It looks like the former were able to acknowledge it, and they seem to\nhave a broader understanding of the negative consequences associated with\ndeveloping unfair AI/ML systems.\n4.7.3 Relationship between AI practitioners’ understanding and their\nstrategies\nAI practitioners"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "nderstanding and their\nstrategies\nAI practitioners’ understanding – in terms of the absence of bias\nvsstrategies\nAccording to the interview, the participants in our study who described ‘fair\nAI/ML’ in terms of the absence of bias in AI/ML systems discussed strate-\ngies falling into both categories, including (i) bias-related strategies and (ii)\nperformance-related strategies. It shows that most participants discussed the\nstrategies to mitigate bias (bias-related strategies) from AI/ML systems to en-\nsure its fairness. In contrast, only a small number of participants reported the\nstrategies of detecting bias (bias-related strategies) and detecting inaccuracy\n(performance-related strategies) to ensure the fairness of AI/ML systems.\nAI practitioners’ understanding – in terms of the presence o"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "ioners’ understanding – in terms of the presence of de-\nsirable attributes vsstrategies\nThe interview shows that the participants in our study who described ‘fair\nAI/ML’ in terms of the presence of desirable attributes discussed strategies\nfalling into both categories, including (i) bias-related strategies and (ii) per-\nformance -related strategies. It shows that a slightly higher number of partic-\nipants discussed the strategy of mitigating bias (bias-related strategies) from\nAI/ML systems to ensure its fairness as compared to other strategies like de-\ntecting bias (bias-related strategies) and detecting inaccuracy (performance-\nrelated strategies). In contrast, an almost equal number of participants dis-\ncussed strategies like detecting bias (bias-related strategies) or detecting inac-\nc"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "ias (bias-related strategies) or detecting inac-\ncuracy (performance-related strategies) to ensure AI/ML system fairness.\nIn summary, the interview shows a consistent trend among participants dis-\ncussing the strategies that they used to ensure the fairness of AI/ML systems,\nregardless of how they described ‘fair AI/ML’. The majority of the partici-\npants who described ‘fair AI/ML’ either in terms of the absence of biasas well\nas in terms of the presence of desirable attributes in AI/ML systems reported\n29\nthe use of the common strategy of mitigating bias (bias-related strategies) to\nensure the fairness of the AI/ML systems they developed.\n5 Discussion\nIn this section, we discuss and compare our findings in light of the related\nworks.\n5.1 Definition/understanding of AI/ML fairness\nIn recen"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "efinition/understanding of AI/ML fairness\nIn recent years, major players in the tech industry, such as Google, Microsoft,\nand IBM, have delved deeply into the concept of fairness in AI. Their consen-\nsus on the ‘fairness’ principle revolves around minimising bias and fostering\ninclusive representation in the development of AI (Google, 2022; Microsoft,\n2024a; IBM, 2022).\nVarious experiments, including those by Harrison et al. (2020) and Sri-\nvastava et al. (2019), have explored user perspectives on AI/ML fairness. The\nstudy conducted by Harrison et al. (2020) with non-technical users on Amazon\nMechanical Turk (AMT) revealed that users reported unbiased models might\nnot be automatically perceived as fair. This finding does not align with our\nstudy as some participants described ‘fair AI/ML’ "
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "study as some participants described ‘fair AI/ML’ in terms of absence of bias.\nOn the other hand, the experiment by Srivastava et al. (2019) on AMT found\nusers defining fairness technically, focusing on accuracy and demographic par-\nity, mirroring our study where AI practitioners also described ‘fair AI/ML’\nin terms of accuracy (the presence of desirable attributes ) of the system. Im-\nportantly, our study involved AI practitioners, while the mentioned studies\nfocused on the general users of AI/ML systems.\nDue to the lack of research that focuses on investigating AI practition-\ners’ understanding of ‘fair AI/ML’, we conducted an empirical study with 22\nAI practitioners to investigate their understanding of what a ‘fair AI/ML’\nis. In our study, AI practitioners described ‘fair AI/ML’ in ter"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "dy, AI practitioners described ‘fair AI/ML’ in terms of the ab-\nsence of bias and in terms of the presence of desirable attributes in AI/ML\nsystems. Ryan et al. (2023) in their empirical study, found that participants\nwhen discussing the term ‘fairness’, commonly focused on preventing biased\ndecisions of ML systems. This aligns with our findings as several AI practition-\ners in our study also described ‘fair AI/ML’ in terms of the absence of bias.\nIt is important to note that both academic and industry professionals in the\nfields of Human-Computer Interaction (HCI) and ML participated in Ryan\net al. (2023)’s study. Similarly, aligning with the definition of ‘fairness’ intro-\nduced by tech companies like Google (Google, 2022) and Microsoft (Microsoft,\n2024a), some AI practitioners in our st"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "Microsoft,\n2024a), some AI practitioners in our study described ‘fair AI/ML’ in terms of\nthe absence of bias. However, when describing in terms of the presence of desir-\nable attributes in AI/ML systems, AI practitioners in our study specified fea-\ntures such as interpretability, transparency, and explainability that an AI/ML\n30\nsystem should possess to be deemed fair. Ryan et al. (2023) also highlighted\nthat a few participants mentioned that a system needs to be transparent to be\nconsidered fair. Notably, principles like ‘explainability’ and ‘transparency’ are\noutlined separately in the AI ethics principles listed by tech companies such as\nGoogle (Google, 2022), IBM (IBM, 2022), Microsoft (Microsoft, 2024a), and\ncountries such as Australia (Australia, 2019) and countries in Europe (Group,"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": " (Australia, 2019) and countries in Europe (Group,\n2019). This suggests a lack of alignment between how AI practitioners under-\nstand ‘fair AI/ML’ and the definitions set forth by tech companies, countries,\nand continents. This misalignment may hinder the development of universally\naccepted principles for fair AI/ML systems, potentially resulting in disparate\napproaches and interpretations within the AI community.\nIn a similar vein, accuracy andfairness are categorised as two different\nnon-functional requirements of an ML system (Habibullah et al., 2023). The\naccuracy of an ML system has been categorised as a non-functional require-\nment, which can be measured using ML-specific or standard measures, whereas\nfairness has been categorised as a non-functional requirement that cannot be\nmeasur"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "a non-functional requirement that cannot be\nmeasured and is non- quantifiable (Habibullah et al., 2023). However, in our\nstudy, we found that the participants [P1, P7, P8, P16, and P22] described\n‘fair AI/ML’ in terms of accuracy of the AI/ML system (Section 4.2). Along\nwith that, when asked about the strategies to ensure AI/ML fairness, some\nparticipants [P5, P7, P8, P9, P10, P15, P17, and P21] reported that they focus\non detecting the inaccuracy of the system (Section 4.5). The participants in\nour study considered accuracy as a requirement to develop a fair AI/ML sys-\ntem. This also shows that the way AI practitioners in our study described ‘fair\nAI/ML’ is different from the definitions of fairness provided by tech companies\nlike Google, IBM, etc, and different countries/continents like "
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "IBM, etc, and different countries/continents like the USA, China,\nAustralia, Europe, etc. Notably, these definitions do not include considerations\nregarding the accuracy of AI/ML systems. Understanding how AI practition-\ners conceptualise ‘fair AI/ML’ is important for developing effective policies and\nguidelines on AI fairness . By incorporating their perspectives, frameworks can\nbe created that not only meet regulatory standards but also align with practi-\ncal implementation challenges and fairness considerations faced in real-world\nAI applications. This can lead to more fair, inclusive, and socially responsible\nAI/ML systems.\n5.2 Challenges in developing a fair AI/ML system\nStudies have highlighted challenges for AI practitioners in developing a fair\nAI/ML system across various domains a"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "oping a fair\nAI/ML system across various domains and phases of development. Our study\nspecifically focused on investigating the overall challenges of AI practitioners\nin developing a fair AI/ML system through semi-structured interviews. A ma-\njority of participants in the studies by Holstein et al. (2019); Fenu et al. (2022)\nfaced challenges related to limited control over data collection, as well as chal-\nlenges in obtaining balanced and representative datasets for model training\ndue to a lack of methods supporting data collection and curation (Holstein\n31\net al., 2019). These findings align with our study, where participants reported\nsimilar challenges in obtaining necessary datasets, attributing them to a lack\nof control over data collection, and expressed difficulties in obtaining bala"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "tion, and expressed difficulties in obtaining balanced\ndatasets due to a lack of methods for data collection and curation (Section\n4.3.2). Most participants in Holstein et al. (2019)’s study reported challenges\nin detecting biases in the ML system, due to a lack of support and challenges\nin developing their own solutions due to limited time. Our findings align with\nthese, as participants in our study also highlighted how constraints, such as\nlack of support and time, pose challenges in detecting biases in systems and de-\nveloping their envisioned ideal system (Section 4.3.1). Similarly, Madaio et al.\n(2022) identified collecting datasets as a challenge for AI practitioners during\nthe development of AI systems, primarily due to the need to safeguard the\npersonal information of user data. Ou"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "afeguard the\npersonal information of user data. Our study’s findings align, as participants\nalso reported facing challenges in accessing necessary datasets due to data\nconfidentiality concerns (Section 4.3.1). Madaio et al. (2022) found that par-\nticipants reported challenges related to the resources required to develop a fair\nAI system, which aligns with one of our findings. Participants in our study also\nreported challenges in obtaining resources such as datasets, technology, and\nhuman resources, as discussed in Section 4.3.2. Both studies identified funding\nissues as a contributing factor to this challenge. Fenu et al. (2022) reported\nthat participants faced challenges in collecting data for training an AI system\ndue to a lack of datasets representing the diversity of the population, wh"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "s representing the diversity of the population, which\naligns with our findings (Section 4.3.2). Fenu et al. (2022) highlighted that\nadhering to regulations related to the fairness of an AI system was a reported\nchallenge. Some participants in our study also emphasised challenges in fol-\nlowing policies and regulations, citing reasons such as the lack of policies and\ninadequate implementation as discussed in Section 4.3.1. Likewise, Hopkins\nand Booth (2021) in their empirical study reported challenges faced by ML\npractitioners in detecting bias in ML, attributed to biased data or insufficient\nmodel testing, as discussed in Section 4.3.1. The participants in our study also\nemphasised the same challenge, citing factors such as the nature of data bias,\nlack of time, and a lack of tools/techniq"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "ta bias,\nlack of time, and a lack of tools/techniques to assist in detecting system biases.\nRyan et al. (2023) in their empirical study highlighted challenges anticipated\nby HCI experts and ML experts in developing a fair AI which includes ob-\ntaining high-quality data to develop and evaluate a model, which aligns with\nour findings (Section 4.3.2). However, Ryan et al. (2023) identified additional\nchallenges, including the importance of clarity regarding the model’s context\nand credibility, as well as the difficulty in aligning the mathematical defini-\ntion of fairness with the accuracy of the model, which does not align with our\nfindings.\nWhile some findings in our study align with previous research, there are\nunique contributions, particularly in uncovering team-related challenges faced\n"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "larly in uncovering team-related challenges faced\nby AI practitioners in developing a fair AI/ML system. Our study uncov-\nered challenges specific to the development team, such as having knowledge\nof bias/fairness and knowledge of AI, as discussed in Section 4.3.3. Under-\nstanding these challenges is crucial, given that AI practitioners play a pivotal\n32\nrole in designing systems that have substantial societal impact and it fosters\nresponsible and effective AI development (Orr and Davis, 2020). Similarly, our\nstudy uncovered challenges AI practitioners face in balancing real-world data\nwith training data in AI/ML system development (Section 4.3.1), a finding\nnot reported in previous studies. Additionally, we identified challenges related\nto obtaining various resources, including technologi"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": " obtaining various resources, including technological and human-related re-\nsources, which impact the development of a fair AI/ML system (Section 4.3.2).\nThese insights contribute new dimensions to the existing understanding of\nchallenges in this domain. Addressing these challenges of AI practitioners can\nhelp in developing fair AI/ML systems that can be crucial for mitigating so-\ncietal inequalities and promoting fairness in society (Holstein et al., 2019).\n5.3 Consequences of developing an unfair AI/ML system\nStudies have explored the consequences of developing an unfair AI/ML system\nfrom the perspectives of different stakeholders (Marcinkowski et al., 2020; Shin\nand Park, 2019). For example, Woodruff et al. (2018) identified that users\nreported the potential negative consequences of alg"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "eported the potential negative consequences of algorithmic unfairness which\ninclude racial discrimination and stereotyping and loss of opportunities for\npersonal advancement.\nWeidener et al. (2024) found that AI experts reported fatal outcomes for\nusers from unfair AI-based systems. Our findings differ, as our participants\ndid not mention fatal outcomes but highlighted other impacts such as users\nobtaining flawed products, and facing emotional distress, and discrimination\nas described in Section 4.4. Given the limited research on AI practitioners’ per-\nspectives on the consequences of developing an unfair AI/ML system, we con-\nducted semi-structured interviews with 22 practitioners. Our study revealed\nnew insights, identifying three main negative consequences perceived by AI\npractitioners:"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "gative consequences perceived by AI\npractitioners: those affecting organisations, users, and the practitioners them-\nselves, as discussed in Section 4.4. Understanding the consequences of develop-\ning an unfair AI/ML system may facilitate the development of specific mitiga-\ntion strategies. Addressing issues at the organisational, user, and practitioner\nlevels may contribute to more effective and comprehensive solutions in tack-\nling unfairness in AI/ML systems. For instance, our study revealed that only a\nsmall number of practitioners acknowledge the negative impact on users when\ndeveloping unfair AI/ML systems (Section 4.4.2). This highlights a critical gap\nin considering user perspectives during AI/ML development, emphasising the\nneed for more user-centric approaches (Dankloff et al., 2"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "r more user-centric approaches (Dankloff et al., 2024). Developing such\nuser-centric systems is essential for fostering user trust in AI/ML systems,\nensuring fairness and reliability.\n5.4 Strategies in ensuring fairness in AI/ML systems\nIn recent years, numerous studies have been conducted exploring strategies\nand approaches to AI/ML fairness. Several qualitative studies, such as those\n33\nby Deng et al. (2022), Richardson et al. (2021), and Balayn et al. (2023),\nhave explored AI practitioners’ experiences and perspectives on specific fair-\nness toolkits. These studies conducted semi-structured interviews with AI/ML\npractitioners to understand their practices in using different fairness toolkits.\nHowever, as our study focuses on general strategies employed by AI practi-\ntioners to ensure fa"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "tegies employed by AI practi-\ntioners to ensure fairness in AI/ML systems, the findings from these studies\ndo not align with our study.\nOn the other hand, Madaio et al. (2020) identified AI practitioners’ pro-\ncesses for recognising and addressing fairness issues in AI systems, emphasising\nunderstanding fairness as a personal priority and adhering to ad-hoc processes.\nHowever, these findings diverge from our study, which concentrates on tacti-\ncal approaches or strategies used by AI practitioners in their day-to-day lives\nto ensure the fairness of AI/ML systems. Ryan et al. (2023) noted that the\ncommon approach used by ML and HCI experts when addressing fairness was\nassociated with data used in an AI system. This finding aligns with our study\nas most of the participants in our study also d"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "dy\nas most of the participants in our study also discussed the bias mitigation strat-\negy in the AI/ML system by balancing datasets (Section 4.5.1). Similarly, in\nthe study by Ryan et al. (2023), a participant mentioned comparing model\naccuracy across demographic groups to assess fairness. This corresponds with\nour findings, where several participants also identified inaccuracies in AI/ML\nsystems through the use of evaluation metrics. However, participants in Ryan\net al. (2023)’s study mentioned not considering fairness in the AI systems they\ndevelop, contrasting with our findings. In our study, each participant reported\nemploying at least one strategy to ensure fairness in AI/ML systems.\nOur study presents unique contributions, notably in revealing strategies\nemployed by AI practitioners "
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "revealing strategies\nemployed by AI practitioners to detect bias for ensuring fairness in AI/ML\nsystems, as discussed in Section 4.5.1. These insights, including collaboration\nwith team members to mitigate data biases and a focus on individual practices\nduring AI/ML system development, are novel findings that have not been\nreported in previous research.\n5.5 Insights\nBased on the memos written for the study, we uncovered several interesting\ninsights and reflections. Research recommendations can be made based on\nthese findings and our insights.\n5.5.1 ‘No bias’- necessary but not sufficient to make a fair AI/ML system\nMost ethical guidelines in AI stress the importance of ensuring fairness , aim-\ning to eliminate bias and discrimination within AI systems. For example, Aus-\ntralia’s AI Ethics "
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": " AI systems. For example, Aus-\ntralia’s AI Ethics Principles defined ‘fairness’ as“AI systems should be inclu-\nsive and accessible, and should not involve or result in unfair discrimination\nagainst individuals, communities or groups” (Australia, 2019). Similarly, the\nEuropean Commission defined ‘Diversity, non-discrimination and fairness’ as,\n34\n“Unfair bias must be avoided, as it could have multiple negative implications,\nfrom the marginalisation of vulnerable groups to the exacerbation of prejudice\nand discrimination. Fostering diversity, AI systems should be accessible to all,\nregardless of any disability, and involve relevant stakeholders throughout their\nentire life circle” (Group, 2019). However, based on the interview with AI\npractitioners, we identified that ‘no bias’ is a crucial "
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "ioners, we identified that ‘no bias’ is a crucial element in developing a\nfair AI/ML system, but it alone is inadequate. For example, when participants\nwere asked to share their understanding of ‘fair AI/ML’, they noted that it\nmust not only be accurate but also exhibit attributes such as transparency\nand reproducibility to qualify as a fair AI/ML system, as discussed in Section\n4.2. Some research has also discussed similar notions. For example, Silberg\nand Manyika (2019) reported that the absence of unwanted bias is insufficient\nto infer that the AI system is ‘fair’. Similarly, participants in an experiment\nperceived that an unbiased AI/ML system does not necessarily mean it is\nperceived as fair, as they often found certain systems unfair despite being un-\nbiased, especially when errors w"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "despite being un-\nbiased, especially when errors were distributed unevenly among different racial\ngroups (Harrison et al., 2020).\nEven though participants had broader ideas about what constitutes a ‘fair\nAI/ML’, it was interesting to observe that their discussions predominantly\nrevolved around the concept of ‘bias’ when responding to various questions.\nFor instance, when queried about strategies employed to ensure the fairness of\nan AI/ML system, the majority focused on detecting or addressing biases in\nAI/ML systems. In contrast, only a small number delved into strategies related\nto optimising the system’s performance for fairness, as outlined in Section\n4.5. Similarly, in discussions about the consequences of developing an unfair\nAI/ML system, nearly all participants used the term ‘bias’"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "stem, nearly all participants used the term ‘bias’ and elaborated on\nthe consequences of developing a ‘biased’ system. Hence, while achieving a ‘no\nbias’ is crucial for developing a fair AI/ML system, it is essential to recognise\nthat it alone is not adequate; nevertheless, it remains a significant aspect in\nthe development of a fair AI/ML system.\n5.5.2 Data bias vs other biases in AI/ML systems\nAs previously discussed, we observed that participants primarily centered their\ndiscussions on the notion of ‘bias’ when working towards a fair AI/ML system.\nMachine learning (ML) can be prone to various biases, including biases from\ndata to algorithm, algorithm to user, and user to data. Moreover, each of these\ncategories encompasses different sub-types of biases (Mehrabi et al., 2021).\nHowever, i"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "types of biases (Mehrabi et al., 2021).\nHowever, in our case, even within the discourse on ‘bias’, a majority of par-\nticipants specifically addressed the concept of ‘data bias’. For example, when\nthe participants were asked about the challenges encountered in developing a\nfair AI/ML system, one of the aspects (concepts) they highlighted pertained\nto handling data-related issues, as detailed in Section 4.3.1. Likewise, the ma-\njority of the participants discussed the strategies related to detecting bias in\nthe data and mitigating data biases when they were asked about the strate-\ngies to ensure the fairness of the AI/ML system as discussed in Section 4.5.1.\n35\nAdditionally, participants discussed the factors leading to their challenges to\ndevelop a fair AI/ML system, mainly related to data"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "evelop a fair AI/ML system, mainly related to data bias, such as the use\nof biased training data, lack of tools to check data bias, etc. as discussed in\nSection 4.3. This indicates that among various biases, data bias stands out\nas particularly prevalent. Effectively addressing data bias is important while\ndeveloping a fair AI/ML system.\n5.5.3 Can a fair AI/ML system ever be developed?\nAs per the participants, a challenge they encountered in developing a fair\nAI/ML system revolved around obtaining the required datasets for training\nthe model. Several factors like ‘lack of cost’, ‘lack of tools/techniques’, ‘lack of\nrepresentative datasets’, and ‘lack of control over data collection’ were reported\nthat led to this challenge as discussed in Section 4.3.2. While factors like ‘lack\nof cost’, ‘"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "ection 4.3.2. While factors like ‘lack\nof cost’, ‘lack of tools/techniques’, and ‘lack of control over data collection’\ncould be addressable to improve the development of a fair AI/ML system,\nthe majority of the participants reported the ‘lack of representative datasets’\nin the real world as one of the factors leading to the challenge of obtaining\nrequired datasets. For example, participants [P1] and [P15] said,\n“In the real world, normally, we don’t get perfect data to train the model. - [P1]\n“So for example, in our human body projects, a lot of things are on bell curves\nin terms of like weight and height. And so it’s often very difficult to get those data\nat the edges of the bell curve, you don’t usually have a lot of very, very, like obese\npeople.” - [P15]\nThis particular factor, ‘the l"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "se\npeople.” - [P15]\nThis particular factor, ‘the lack of representative datasets’, appears to be\nmore persistent because we cannot change real-world data, and it may pose a\ngreater challenge that is not easily overcome. It was intriguing to learn that\nsome participants believe developing a fair AI/ML system is not possible,\nasserting that while bias can be minimised, it cannot be entirely eradicated\nfrom the systems. For example, participants [P9] and [P11] said:\n“The real world is not perfect so you don’t have all the datasets you need. So you\nwon’t be able to remove some sort of biases from them. You will have some sort of\nbiases, the only thing that we can do is reduce it to a certain acceptable level. We\nwon’t be getting a perfect fair model. It’s not there. Model reflects the data. Re"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "model. It’s not there. Model reflects the data. Real\ndata is not perfect. So you cannot expect a perfect model with it.” - [P9]\n“”In my view, because we are talking about bias here, like, there is no model I\nmean, even in AI, as far as I know, there is no perfect machine or model when pre-\ndiction is involved. So in my opinion, I guess like a fair model should be something\nthat decreases the bias, as you know, as I mean, it should decrease it. It should\ndecrease it to be very less. But then I don’t think there can be any model, which is\nnot biased.” - [P11]\nBecause only a small number of participants in our study talked about\nthis subject, there is room for further investigation into why AI practitioners\nbelieve developing a perfectly fair (bias-free) AI/ML system is not feasible.\n5.5.4 Or"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "(bias-free) AI/ML system is not feasible.\n5.5.4 Organisational Impact vs. User Well-being\nWe found that participants in our study believed that the repercussions of de-\nveloping an unfair AI/ML system impact organisations more significantly than\n36\nthe users who interact with it. When asked about the consequences of develop-\ning an unfair AI/ML system, the majority highlighted potential financial losses\nand damage to the organisation’s reputation. Participants expressed opinions\nsuch as “it could ultimately tarnish the image of these organisations (Twitter)” and\n“incur a substantial loss for the company” .Only a small number of participants\nmentioned that users may experience emotional distress and discrimination\nwhen unfair AI/ML systems are developed. Participants quoted, “the sen-\ntimen"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "re developed. Participants quoted, “the sen-\ntiments of the people can be hurt” .The limited acknowledgment of potential user\nexperiences, such as emotional distress and discrimination, suggests a poten-\ntial gap in awareness or consideration of the individual implications of unfair\nAI/ML systems. It may highlight a tendency to prioritise the broader conse-\nquences for organisations over the direct effects on the individuals interacting\nwith these systems.\n5.6 Implications\nThis section outlines implications for researchers and AI practitioners involved\nin AI/ML system development, derived from our study findings. Additionally,\nwe offer recommendations for AI practitioners and AI companies to assist in\nthe development of fair AI/ML systems.\n5.6.1 Implications for Research and Future Work\nWe"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "5.6.1 Implications for Research and Future Work\nWe developed a framework to show the relationship between AI practitioners’\nunderstanding of ‘fair AI/ML’ and the associated challenges in developing a\nfair AI/ML system, the consequences of developing an unfair AI/ML system\nperceived by them, and the strategies employed to ensure fairness in AI/ML\nsystems (Figure 8) based on empirical findings. This framework can be used\nto identify patterns, and potential areas for intervention, ultimately contribut-\ning to a more nuanced understanding of how to enhance fairness in AI/ML\nsystems. The insights drawn from this framework can inform future studies,\nshaping the direction of research in the field. Researchers can use our findings\nfor future research in the following areas:\nInvestigating factors a"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "ch in the following areas:\nInvestigating factors and solutions for the challenge of obtain-\ning required datasets: Our findings reveal that a common challenge faced\nby AI practitioners who shared their understanding of ‘fair AI/ML’ either in\nterms of the absence of biasor in terms of the presence of desirable attributes\nin AI/ML systems such as transparency, interpretability, accuracy etc. was\nobtaining necessary datasets during AI/ML system development, as discussed\nin Section 4.7.1. Future work can focus on addressing more factors leading\nto this challenge and investigating approaches to mitigate them, which can\ncontribute to the development of a fair AI/ML system.\nMapping countries’/companies’ definitions of ‘AI fairness’ with\npractitioners’ understanding: Our findings reveal variations"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "ers’ understanding: Our findings reveal variations in AI practi-\ntioners’ understanding of ‘fair AI/ML’ compared to definitions set by different\n37\ncountries and tech companies on ‘AI fairness’ (section 5.1). Future research\ncould explore the alignment between these perspectives through a mapping\nexercise.\nDelving deeper into strategies for ensuring fairness in AI/ML\nsystems: Our findings show that AI practitioners commonly use mitigating\nbias (bias-related strategy) to ensure fairness in AI/ML systems, regardless\nof how they describe ‘fair AI/ML’, as discussed in Section 4.7.3. Similarly,\nthe participants discussed the strategy of detecting inaccuracy to ensure fair-\nness in AI/ML systems; however, there is no mention of strategies to address\naccuracy-related issues for ensuring fairness."
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "ess\naccuracy-related issues for ensuring fairness. Future research can delve into\nwhy mitigating bias is the predominant strategy and explore if practitioners\nemploy strategies to address accuracy-related issues in the system to ensure\nfairness. This may help to inform the development of comprehensive strategies\nto address both bias and accuracy concerns, leading to more robust and fair\nAI/ML systems.\nExploring links between various aspects: In our study, we explored\nthe link between what AI practitioners understand by ‘fair AI/ML’ and the\nchallenges they face in development, the consequences of developing an unfair\nAI/ML systems perceived by them, and the strategies they employed to ensure\nfairness of an AI/ML system. In the future, researchers can delve into explor-\ning connections betwe"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "chers can delve into explor-\ning connections between other aspects, such as the challenges encountered in\ndeveloping a fair AI/ML system and the consequences of developing an un-\nfair AI/ML system perceived by AI practitioners. This may help to uncover\ndeeper insights and connections within the complex landscape of developing\na fair AI/ML system, guiding researchers in refining methodologies, devising\nmore effective strategies, and advancing fair and ethical practices in AI/ML\nsystem development.\n5.6.2 Implications for Practice and Recommendations\nOur study focuses on investigating AI practitioners’ experiences and percep-\ntions about various aspects related to the development of a fair AI/ML system.\nWe conducted semi-structured interviews with 22 AI practitioners, exploring\ntheirunderstan"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "with 22 AI practitioners, exploring\ntheirunderstanding of ‘fair AI/ML’, the challenges encountered in its de-\nvelopment, consequences of developing an unfair AI/ML system, and the\nstrategies employed to ensure the fairness of an AI/ML system. Our find-\nings provide AI practitioners with valuable insights, into how people in the\nsame field understand a ‘fair AI/ML’, the challenges they encounter, the con-\nsequences of developing an unfair AI/ML system, and the strategies they em-\nploy to ensure fairness in an AI/ML system. This comprehensive understand-\ning, derived from real-world experiences, can inform practitioners’ approaches,\nenhance decision-making, and contribute to the use of more effective strategies\nfor developing a fair AI/ML system. It may provide a practical and grounded\npersp"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "tem. It may provide a practical and grounded\nperspective that can guide practitioners in navigating the complexities of fair-\nness in their AI/ML development processes.\n38\nDrawing from our study’s findings, we present some recommendations for\nAI practitioners and AI companies to support the development of a fair AI/ML\nsystem, as detailed below.\nRecommendation 1: Striking a balance between the fairness of\na system and its working version: Several participants in our study high-\nlighted the challenge of developing their envisioned ideal system, attributing\nit to factors like a shortage of time. Consequently, they prioritise creating a\nfunctional system over ensuring its fairness, as discussed in Section 4.3.1. AI\nmanagers can help AI practitioners by fostering a culture that values and pri-\n"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "oners by fostering a culture that values and pri-\noritises fairness in AI/ML system development. They can allocate resources,\nboth in terms of time and support, to enable practitioners to strike a balance\nbetween developing a working system and ensuring its fairness.\nRecommendation 2: Providing AI practitioners with necessary\ntools/techniques: Many participants in our study emphasised the challenges\nof developing a fair AI/ML system, citing a lack of tools, or techniques as dis-\ncussed in Sections 4.3.1 and 4.3.2. They specifically pointed out challenges in\ndetecting and addressing data bias and obtaining necessary datasets due to the\nabsence of adequate tools. AI companies can provide substantial support by\ninvesting in the development and provision of specialised tools, and/or tech-\nniqu"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": " provision of specialised tools, and/or tech-\nniques aimed at addressing the challenges highlighted by participants (Holstein\net al., 2019).\nRecommendation 3: Focusing on enhancing own knowledge and\nawareness of different concepts: The majority of participants in our study\nacknowledged facing challenges in grasping the concepts of ‘bias’ and ‘fairness’\nas discussed in Section 4.3.3. They attributed this challenge to a lack of aware-\nness and knowledge about these concepts, as well as a deficit in understanding\nthe domain they work in. AI practitioners can take proactive steps such as\nseeking additional training or education on the concepts of ‘bias’ and ‘fair-\nness.’ Engaging in domain-specific learning to enhance their understanding of\nthe context they work in might also be beneficial. St"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": " context they work in might also be beneficial. Staying informed about the\nlatest developments and best practices in AI fairness can contribute to a more\ncomprehensive understanding of these concepts.\nRecommendation 4: Prioritising users in AI/ML system devel-\nopment: In discussions about the consequences of developing unfair AI/ML\nsystems perceived by the participants, most participants focused on the nega-\ntive impacts on organisations, including financial losses and reputational reper-\ncussions (Section 5). Interestingly, only a small number recognised the poten-\ntial emotional distress and discrimination experienced by end users as a con-\nsequence of such systems. AI practitioners can make a conscious effort to shift\nthe focus from solely considering organisational consequences to unde"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "ly considering organisational consequences to understand-\ning the direct impact on users. This might allow them to identify and address\npotential biases and discriminatory outcomes, contributing to the development\nof fair AI/ML systems that treat users equitably. Involving users across var-\nious phases of AI/ML system development to gather feedback might help in\nensuring user-centric AI/ML system development. A recent study highlights\n39\nthe need for increased user engagement throughout algorithmic development\nto enhance fairness in AI algorithms (Dankloff et al., 2024).\nRecommendation 5: Updating and adapting AI ethics policies\nin organisations: Participants in our study identified challenges in adhering\nto policies and regulations within their organisations, citing outdated AI ethics\npol"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "their organisations, citing outdated AI ethics\npolicies and a lack of adaptation as the factors leading to those challenges,\nas discussed in Section 4.3.1. To address this, AI companies can prioritise\nupdating and adapting their AI ethics policies, ensuring strict adherence by\npractitioners. This proactive approach can help ensure that AI practitioners\nare equipped with the latest guidelines to navigate complex ethical challenges,\npromoting responsible AI development.\n6 Limitations and Threats to Validity\nWhile we advertised our study on platforms such as LinkedIn and Twitter\nto attract participants globally, our data collection lacks an even distribution\nof participants worldwide. The majority of study participants are based in\nAustralia. The findings of our study hold the most relevance "
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "The findings of our study hold the most relevance for participants’\norganisations and their respective countries, potentially extending to similar\ncontexts. However, generalising these findings to the entire global software en-\ngineering community is deemed impractical in practice (Masood et al., 2020).\nThe limitation of this study is that all interview participants held purely tech-\nnical roles, such as AI/ML developers, engineers, experts, and data scientists\ninvolved in AI/ML system design and development. The study did not in-\nclude a broader range of profiles like data science managers, business experts,\nethics/compliance officers, risk managers, heads of innovation, and heads of\noperations. Including these profiles could have provided a wider range of per-\nspectives on AI/ML fairness"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": " a wider range of per-\nspectives on AI/ML fairness. Future studies should incorporate these roles to\nbetter understand their perspectives on AI/ML fairness.\nLikewise, our main interview guide was developed after conducting two\npilot interviews. The interview recordings underwent automatic transcription,\nand any errors introduced during this process were manually rectified by listen-\ning to each audio recording during the coding phase. In the interviews, there\ncould be a possibility of misalignment between our intended questions and\nparticipants’ understanding, leading to potential misinterpretations or mis-\nunderstandings. To address this, we employed follow-up questions to ensure\nclarity on the participants’ statements.\nAll four authors were involved in designing the interview guide, with"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "re involved in designing the interview guide, with the\ninitial coding primarily handled by the first author. However, all authors ac-\ntively participated in refining and finalising the codes, concepts, and categories\nthrough collaborative discussions. We have also included various interview\nquotes as examples, aiming to minimise any potential reporting biases in the\nstudy.\nIn addition, there could be a potential risk to the research’s internal va-\nlidity when using the payment for the second round of data collection. As\n40\na way of mitigating this risk, we initially provided the candidates with an\nanonymous pre-interview questionnaire asking them about their years of ex-\nperience in AI/ML system development. Using this information, we selected\nparticipants for interviews, and approval for "
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "ted\nparticipants for interviews, and approval for payment was granted only after\nconfirming alignment with our predetermined participation criteria. The pro-\ncess was carried out with ethics approval. Candidates with no experience in\nAI/ML system development were not selected for the interview.\n7 Conclusion\nThis study aimed to investigate AI practitioners’ perspectives and experiences\nin developing a fair AI/ML system, recognising their pivotal role in devel-\nopment and deployment. The study contributes to gaining insights into the\nindustry’s standpoint on the understanding of a ‘fair AI/ML’, the challenges\ninvolved in its development, the consequences of developing an unfair AI/ML\nsystem perceived by them, and the strategies they employed to ensure fairness\nof an AI/ML system.\nWe conducte"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "to ensure fairness\nof an AI/ML system.\nWe conducted semi-structured interviews with 22 AI practitioners to fulfill\nthe objective of our study and analysed the qualitative data using the STGT\nfor data analysis (Hoda, 2021). The analysis revealed two categories of AI prac-\ntitioners’ understanding of ‘fair AI/ML’ including, (i) In terms of the absence\nof bias and (ii) In terms of the presence of desirable attributes in AI/ML sys-\ntems. We also categorised the challenges of the participants in developing a\nfair AI/ML system into three sections including, (i) Process-related challenges ,\n(ii)Resource-related challenges , and (iii) Team-related challenges . Similarly,\nour analysis showed three categories of negative consequences perceived by\nparticipants in developing an unfair AI/ML system: (i"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "ticipants in developing an unfair AI/ML system: (i) Impact on organisa-\ntions , (ii) Impact on users , and (iii) Impact on practitioners . We also classified\nthestrategies employed by participants to ensure the fairness of an AI/ML\nsystem into two categories: (i) Bias-related strategies and (ii) Performance-\nrelated strategies . Based on the findings, we also developed a framework to show\nthe relationship between AI practitioners’ understanding of ‘fair AI/ML’ and\nthree other aspects including, (i) their challenges in developing a fair AI/ML\nsystem, (ii) the consequences of developing an unfair AI/ML system perceived\nby them and (iii) their strategies to ensure the fairness of an AI/ML system.\nOur findings offer valuable insights into the industry’s perspective and\nexperiences in developin"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "ndustry’s perspective and\nexperiences in developing a fair AI/ML system, aiding the AI research com-\nmunity in better understanding how AI practitioners perceive and experience\nthis process. We also identified areas that need further investigation within\nthe AI research community, enabling researchers to make more informed deci-\nsions about the direction of their studies. This might ensure that their efforts\naddress the critical areas identified by the study for further exploration. We\nalso offered recommendations to AI practitioners and AI companies, aiming\nto assist in enhancing the development of a fair AI/ML system.\n41\nAcknowledgements Aastha Pant is supported by the Faculty of IT Ph.D. scholarship\nfrom Monash University. C. Tantithamthavorn is partially supported by the Australian\nRes"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "avorn is partially supported by the Australian\nResearch Council’s Discovery Early Career Researcher Award (DECRA) funding scheme\n(DE200100941). We would like to thank all the interviewees for their participation in our\nstudy.\n8 Appendices\nA Appendix A: Interview Protocol\nSection A: Demographic Information (via Qualtrics )1. Your full name:\n2. Please enter your email address so the researcher can contact you to schedule a time for\nan interview:\n3. What is your current job title?\n–AI Engineer\n–AI/ML/Data Scientist\n–AI/ML Expert\n–AI/ML Practitioner\n–AI/ML Developer\n–Other:\n4. How many years of experience do you have in the area of AI/ML system development?\n–No Experience\n–Less than 1 year\n–Between 1 to 2 years\n–Between 2 to 5 years\n–More than 5 years\n5. How old are you?\n–Below 20\n–20-25\n–26-3"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "5 years\n5. How old are you?\n–Below 20\n–20-25\n–26-30\n–31-35\n–36-40\n–41-45\n–46-50\n–50+\n6. How would you describe your gender?\n–Woman\n–Man\n–Non-binary/ gender diverse\n–My gender identity isn’t listed. I identify as:\n–Prefer not to say\n7. What is your country of residence?\n8. What is the highest degree or level of education you have completed?\n–High School\n–Bachelor degree\n–Master degree\n–Ph.D. or Higher\n–Prefer not to answer\n–Other:\n9. What activities are you involved in? Select allthat apply.\n–Model requirements\n–Data collection\n–Data cleaning\n–Data labeling\n–Feature engineering\n–Model training\n–Model evaluation\n–Model deployment\n–Model monitoring\n–Other:\n42\nSection B: Practitioners’ Perception and Experiences on AI/ML Fairness (via semi-\nstructured interviews)\nSection B.1- Questions on ‘AI/"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "uctured interviews)\nSection B.1- Questions on ‘AI/ML Bias’\n1. Can you briefly tell me about your professional background and current role?\n2. Are you aware of the term ‘AI/ML bias’?\n(a) (If yes), what do you understand by the term ‘AI/ML bias’?\n3. Based on your professional experience, can you tell me if something like ‘AI/ML bias’ exists\nin practice?\n(a) (If yes), why do you say so? In your professional experience, have you come across any\ncases related to AI/ML bias?)\n(b) (If yes), can you give an example?\n(c) What kind of AI/ML system were you developing?\n(d) How did you find out that the system was biased?\n(e) What kind of bias crept into the system?\n(f) What caused the bias?\n(g) Once you found that the system was biased, did you deploy that system? (Yes/No)\ni. (If yes), any strategies"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": " that system? (Yes/No)\ni. (If yes), any strategies/methods were used to mitigate those biases before deploying\nit?\nii. (If yes), what strategies/methods did you use?\niii. (If not), why weren’t any strategies/methods used?\n(h) (If no), do you think the term ‘AI/ML bias’ is theoretical and does not exist in practice?\ni. You do not have to deal with/haven’t dealt with any AI/ML biases? Can you tell\nme based on your experience?\nii. Why don’t you have to deal with AI/ML bias?\n4. Based on your professional experience, what can help you in addressing/ preventing/ miti-\ngating biases in the AI/ML system you develop?\n(a) In what way can it help you?\nSection B.2- Questions on ‘Fair AI/ML’\n1. Are you aware of the term ‘fair AI/ML’?\n(a) (If yes), what would you consider as fair? Can you give an exampl"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "would you consider as fair? Can you give an example?\n2. Based on your professional experience, do you think it is important to create a fair AI/ML\nsystem?\n(a) (If yes), why is it important for the AI/ML system to be fair?\n(b) (If not), why is it not important to create fair AI/ML systems?\n3. Based on your professional experience, how do you know the AI/ML system that you devel-\noped is fair? Do you use any strategies to ensure its fairness?\n(a) (If yes), what strategies/ tools/ techniques do you use?\n(b) (If not), is it not mandatory to use tools/strategies/techniques?\n(c) Why is it not mandatory?\n4. Based on your professional experience, do you face any challenges in developing a fair AI/ML\nsystem?\n(a) (If yes), what challenges do you face?\n(b) What do you think are the factors leading to"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "?\n(b) What do you think are the factors leading to those challenges?\n5. Based on your professional experience, what does it take to develop AI/ML systems that are\nfair?\n(a) Why?\n6. If an AI/ML system is unfair, who does it impact, according to you?\n(a) (If yes), how does it impact them? Can you give an example?\n(b) (If not), why not? Can you give an example?\nData Availability Statement\nThe data are protected and are not available due to data privacy laws.\nConflict of interest\nConflicts of interest include Klaas-Jan Stol, Paul Ralph, Brian Fitzgerald, Burak Turhan,\nPatanamon Thongtanunam.\n43\nReferences\nAngwin J, Larson J, Mattu S, Kirchner L (2016) Machine bias. URL https://www.\npropublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing ,\naccessed 17 January 2024\nAugustin L"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "l-sentencing ,\naccessed 17 January 2024\nAugustin L, Bressler D, Smith G (2002) Accelerating software development through collab-\noration. In: Proceedings of the 24th International Conference on Software Engineering,\npp 559–563, DOI https://doi.org/10.1145/581339.581409\nAustralia G (2019) Australia’s AI ethics principles. URL https://www.industry.\ngov.au/publications/australias-artificial-intelligence-ethics-framework/\naustralias-ai-ethics-principles , accessed 10 January 2024\nBacelar M (2021) Monitoring bias and fairness in machine learning models: A review. Sci-\nenceOpen Preprints DOI 10.14293/S2199-1006.1.SOR-.PP59WRH.v1\nBalayn A, Yurrita M, Yang J, Gadiraju U (2023) “Fairness toolkits, A checkbox culture?”\nOn the factors that fragment developer practices in handling algorithmic harms. I"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "veloper practices in handling algorithmic harms. In: Pro-\nceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society, pp 482–495,\nDOI https://doi.org/10.1145/3600211.3604674\nBaltes S, Ralph P (2022) Sampling in software engineering research: A critical review\nand guidelines. Empirical Software Engineering 27(4):94, DOI https://doi.org/10.1007/\ns10664-021-10072-8\nBinns R (2018) Fairness in machine learning: Lessons from political philosophy. In: Confer-\nence on Fairness, Accountability and Transparency, PMLR, pp 149–159\nCaliskan A, Bryson JJ, Narayanan A (2017) Semantics derived automatically from language\ncorpora contain human-like biases. Science 356(6334):183–186, DOI 10.1126/science.aal42\nCaton S, Haas C (2020) Fairness in machine learning: A survey. ACM Computing Surveys\n56(16"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "ne learning: A survey. ACM Computing Surveys\n56(166):1–38, DOI https://doi.org/10.1145/3616865\nChen P, Wu L, Wang L (2023) AI fairness in data management and analytics: A review on\nchallenges, methodologies and applications. Applied Sciences 13(18):10258, DOI https:\n//doi.org/10.3390/app131810258\nChouldechova A, Roth A (2018) The frontiers of fairness in machine learning. arXiv preprint\narXiv:181008810\nD’Amour A, Srinivasan H, Atwood J, Baljekar P, Sculley D, Halpern Y (2020) Fairness is not\nstatic: Deeper understanding of long term fairness via simulation studies. In: Proceedings\nof the 2020 Conference on Fairness, Accountability, and Transparency, pp 525–534, DOI\nhttps://doi.org/10.1145/3351095.3372878\nDankloff M, Skoric V, Sileno G, Ghebreab S, Ossenbruggen Jv, Beauxis-Aussalet E (2024)"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "reab S, Ossenbruggen Jv, Beauxis-Aussalet E (2024)\nAnalysing and organising human communications for AI fairness assessment: Use cases\nfrom the dutch public sector. AI & Society pp 1–21, DOI https://doi.org/10.1007/\ns00146-024-01974-4\nDeng WH, Nagireddy M, Lee MSA, Singh J, Wu ZS, Holstein K, Zhu H (2022) Exploring\nhow machine learning practitioners (try to) use fairness toolkits. In: Proceedings of the\n2022 ACM Conference on Fairness, Accountability, and Transparency, pp 473–484, DOI\nhttps://doi.org/10.1145/3531146.3533113\nDrivenData (2024) Deon. URL https://deon.drivendata.org/ , accessed 17 January 2024\nFenu G, Galici R, Marras M (2022) Experts’ view on challenges and needs for fairness in\nartificial intelligence for education. In: International Conference on Artificial Intelligence\nin "
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "national Conference on Artificial Intelligence\nin Education, Springer, pp 243–255, DOI https://doi.org/10.1007/978-3-031-11644-5 20\nFinkelstein A, Harman M, Mansouri SA, Ren J, Zhang Y (2008) “Fairness analysis” in\nrequirements assignments. In: 16th IEEE International Requirements Engineering Con-\nference, IEEE, pp 115–124, DOI 10.1109/RE.2008.61\nFriedler SA, Scheidegger C, Venkatasubramanian S, Choudhary S, Hamilton EP, Roth D\n(2019) A comparative study of fairness-enhancing interventions in machine learning. In:\nProceedings of the Conference on Fairness, Accountability, and Transparency, pp 329–338,\nDOI https://doi.org/10.1145/3287560.3287589\nGoogle (2022) Responsible AI practices. URL https://ai.google/responsibility/\nresponsible-ai-practices/ , accessed 10 January 2024\n44\nGroup HLE (20"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "tices/ , accessed 10 January 2024\n44\nGroup HLE (2019) Ethics guidelines for trustworthy AI. URL https://digital-strategy.\nec.europa.eu/en/library/ethics-guidelines-trustworthy-ai , accessed 10 January\n2024\nHabibullah KM, Gay G, Horkoff J (2023) Non-functional requirements for machine learning:\nUnderstanding current use and challenges among practitioners. Requirements Engineering\n28(2):283–316, DOI https://doi.org/10.1007/s00766-022-00395-3\nHarrison G, Hanson J, Jacinto C, Ramirez J, Ur B (2020) An empirical study on the\nperceived fairness of realistic, imperfect machine learning models. In: Proceedings of\nthe 2020 Conference on Fairness, Accountability, and Transparency, pp 392–402, DOI\nhttps://doi.org/10.1145/3351095.3372831\nHoda R (2021) Socio-technical grounded theory for software engin"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "Socio-technical grounded theory for software engineering. IEEE Transactions\non Software Engineering 48(10):3808–3832, DOI 10.1109/TSE.2021.3106280\nHolstein K, Wortman Vaughan J, Daum´ e III H, Dudik M, Wallach H (2019) Improving\nfairness in machine learning systems: What do industry practitioners need? In: Proceed-\nings of the 2019 CHI Conference on Human Factors in Computing Systems, pp 1–16,\nDOI https://doi.org/10.1145/3290605.3300830\nHopkins A, Booth S (2021) Machine learning practices outside big tech: How resource con-\nstraints challenge responsible development. In: Proceedings of the 2021 AAAI/ACM Con-\nference on AI, Ethics, and Society, ACM New York, United States, pp 134–145, DOI\nhttps://doi.org/10.1145/3461702.3462527\nHua SS, Belfield H (2020) AI & antitrust: Reconciling tensions "
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "eld H (2020) AI & antitrust: Reconciling tensions between competition law\nand cooperative AI development. Yale JL & Tech 23:415\nHutchinson B, Mitchell M (2019) 50 years of test (un) fairness: Lessons for machine learning.\nIn: Proceedings of the Conference on Fairness, Accountability, and Transparency, ACM\nNew York, USA, pp 49–58, DOI https://doi.org/10.1145/3287560.3287600\nIBM (2022) Everyday ethics for AI. URL https://www.ibm.com/design/ai/ethics/\neveryday-ethics , accessed 10 January 2024\nIBM (2024a) AI factsheets. URL https://dataplatform.cloud.ibm.com/docs/content/\nwsj/analyze-data/factsheets-model-inventory.html?context=cpdaas , accessed 17\nJanuary 2024\nIBM (2024b) AI fairness 360. URL https://www.ibm.com/opensource/open/projects/\nai-fairness-360/ , accessed 17 January 2024\nJohnson B,"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "airness-360/ , accessed 17 January 2024\nJohnson B, Brun Y (2022) Fairkit-learn: A fairness evaluation and comparison toolkit. In:\nProceedings of the ACM/IEEE 44th International Conference on Software Engineering:\nCompanion Proceedings, pp 70–74, DOI https://doi.org/10.1145/3510454.3516830\nMadaio M, Egede L, Subramonyam H, Wortman Vaughan J, Wallach H (2022) Assessing\nthe fairness of AI systems: AI practitioners’ processes, challenges, and needs for support.\nProceedings of the ACM on Human-Computer Interaction 6(CSCW1):1–26, DOI https:\n//doi.org/10.1145/3512899\nMadaio MA, Stark L, Wortman Vaughan J, Wallach H (2020) Co-designing checklists to\nunderstand organizational challenges and opportunities around fairness in AI. In: Pro-\nceedings of the 2020 CHI Conference on Human Factors in Computi"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "he 2020 CHI Conference on Human Factors in Computing Systems, ACM\nNew York, USA, pp 1–14, DOI https://doi.org/10.1145/3313831.3376445\nMajumder S, Chakraborty J, Bai GR, Stolee KT, Menzies T (2023) Fair enough: Search-\ning for sufficient measures of fairness. ACM Transactions on Software Engineering and\nMethodology 32(6):1–22, DOI https://doi.org/10.1145/3585006\nMarcinkowski F, Kieslich K, Starke C, L¨ unich M (2020) Implications of AI (un-) fair-\nness in higher education admissions: The effects of perceived AI (un-) fairness on\nexit, voice and organizational reputation. In: Proceedings of the 2020 Conference on\nFairness, Accountability, and Transparency, ACM New York, USA, pp 122–130, DOI\nhttps://doi.org/10.1145/3351095.3372867\nMartin N (2018) Are AI hiring programs eliminating bias or mak"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "18) Are AI hiring programs eliminating bias or making it\nworse? URL https://www.forbes.com/sites/nicolemartin1/2018/12/13/\nare-ai-hiring-programs-eliminating-bias-or-making-it-worse/?sh=552bb0cc22b8 ,\naccessed 17 January 2024\nMasood Z, Hoda R, Blincoe K (2020) How agile teams make self-assignment work: A\ngrounded theory study. Empirical Software Engineering 25:4962–5005, DOI https://doi.\norg/10.1007/s10664-020-09876-x\n45\nMehrabi N, Morstatter F, Saxena N, Lerman K, Galstyan A (2021) A survey on bias and\nfairness in machine learning. ACM Computing Surveys (CSUR) 54(6):1–35, DOI https:\n//doi.org/10.1145/3457607\nMicrosoft (2024a) Microsoft responsible AI standard. URL https://www.microsoft.com/\nen-us/ai/responsible-ai?activetab=pivot1%3aprimaryr6 , accessed 10 January 2024\nMicrosoft (2024b) A"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "yr6 , accessed 10 January 2024\nMicrosoft (2024b) AI fairness checklist. URL https://www.microsoft.com/en-us/\nresearch/project/ai-fairness-checklist/ , accessed 17 January 2024\nOrr W, Davis JL (2020) Attributions of ethical responsibility by artificial intelligence practi-\ntioners. Information, Communication & Society 23(5):719–735, DOI https://doi.org/10.\n1080/1369118X.2020.1713842\nPagano TP, Loureiro RB, Lisboa FV, Peixoto RM, Guimar˜ aes GA, Cruz GO, Araujo MM,\nSantos LL, Cruz MA, Oliveira EL, et al. (2023) Bias and unfairness in machine learning\nmodels: A systematic review on datasets, tools, fairness metrics, and identification and\nmitigation methods. Big Data and Cognitive Computing 7(1):15, DOI https://doi.org/\n10.3390/bdcc7010015\nPant A, Hoda R, Spiegler SV, Tantithamthavorn C, Turh"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "t A, Hoda R, Spiegler SV, Tantithamthavorn C, Turhan B (2023) Ethics in the age of AI:\nAn analysis of AI practitioners’ awareness and challenges. ACM Transactions on Software\nEngineering and Methodology 33(80):1–35, DOI https://doi.org/10.1145/3635715\nPant A, Hoda R, Turhan B, Tantithamthavorn C (2024) What do AI/ML practitioners\nthink about AI/ML bias? URL https://arxiv.org/abs/2407.08895 ,2407.08895\nPessach D, Shmueli E (2022) A review on fairness in machine learning. ACM Computing\nSurveys (CSUR) 55(3):1–44, DOI https://doi.org/10.1145/3494672\nPrates MO, Avelar PH, Lamb LC (2020) Assessing gender bias in machine translation:\nA case study with google translate. Neural Computing and Applications 32:6363–6381,\nDOI https://doi.org/10.1007/s00521-019-04144-6\nRichardson B, Garcia-Gathright J, "
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "521-019-04144-6\nRichardson B, Garcia-Gathright J, Way SF, Thom J, Cramer H (2021) Towards fairness in\npractice: A practitioner-oriented rubric for evaluating fair ML toolkits. In: Proceedings\nof the 2021 CHI Conference on Human Factors in Computing Systems, ACM New York,\nUSA, pp 1–13, DOI https://doi.org/10.1145/3411764.3445604\nRyan S, Nadal C, Doherty G (2023) Integrating fairness in the software design process: An\ninterview study with HCI and ML experts. IEEE Access 11:29296–29313, DOI 10.1109/\nACCESS.2023.3260639\nSeaman CB (1999) Qualitative methods in empirical studies of software engineering. IEEE\nTransactions on Software Engineering 25(4):557–572, DOI 10.1109/32.799955\nShin D, Park YJ (2019) Role of fairness, accountability, and transparency in algorithmic\naffordance. Computers in Hu"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "parency in algorithmic\naffordance. Computers in Human Behavior 98:277–284, DOI https://doi.org/10.1016/j.\nchb.2019.04.019\nSilberg J, Manyika J (2019) Notes from the AI frontier: Tackling bias in AI (and in humans).\nMcKinsey Global Institute 1(6):1–31\nSrivastava M, Heidari H, Krause A (2019) Mathematical notions vs. human perception of\nfairness: A descriptive approach to fairness for machine learning. In: Proceedings of the\n25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,\nACM New York, USA, pp 2459–2468, DOI https://doi.org/10.1145/3292500.3330664\nUeda D, Kakinuma T, Fujita S, Kamagata K, Fushimi Y, Ito R, Matsui Y, Nozaki T,\nNakaura T, Fujima N, et al. (2024) Fairness of artificial intelligence in healthcare: Review\nand recommendations. Japanese Journal of Rad"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "eview\nand recommendations. Japanese Journal of Radiology 42(1):3–15, DOI https://doi.org/\n10.1007/s11604-023-01474-3\nVasudevan S, Kenthapadi K (2020) Lift: A scalable framework for measuring fairness in ML\napplications. In: Proceedings of the 29th ACM International Conference on Information\n& Knowledge Management, ACM New York, USA, pp 2773–2780, DOI https://doi.org/\n10.1145/3340531.3412705\nVerma S, Rubin J (2018) Fairness definitions explained. In: Proceedings of the International\nWorkshop on Software Fairness, ACM New York, USA, pp 1–7, DOI https://doi.org/10.\n1145/3194770.3194776\nWan M, Zha D, Liu N, Zou N (2023) In-processing modeling techniques for machine learning\nfairness: A survey. ACM Transactions on Knowledge Discovery from Data 17(3):1–27,\nDOI https://doi.org/10.1145/3551390\n46\n"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "7(3):1–27,\nDOI https://doi.org/10.1145/3551390\n46\nWang Y, Song Y, Ma Z, Han X (2023) Multidisciplinary considerations of fairness in medical\nAI: A scoping review. International Journal of Medical Informatics 178:105175, DOI\nhttps://doi.org/10.1016/j.ijmedinf.2023.105175\nWeidener L, Fischer M, et al. (2024) Role of ethics in developing AI-based applications\nin medicine: Insights from expert interviews and discussion of implications. JMIR AI\n3(1):e51204, DOI 10.2196/51204\nWoodruff A, Fox SE, Rousso-Schindler S, Warshaw J (2018) A qualitative exploration of\nperceptions of algorithmic fairness. In: Proceedings of the 2018 CHI Conference on Human\nFactors in Computing Systems, ACM New York, USA, pp 1–14, DOI https://doi.org/10.\n1145/3173574.3174230\nXavier B (2024) Biases within AI: Challenging t"
  },
  {
    "arxiv_id": "2403.15481",
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and\n  Strategies in AI/ML Development",
    "chunk": "30\nXavier B (2024) Biases within AI: Challenging the illusion of neutrality. AI & Society pp\n1–2, DOI https://doi.org/10.1007/s00146-024-01985-1\nXivuri K, Twinomurinzi H (2021) A systematic review of fairness in artificial intelligence al-\ngorithms. In: Responsible AI and Analytics for an Ethical and Inclusive Digitized Society,\nSpringer, vol 12896, pp 271–284, DOI https://doi.org/10.1007/978-3-030-85447-8 24\nZhang J, Shu Y, Yu H (2023) Fairness in design: A framework for facilitating ethical artificial\nintelligence designs. International Journal of Crowd Science 7(1):32–39, DOI 10.26599/\nIJCS.2022.9100033\n"
  },
  {
    "arxiv_id": "2103.15294",
    "title": "\"Weak AI\" is Likely to Never Become \"Strong AI\", So What is its Greatest\n  Value for us?",
    "chunk": "arXiv:2103.15294v1  [cs.AI]  29 Mar 20211\n“Weak AI” is Likely to Never Become “Strong AI”,\nSo What is its Greatest Value for us?\n⋆Bin Liu\nFirst posted March 30th, 2021\nAbstract\nAI has surpassed humans across a variety of tasks such as imag e classiﬁcation, playing games (e.g., go,\n“Starcraft” and poker), and protein structure prediction. However, at the same time, AI is also bearing serious\ncontroversies. Many researchers argue that little substan tial progress has been made for AI in recent decades. In\nthis paper, the author (1) explains why controversies about AI exist; (2) discriminates two paradigms of AI research,\ntermed “weak AI” and “strong AI” (a.k.a. artiﬁcial general i ntelligence); (3) clariﬁes how to judge which paradigm\na research work should be classiﬁed into; (4) discusses w"
  },
  {
    "arxiv_id": "2103.15294",
    "title": "\"Weak AI\" is Likely to Never Become \"Strong AI\", So What is its Greatest\n  Value for us?",
    "chunk": "rch work should be classiﬁed into; (4) discusses what is the greatest value of “weak AI” if it has no chance\nto develop into “strong AI”.\nIndex Terms\nArtiﬁcial intelligence, artiﬁcial general intelligence, deep learning, weak AI, strong AI\nI. I NTRODUCTION\nThe last decade has seen impressive applications of AI repre sented mostly by deep neural networks,\ni.e., deep learning [1]. The striking point lies in that the c omputing agent has reached and even surpassed\nhumans in many tasks, e.g., image classiﬁcation [2], speech recognition [3, 4], games [5–7], protein\nstructure prediction [8]. Even ten years ago, it was hard to i magine that AI would achieve so many\namazing breakthroughs.\nOn the other side, AI is also bearing serious controversies d uring the same period. Among the critics,\nJudea "
  },
  {
    "arxiv_id": "2103.15294",
    "title": "\"Weak AI\" is Likely to Never Become \"Strong AI\", So What is its Greatest\n  Value for us?",
    "chunk": "d uring the same period. Among the critics,\nJudea Pearl, a pioneer for probabilistic reasoning in AI and a winner of the Turing award, argues that\n“...all the impressive achievements of deep learning amount to j ust curve ﬁtting ,” and a necessary ability\nto be supplemented for AI is causal reasoning [9, 10]. Gary Ma rcus, a professor of cognitive science,\n⋆B. Liu is with Zhejiang Lab, Hangzhou, China. e-mail: bins@i eee.org or liubin@zhejianglab.com.\n2\nsummarizes ten limitations of deep learning [11], namely, “ ... it is data-hungry, ... it has limited capacity\nfor transfer, ... it has no natural way to deal with hierarchi cal structure, ... it struggles with open-ended\ninference, ... it is not sufﬁciently transparent, ... it has not been well integrated with prior knowledge,\n... it canno"
  },
  {
    "arxiv_id": "2103.15294",
    "title": "\"Weak AI\" is Likely to Never Become \"Strong AI\", So What is its Greatest\n  Value for us?",
    "chunk": "well integrated with prior knowledge,\n... it cannot inherently distinguish causation from correl ation, ... it presumes a largely stable world, in\nways that may be problematic, ... it works well as an approxim ation, but its answers often cannot be fully\ntrusted, ... it is difﬁcult to engineer with ”. In a recent issue of the journal Frontiers in Psychology, a nother\ncognitive scientist J. Mark Bishop argues that AI “ is stupid and causal reasoning will not ﬁx it ” [12].\nIn this paper, I attempt to concisely respond to current cont roversies about AI. Speciﬁcally, I emphasize\ndiscrimination between two paradigms of AI research, namel y “weak AI” and “strong AI” (Section II);\nprovide a conceptual guide to judge which paradigm a researc h work should be classiﬁed into (Section\nII-A), explain"
  },
  {
    "arxiv_id": "2103.15294",
    "title": "\"Weak AI\" is Likely to Never Become \"Strong AI\", So What is its Greatest\n  Value for us?",
    "chunk": "k should be classiﬁed into (Section\nII-A), explain why controversies about AI last (Section III ), present major views on whether “weak AI”\nwill grow into “strong AI” (Section IV) and discuss what is th e greatest value of “weak AI” if it has no\nchance to become “strong AI” (Section V).\nII. W HAT DO “WEAK AI” AND “STRONG AI” M EAN?\n“Weak AI” and “Strong AI” are two terms coined by John Searle i n the “Chinese room argument”\n(CRA) [13]. CRA is a thought experiment as follows: “ Searle imagines himself alone in a room following\na computer program for responding to Chinese characters sli pped under the door. Searle understands\nnothing of Chinese, and yet, by following the program for man ipulating symbols and numerals just as\na computer does, he sends appropriate strings of Chinese cha racter"
  },
  {
    "arxiv_id": "2103.15294",
    "title": "\"Weak AI\" is Likely to Never Become \"Strong AI\", So What is its Greatest\n  Value for us?",
    "chunk": "he sends appropriate strings of Chinese cha racters back out under the door, and this\nleads those outside to mistakenly suppose there is a Chinese speaker in the room ” [14]. The term “strong\nAI” entails that, “ ... the computer is not merely a tool in the study of the mind; r ather, the appropriately\nprogrammed computer really is a mind, in the sense that compu ters given the right programs can be\nliterally said to understand and have other cognitive state s.” In contrast, the term “weak AI” implies that\n“... the principal value of the computer in the study of the min d is that it gives us a very powerful tool. ” J.\nMark Bishop summarizes that ‘ ‘weak AI focuses on epistemic issues relating to engineerin g a simulation\nof human intelligent behavior, whereas strong AI, in seekin g to engin"
  },
  {
    "arxiv_id": "2103.15294",
    "title": "\"Weak AI\" is Likely to Never Become \"Strong AI\", So What is its Greatest\n  Value for us?",
    "chunk": " behavior, whereas strong AI, in seekin g to engineer a computational system with all\nthe causal power of a mind, focuses on the ontological ” [12].\nI borrow the terms “weak AI” and “strong AI” here without an in tent to discuss CRA. See related\ndiscussions in e.g., [15–18].\n3\nSimply put, “weak AI” represents computational systems tha t exhibit as if they own human intelligence,\nbut they do not. In contrast, “strong AI” represents computa tional systems that have human intelligence.\nCorrespondingly, all AI research can be categorized into tw o paradigms: one is targeted for realizing\n“strong AI”; and the other produces advanced “weak AI” syste ms to meet a variety of practical needs.\nA. How to Judge a Research Work Belongs to Which Paradigm?\nThe biggest motivation for realizing “strong AI”"
  },
  {
    "arxiv_id": "2103.15294",
    "title": "\"Weak AI\" is Likely to Never Become \"Strong AI\", So What is its Greatest\n  Value for us?",
    "chunk": "?\nThe biggest motivation for realizing “strong AI” is to answe r the question: what are the generation\nmechanisms of humans’ intelligence and how to implement the se mechanisms with a machine. Therefore,\ngiven a research work, it is easy to judge whether it belongs t o the “strong AI” paradigm. If this work\nprovides any new and useful clue for us to answer the above que stion, it falls within the “strong AI”\nparadigm; otherwise, it belongs to the “weak AI” paradigm.\nBased on the above method, part of the (especially early) wor ks on neural networks that deepen our\nunderstanding of the working mechanism of biological neura l systems, surely belongs to the “strong AI”\nparadigm. On the other hand, most research works that involv e artiﬁcial neural networks and deep learning,\neven if they are "
  },
  {
    "arxiv_id": "2103.15294",
    "title": "\"Weak AI\" is Likely to Never Become \"Strong AI\", So What is its Greatest\n  Value for us?",
    "chunk": "ural networks and deep learning,\neven if they are proposed under the inspiration of research o n neuroscience, cognitive science, behavior\npsychology, they belong to the “weak AI” paradigm as long as t hey do not give us any new insight on\nthe generation mechanisms of humans’ intelligence or on how to better implement mechanisms that have\nalready been found.\nIII. W HYCONTROVERSIES ABOUT AI L AST?\nIn controversies about AI, party A believes that AI has made s ubstantial progress in the past decade;\nparty B doubts or even negates the development of AI.\nI argue that controversies arise mainly because these two pa rties mix two different concepts, “weak\nAI” and “strong AI”, together, when they talk about AI. The fa ct is that “weak AI” has made substantial\nprogress in the past decade, while “s"
  },
  {
    "arxiv_id": "2103.15294",
    "title": "\"Weak AI\" is Likely to Never Become \"Strong AI\", So What is its Greatest\n  Value for us?",
    "chunk": " substantial\nprogress in the past decade, while “strong AI” has not. Party A thinks that “weak AI” is an important\nmember of the AI family; progress gained from “weak AI” also b elongs to this AI family. In contrast,\nin the mind of Party B, there always exists one ideal form of AI , namely a realized “strong AI”, and\nthe “distance” between current AI and this ideal AI is treate d as a criterion for evaluating current AI.\nCompared with decades ago, current AI still lacks basic huma n-level abilities such as causal reasoning\n[9], robust decision making [19], commonsense utilization [20], and knowledge transfer, which implies\n4\nthat the “distance” between the realized AI and the ideal “st rong AI” has not been remarkably shortened.\nTherefore, it is reasonable for party B to doubt or even nega"
  },
  {
    "arxiv_id": "2103.15294",
    "title": "\"Weak AI\" is Likely to Never Become \"Strong AI\", So What is its Greatest\n  Value for us?",
    "chunk": "it is reasonable for party B to doubt or even negat e the development of AI.\nA natural question arises: how breakthroughs of “weak AI” ha ve come out in the past decade? Judea\nPearl argues that “... all the impressive achievements of deep learning amount to j ust curve ﬁtting” .\nHowever, the point is that, different from previous ﬁtting m ethods, deep learning permits to do an\nextraordinary ﬁtting - ﬁtting multi-modal big data in an end -to-end way. This deep learning type of\nﬁtting requires a big consumption of both computing and stor age resources but avoids labor-intensive\nfeature engineering. Big data, big computing, and big stora ge are three requisites that make deep learning\nsurpass humans in playing Go, image classiﬁcation, speech r ecognition, and so on. The luck for deep\nlearning"
  },
  {
    "arxiv_id": "2103.15294",
    "title": "\"Weak AI\" is Likely to Never Become \"Strong AI\", So What is its Greatest\n  Value for us?",
    "chunk": " ecognition, and so on. The luck for deep\nlearning is that the past decade happens to witness great imp rovements in sensing technologies, wireless\nmobile phones, cloud computing, computing devices, comput er storage, and databases, which give birth\nto big data, big computing, and big storage required by deep l earning.\nIV. W ILL“WEAK AI” G ROW INTO “STRONG AI”?\nA metaphor is often used to reply to this question: the relati onship between “weak AI” and “strong AI”\nis like that between ﬂying machines and birds. Flying machin es are not developed by accurately mimicking\nbirds’ ﬂying. Birds perform much better in maneuvering than the most advanced ﬂying machine today.\nBirds can ﬂexibly re-purpose their behaviors while ﬂying ma chines cannot. But the appearance of ﬂying\nmachines has met demand"
  },
  {
    "arxiv_id": "2103.15294",
    "title": "\"Weak AI\" is Likely to Never Become \"Strong AI\", So What is its Greatest\n  Value for us?",
    "chunk": "ut the appearance of ﬂying\nmachines has met demands of speedy transportation and other s. People may think that, since it is unlikely\nand not necessary for ﬂying machines to develop into birds, t hen similarly, “weak AI” is unlikely and not\nnecessary to grow into “strong AI”.\nTo formally consider whether “weak AI” will grow into “stron g AI”, let recall the Turing test [21] and the\nCRA (mentioned in Section II). An example statement of the Tu ring test is as follows [22]: “ Originally\nknown as the Imitation Game, the test evaluates if a machine’ s behavior can be distinguished from a\nhuman. In this test, there is a person known as the “interroga tor” who seeks to identify a difference between\ncomputer-generated output and human-generated ones throu gh a series of questions. If the interrog"
  },
  {
    "arxiv_id": "2103.15294",
    "title": "\"Weak AI\" is Likely to Never Become \"Strong AI\", So What is its Greatest\n  Value for us?",
    "chunk": "es throu gh a series of questions. If the interrogator\ncannot reliably discern the machines from human subjects, t he machine passes the test. However, if\nthe evaluator can identify the human responses correctly, t hen this eliminates the machine from being\ncategorized as intelligent. ” Through the lens of CRA, Searle argues that the Turing test h as serious ﬂaws,\nas passing the test does not indicate that the machine has con sciousness or understanding. The absence\nof an effective evaluation method hampers the development o f “strong AI”.\n5\nBesides, philosophers and cognitive scientists often use G ¨odel’s ﬁrst incompleteness theorem [23] to\nargue that a machine cannot generate humans’ consciousness or understanding. See related discussions in\ne.g., [12].\nV. W HAT IS THE GREATEST VALUE OF"
  },
  {
    "arxiv_id": "2103.15294",
    "title": "\"Weak AI\" is Likely to Never Become \"Strong AI\", So What is its Greatest\n  Value for us?",
    "chunk": "s in\ne.g., [12].\nV. W HAT IS THE GREATEST VALUE OF “WEAK AI” FOR US ?\nIn his most recent paper, Geoffrey Hinton states that “ The difference between science and philosophy is\nthat experiments can show that extremely plausible ideas ar e just wrong and extremely implausible ones,\nlike learning an entire complicated system by end-to-end gr adient decent, are just right ” [24]. In [25], Judea\nPearl argues that “ Modern connectionism has in fact been viewed as a Triumph of R adical Empiricism\nover its rationalistic rivals. Indeed, the ability to emula te knowledge acquisition processes on digital\nmachines offer enormously ﬂexible testing grounds in which philosophical theories about the balance\nbetween empiricism and innateness can be submitted to exper imental evaluation on digital machines. "
  },
  {
    "arxiv_id": "2103.15294",
    "title": "\"Weak AI\" is Likely to Never Become \"Strong AI\", So What is its Greatest\n  Value for us?",
    "chunk": " to exper imental evaluation on digital machines. ”\nCombining their arguments, one can see that they both attrib ute recent deep learning’s success as a success\nof empiricism which is data-driven, other than driven by phi losophical theory or intuition.\nA very important lesson that can be learned from the fast-pac ing development and applications of AI\nin the past decade is that deep learning running on big enough data can produce unexpected shortcuts\nto solve extremely difﬁcult problems. For example, by combi ning deep learning, reinforcement learning\n[26], and Monte Carlo tree search [27], a computer program Al phaGO [28] can win the human champion\nwithout having to understand any of the Go-playing strategi es that have been accumulated by humans\nfor more than four thousand years. The Ge"
  },
  {
    "arxiv_id": "2103.15294",
    "title": "\"Weak AI\" is Likely to Never Become \"Strong AI\", So What is its Greatest\n  Value for us?",
    "chunk": "y humans\nfor more than four thousand years. The Generative Pre-train ed Transformer 3 (GPT-3) [29] can generate\nhuman-like texts through deep learning without having to un derstand any syntax or semantics underlying\nthe texts. It is shown that the greatest value of “weak AI” rep resented by deep learning lies in that it\nprovides scalable, less-labor-involved, accurate, and ge neralizable tools for distilling, representing and\nthen exploiting patterns hidden from big data. Although suc h “weak AI” has no real intelligence, to a\nlarge extent it meets urgent needs for scalable, efﬁcient, a nd accurate processing of big data.\nIn a foreseeable future, “weak AI” is likely to become more ro bustly (with e.g., portfolio [19] or\ndynamic portfolio methods [30–35]), while a big challenge i s how to m"
  },
  {
    "arxiv_id": "2103.15294",
    "title": "\"Weak AI\" is Likely to Never Become \"Strong AI\", So What is its Greatest\n  Value for us?",
    "chunk": "thods [30–35]), while a big challenge i s how to model “unknown unknowns”; it will\nperform more automatically through e.g., auto machine lear ning [36], but it can not become completely\nautomatic provided that “strong AI” is realized [37]; it may perform as if it owns abilities of cognition\nand understanding, but it does not.\n6\nVI. C ONCLUSIONS\nAI has made great progress in the past decade. It has inﬂuence d almost all facets of human society\nby providing more efﬁcient algorithmic solutions to repres entation, management, analysis of multi-modal\nbig data. Controversies about AI last mainly because “weak A I” becomes so strong while “strong AI” is\nalmost as weak as it was decades ago. Almost all breakthrough s of AI that have attracted the public’s\nattention in the past decade are within th"
  },
  {
    "arxiv_id": "2103.15294",
    "title": "\"Weak AI\" is Likely to Never Become \"Strong AI\", So What is its Greatest\n  Value for us?",
    "chunk": "ublic’s\nattention in the past decade are within the “weak AI” paradig m. “Weak AI” is developing much faster\nthan expected. Even ten years ago, one could not imagine that a computer program would beat the human\nchampion soon in playing Go. In contrast, the “fruits” peopl e have got from the “strong AI” paradigm are\nnot so striking as from “weak AI”. I suggest, when talking abo ut AI in the future, one should better make\na statement in advance whether this talk is about “weak AI” or “strong AI”. In this way, more focused\nand constructive discussions can be expected.\nIn a foreseeable future, “weak AI” cannot develop into “stro ng AI” (see why in Section IV), but it\nprovides a channel to synthesize advances obtained from rel ated disciplines such as cloud computing,\ncomputer storage, high-spe"
  },
  {
    "arxiv_id": "2103.15294",
    "title": "\"Weak AI\" is Likely to Never Become \"Strong AI\", So What is its Greatest\n  Value for us?",
    "chunk": "uch as cloud computing,\ncomputer storage, high-speed wireless mobile communicati ons. Through this synthesis of technologies,\nmore advanced algorithmic tools will be developed in the “we ak AI” paradigm, then “weak AI” will\ncontinue to inﬂuence human society more profoundly, throug h big data. The man-computer symbiosis\nworld that Licklider predicted more than sixty years ago [38 ] is becoming a reality.\nREFERENCES\n[1] Y . LeCun, Y . Bengio, and G. Hinton, “Deep learning,” Nature , vol. 521, no. 7553, pp. 436–444, 2015.\n[2] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation with deep convolutional neural networks,” Advances in neural\ninformation processing systems , vol. 25, pp. 1097–1105, 2012.\n[3] W. Xiong, L. Wu, F. Alleva, J. Droppo, X. Huang, and A. Stol cke, “The "
  },
  {
    "arxiv_id": "2103.15294",
    "title": "\"Weak AI\" is Likely to Never Become \"Strong AI\", So What is its Greatest\n  Value for us?",
    "chunk": "lleva, J. Droppo, X. Huang, and A. Stol cke, “The microsoft 2017 conversational speech recognitio n system,” in\nIEEE int’l conf. on acoustics, speech and signal processing (ICASSP) . IEEE, 2018, pp. 5934–5938.\n[4] G. Saon, G. Kurata, T. Sercu et al. , “English conversational telephone speech recognition by humans and machines,” arXiv preprint\narXiv:1703.02136 , 2017.\n[5] D. Silver, T. Hubert, J. Schrittwieser et al. , “A general reinforcement learning algorithm that masters chess, shogi, and go through\nself-play,” Science , vol. 362, no. 6419, pp. 1140–1144, 2018.\n[6] N. Brown and T. Sandholm, “Superhuman ai for heads-up no- limit poker: Libratus beats top professionals,” Science , vol. 359, no.\n6374, pp. 418–424, 2018.\n[7] O. Vinyals, I. Babuschkin, W. M. Czarnecki et al. , “Grandmaster l"
  },
  {
    "arxiv_id": "2103.15294",
    "title": "\"Weak AI\" is Likely to Never Become \"Strong AI\", So What is its Greatest\n  Value for us?",
    "chunk": "abuschkin, W. M. Czarnecki et al. , “Grandmaster level in starcraft ii using multi-agent rein forcement learning,” Nature ,\nvol. 575, no. 7782, pp. 350–354, 2019.\n[8] A. W. Senior, R. Evans, J. Jumper et al. , “Improved protein structure prediction using potentials from deep learning,” Nature , vol. 577,\nno. 7792, pp. 706–710, 2020.\n7\n[9] J. Pearl, “The limitations of opaque learning machines, ”Possible minds: twenty-ﬁve ways of looking at AI , pp. 13–19, 2019.\n[10] J. Pearl and D. Mackenzie, “Ai can’t reason why,” Wall Street Journal , 2018.\n[11] G. Marcus, “Deep learning: A critical appraisal,” arXiv preprint arXiv:1801.00631 , 2018.\n[12] J. M. Bishop, “Artiﬁcial intelligence is stupid and cau sal reasoning will not ﬁx it,” Frontiers in Psychology , vol. 11, pp. 1–18, 2021.\n[13] S. John,"
  },
  {
    "arxiv_id": "2103.15294",
    "title": "\"Weak AI\" is Likely to Never Become \"Strong AI\", So What is its Greatest\n  Value for us?",
    "chunk": "sychology , vol. 11, pp. 1–18, 2021.\n[13] S. John, “Minds, brains, and programs,” Behavioral and Brain Sciences , vol. 3, no. 3, pp. 417–457, 1980.\n[14] D. Cole, “The chinese room argument,” https://plato.s tanford.edu/entries/chinese-room/.\n[15] G. Rey, “What’s really going on in Searle’s “Chinese roo m”,” Philosophical Studies , vol. 50, no. 2, pp. 169–85, 1986.\n[16] M. J. Shaffer, “A logical hole in the chinese room,” Minds and Machines , vol. 19, no. 2, pp. 229–235, 2009.\n[17] A. Sloman and M. Croucher, “How to turn an information pr ocessor into an understander,” Behavioral and Brain Sciences , vol. 3,\nno. 3, pp. 447–448, 1980.\n[18] M. A. Boden, Computer models of mind: Computational approaches in theor etical psychology . Cambridge University Press, 1988.\n[19] T. G. Dietterich, “Step"
  },
  {
    "arxiv_id": "2103.15294",
    "title": "\"Weak AI\" is Likely to Never Become \"Strong AI\", So What is its Greatest\n  Value for us?",
    "chunk": "iversity Press, 1988.\n[19] T. G. Dietterich, “Steps toward robust artiﬁcial intel ligence,” AI Magazine , vol. 38, no. 3, pp. 3–24, 2017.\n[20] G. Marcus, “The next decade in AI: four steps towards rob ust artiﬁcial intelligence,” arXiv preprint arXiv:2002.06177 , 2020.\n[21] A. M. Turing, “Computing machinery and intelligence,” inParsing the turing test . Springer, 2009, pp. 23–65.\n[22] IBM Cloud Education, “Strong AI,” https://www.ibm.co m/cloud/learn/strong-ai.\n[23] P. Raatikainen, “G ¨odel’s incompleteness theorems,” https://plato.stanfor d.edu/entries/goedel-incompleteness/.\n[24] G. Hinton, “How to represent part-whole hierarchies in a neural network,” arXiv preprint arXiv:2102.12627 , 2021.\n[25] J. Pearl, “Radical empiricism and machine learning res earch,” Causal Analysis in Theory an"
  },
  {
    "arxiv_id": "2103.15294",
    "title": "\"Weak AI\" is Likely to Never Become \"Strong AI\", So What is its Greatest\n  Value for us?",
    "chunk": " learning res earch,” Causal Analysis in Theory and Practice (Blog) , vol. 26, 2020.\n[26] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction . MIT press, 2018.\n[27] S. Gelly and D. Silver, “Monte-carlo tree search and rap id action value estimation in computer go,” Artiﬁcial Intelligence , vol. 175,\nno. 11, pp. 1856–1875, 2011.\n[28] D. Silver, A. Huang, C. J. Maddison et al. , “Mastering the game of go with deep neural networks and tree search,” Nature , vol. 529,\nno. 7587, pp. 484–489, 2016.\n[29] T. B. Brown, B. Mann, N. Ryder et al. , “Language models are few-shot learners,” arXiv preprint arXiv:2005.14165 , 2020.\n[30] B. Liu, Y . Qi, and K. Chen, “Sequential online predictio n in the presence of outliers and change points: an instant te mporal structure\nlearning appro"
  },
  {
    "arxiv_id": "2103.15294",
    "title": "\"Weak AI\" is Likely to Never Become \"Strong AI\", So What is its Greatest\n  Value for us?",
    "chunk": "nts: an instant te mporal structure\nlearning approach,” Neurocomputing , vol. 413, pp. 240–258, 2020.\n[31] Y . Qi, B. Liu, Y . Wang, and G. Pan, “Dynamic ensemble mode ling approach to nonstationary neural decoding in brain-co mputer\ninterfaces,” Advances in neural information processing systems , pp. 6087–6096, 2019.\n[32] B. Liu, “Robust particle ﬁlter by dynamic averaging of m ultiple noise models,” in IEEE Int’l Conf. on Acoustics, Speech and Signal\nProcessing (ICASSP) . IEEE, 2017, pp. 4034–4038.\n[33] Y . Dai and B. Liu, “Robust video object tracking via baye sian model averaging-based feature fusion,” Optical Engineering , vol. 55,\nno. 8, pp. 1–11, 2016.\n[34] B. Liu, “Data-driven model set design for model average d particle ﬁlter,” in IEEE Int’l Conf. on Acoustics, Speech and Signal "
  },
  {
    "arxiv_id": "2103.15294",
    "title": "\"Weak AI\" is Likely to Never Become \"Strong AI\", So What is its Greatest\n  Value for us?",
    "chunk": " IEEE Int’l Conf. on Acoustics, Speech and Signal Processing\n(ICASSP) . IEEE, 2020, pp. 5835–5839.\n[35] ——, “Instantaneous frequency tracking under model unc ertainty via dynamic model averaging and particle ﬁltering ,”IEEE Trans. on\nWireless Communications , vol. 10, no. 6, pp. 1810–1819, 2011.\n[36] F. Hutter, L. Kotthoff, and J. Vanschoren, Automated machine learning: methods, systems, challenges . Springer Nature, 2019.\n[37] B. Liu, “A very brief and critical discussion on automl, ”arXiv preprint arXiv:1811.03822 , 2018.\n[38] J. Licklider, “Man-computer symbiosis,” IRE Transactions on human factors in electronics , no. 1, pp. 4–11, 1960.\n"
  },
  {
    "arxiv_id": "2403.05551",
    "title": "A Bibliometric View of AI Ethics Development",
    "chunk": " A bibliometric view of AI Ethics development   Di Kevin Gao Management Department -  California State University, East Bay  Hayward, CA, USA kevin.gao@csueastbay.edu    Andrew Haverly Computer Science and Engineering Dept Mississippi State University Mississippi State, MS 39762 arh876@msstate.edu   Dr. Sudip Mittal  Computer Science and Engineering Dept Mississippi State University Mississippi State, MS 39762 mittal@cse.msstate.edu   Dr. Jingdao Chen Computer Science and Engineering Dept Mississippi State University Mississippi State, MS 39762 chenjingdao@cse.msstate.edu   Abstract— Artificial Intelligence (AI) Ethics is a nascent yet critical research field. Recent developments in generative AI and foundational models necessitate a renewed look at the problem of AI Ethics. In this study,"
  },
  {
    "arxiv_id": "2403.05551",
    "title": "A Bibliometric View of AI Ethics Development",
    "chunk": "d look at the problem of AI Ethics. In this study, we perform a bibliometric analysis of AI Ethics literature for the last 20 years based on keyword search. Our study reveals a three-phase development in AI Ethics, namely an incubation phase, making AI human-like machines phase, and making AI human-centric machines phase. We conjecture that the next phase of AI ethics is likely to focus on making AI more machine-like as AI matches or surpasses humans intellectually, a term we coin as “machine-like human”.  Keywords— artificial intelligence ethics, AI ethics, machine ethics, algorithm ethics, Roboethics, human-like machine, machine-like human  I. INTRODUCTION Artificial Intelligence (AI) Ethics is the study of the ethical and responsible development and deployment of AI technology. Our bibl"
  },
  {
    "arxiv_id": "2403.05551",
    "title": "A Bibliometric View of AI Ethics Development",
    "chunk": "elopment and deployment of AI technology. Our bibliometric analysis of AI Ethics literature published between 2004 and 2023 points us to a three-phase development: 1. Incubation; 2. Making AI human-like machines; 3. Making AI human-centric machines.  This article contributes to AI Ethics discussions with unique insights based on keyword usage patterns. It also contrasts “human-like machine”, “human-centric machine”, and “machine-like human”, which represent the past, current, and potential future phases of AI Ethics development. II. DEFINITIONS AND HISTORICAL DEVELOPMENT  AI was first coined in 1955 at the Dartmouth Workshop [1]. John McCarthy, one of the key contributors at the conference and an AI pioneer, defined AI as \"the science and engineering of making intelligent machines, especia"
  },
  {
    "arxiv_id": "2403.05551",
    "title": "A Bibliometric View of AI Ethics Development",
    "chunk": "ngineering of making intelligent machines, especially intelligent computer programs. It is related to the similar task of using computers to understand human intelligence, but AI does not have to confine itself to methods that are biologically observable\" [2].  AI has gone through multiple cycles of boom and bust. It was a rising star from its inception to 1973. Scientists were excited by its potential to solve algebra word problems, prove geometry theorems, and even learn to speak. However, it failed to deliver on the hyped expectations. That led to the Lighthill Report in 1974, which triggered a massive loss of confidence in AI [3]. In the United States, the Defense Advanced Research Projects Agency (DARPA) also drastically reduced its AI funding. AI sank into an “AI Winter\" until 1980. "
  },
  {
    "arxiv_id": "2403.05551",
    "title": "A Bibliometric View of AI Ethics Development",
    "chunk": " funding. AI sank into an “AI Winter\" until 1980. The relief in the 80s turned out to be short-lived. In 1987, AI was again placed in a freezer. By the early 2000s, AI was haunted by over-promises and under-delivery. In 2005, John Markoff in the New York Times described that some computer scientists avoided the term artificial intelligence altogether “for fear of being viewed as wild-eyed dreamers\" [4]. In 2007, Alex Castro referred to artificial intelligence as a subject that has “too often failed to live up to their promises\" [5]. That has resulted in “once something becomes useful enough and common enough it’s not labeled AI anymore\" [6]. In the 1990s and 2000s, new computer science disciplines flourished. However, they were deliberately not categorized under Artificial Intelligence, fo"
  },
  {
    "arxiv_id": "2403.05551",
    "title": "A Bibliometric View of AI Ethics Development",
    "chunk": " not categorized under Artificial Intelligence, for example, informatics, machine learning, machine perception, analytics, predictive analytics, decision support systems, knowledge-based systems, business rules management, cognitive systems, intelligent systems, language models, intelligent agents, or computational intelligence. AI regained its popularity with Google Translate, Google Image Search, and IBM Watson’s winning the Jeopardy game in 2011 [7]. 2012 marked a turning point for AI due to breakthroughs in deep learning and GPU technology. AlexNet used GPU to train its Convolutional Neural Network (CNN) model to recognize and label images automatically and won the ImageNet 2012 Challenge by a large margin[8] [9]. AI’s resurgence became insurmountable. AI broadened its scope to absorb "
  },
  {
    "arxiv_id": "2403.05551",
    "title": "A Bibliometric View of AI Ethics Development",
    "chunk": " insurmountable. AI broadened its scope to absorb many downstream research fields. It became an aggregator and a destination.  In November 2022, Open AI released ChatGPT which attracted intense interest from the general public. It triggered an all-out war between the Big Techs and stiffened competition between rival countries. Ethical AI development and deployment have become more important than ever.   III. METHODS  \nWe selected SCOPUS1 as the main data source and VOSviewer2 as the data aggregator. In the SCOPUS database, we searched for “AI ethics\" OR “artificial intelligence ethics\" OR “machine ethics\" OR “algorithm ethics\" OR “information ethics\" OR \"ethics of technology\" OR “Robotic Ethics\" OR “Robot Ethics\" OR “artificial moral agent\" OR “artificial moral agents\" from 2004 to 2023, a"
  },
  {
    "arxiv_id": "2403.05551",
    "title": "A Bibliometric View of AI Ethics Development",
    "chunk": " OR “artificial moral agents\" from 2004 to 2023, a period of 20 years, for all languages, all countries, and territories. In total, 2,517 articles were selected. After removing 60 entries due to missing info, a total of 2,457 pieces of literature were included in this analysis. For keyword analysis, we used 2004-2023 co-occurrence author keywords from VOSviewer. We used “Full counting\", which means each keyword is counted as one regardless of how many keywords were listed in the literature. We then exported the results for time series and pattern analysis.  IV. AI ETHICS DEVELOPMENT  A. AI ethics and related ethics usage analysis We calculated the keyword usage frequencies for AI Ethics and other related ethical fields based on SCOPUS data. Other related ethical fields included information"
  },
  {
    "arxiv_id": "2403.05551",
    "title": "A Bibliometric View of AI Ethics Development",
    "chunk": " Other related ethical fields included information ethics, machine ethics, roboethics, technology ethics, computer ethics, data ethics, engineering ethics, digital ethics, and computational ethics. The data is summarized in Figure 1.  \"AI Ethics\" or \"Artificial Intelligence ethics\" first appeared in keywords in 2008, followed by five 5 years of hibernation. In 2014, AI Ethics reemerged and has since enjoyed exponential growth. In 2014, there was only one occurrence of the keyword “AI Ethics” in our literature research. However, by 2022, the keyword frequency had increased to 148. In the 2023 partial year till July 28, the usage has also reached 114. The keyword “AI Ethics” completely outnumbered the rest of the terms such as “roboethics”, “data ethics”, or “machine ethics”.  This finding i"
  },
  {
    "arxiv_id": "2403.05551",
    "title": "A Bibliometric View of AI Ethics Development",
    "chunk": "data ethics”, or “machine ethics”.  This finding is important because in the historical development section of AI, we know that 2012 marked the turning point for AI as a research field. We believe 2014 is the year that AI Ethics was formed. Before that, AI Ethics was dispersed across information ethics, machine ethics, roboethics, technology ethics, and computer ethics. Thus, we define the pre-2014 period as the incubation period.    B. Ethics principles usage pattern analysis We leveraged VOSviewer to unpack keyword usages that are related to AI Ethics. Figure 2 is generated from the VOSviewer based on data since 2004 by using co-occurrence data and author keywords. We used full counting instead of partial counting, which means each author keyword is counted as one regardless of how many "
  },
  {
    "arxiv_id": "2403.05551",
    "title": "A Bibliometric View of AI Ethics Development",
    "chunk": " keyword is counted as one regardless of how many keywords were used in the literature. The size of the circle indicates the keywords’ relative frequency. Color represents the closeness of the topics. We sorted this information chronologically and further bifurcated the keywords based on product orientation and their associations with AI Ethics principles. The information is presented in Figure 3. The top rectangles are product-oriented  1 www.scopus.com features, the middle ovals illustrate when keywords started to be consistently used. The results revealed a significant shift in AI Ethics research principles from 2020. Between 2014 and 2019, AI Ethics keywords focused on principles to make AI ethical humans, e.g., trust, empathy, justice, care, and fairness. From 2020, however, the AI Et"
  },
  {
    "arxiv_id": "2403.05551",
    "title": "A Bibliometric View of AI Ethics Development",
    "chunk": " care, and fairness. From 2020, however, the AI Ethics keywords were increasingly focused on protecting the downside risks and making AI explainable, accountable, trustworthy, non-biased, non-discriminatory, less opaque, and for-diversity. Based on this observation, we grouped AI Ethics development into the following three phases:  • Phase I: Incubation (2004 to 2013) • Phase II: Making AI human-like machines (2014 to 2019)  • Phase III: Making AI human-centric machines (2020 and on)  C. AI Ethics Development Phases In the following section, we will go through each phase and highlight the major developments. 1) Phase I: Incubation (2004-2013)  AI Ethics trailed AI’s rapid development. In 1997, IBM’s Deep Blue defeated the world chess champion Garry Kasparov [10]. In 2005, the Stanford auto"
  },
  {
    "arxiv_id": "2403.05551",
    "title": "A Bibliometric View of AI Ethics Development",
    "chunk": "on Garry Kasparov [10]. In 2005, the Stanford autonomous vehicle, Stanley, successfully crossed 212 kilometers of terrain in the Mojave Desert [11]. In 2011, IBM’s Watson won the Jeopardy! that conventional wisdom believed only humans could master [12]. AI technology’s fast development galvanized many exciting research fronts and, in a way, “pushed” AI Ethics to the forefront. It became apparent that other ethics would not be sufficient to cover the wide spectrum of fields that AI covers. AI Ethics as a research field was born.  The popular keyword in 2004 was “privacy\". Privacy was not a new discipline; neither was it exclusive to AI. It became a hot topic with the explosive growth of data collection and data usage in the Internet age. In the next few years, “autonomy\", “reliability\", “sa"
  },
  {
    "arxiv_id": "2403.05551",
    "title": "A Bibliometric View of AI Ethics Development",
    "chunk": "the next few years, “autonomy\", “reliability\", “safety\", “security\", and “sustainability\" surfaced. The majority of the keywords were product-oriented. 2) Phase II: Make AI Human-like Machines (2014-2019)  In Phase II, AI has increasingly demonstrated its potential to function like a human. AI Ethicists and the general public welcomed the development. AI Ethics’ focus was on the ethical application of AI, the mini-human, in different fields. For this reason, we labeled this phase “Make AI Human-like Machines\".  During this phase, AI continued its breakneck advancement and pushed deep into new frontiers. In 2014, generative adversarial networks (GAN) were developed to synthesize new and creative images from existing ones. In 2015, AI enabled machines to \"see\" and label images better than hu"
  },
  {
    "arxiv_id": "2403.05551",
    "title": "A Bibliometric View of AI Ethics Development",
    "chunk": " machines to \"see\" and label images better than humans [13]. In 2016, Deep Mind’s AlphaGo defeated    2 Vosviewer.com \n  \n Fig 1: Usage of different ethics in literature key words. AI Ethics first appeared in 2008. It consistently appeared after 2014 and have taken off since 2020.    \n    Fig 2: AI Ethics and related fields based on bibliographical data in VOSviewer by using Scopus data from 2004 to July 28, 2023. 212219165799148114020406080100120140160\n20042005200620072008200920102011201220132014201520162017201820192020202120222023Usage of different ethics in literature keywords (Since 2004. 2023 partial year till 7/28)\nai ethicsinformation ethicsmachine ethicsroboethicstechnology ethicscomputer ethicsdata ethicsengineering ethicsdigital ethicscomputational ethics\n\n  \n Fig 3: AI Ethics De"
  },
  {
    "arxiv_id": "2403.05551",
    "title": "A Bibliometric View of AI Ethics Development",
    "chunk": "thicscomputational ethics\n\n  \n Fig 3: AI Ethics Development Phases Based on Keyword Analysis  world Go champion Lee Sedol [14]. In 2018, AI beat human dermatologists in accurately detecting skin cancer [15]. In 2018, Google Waymo’s Robotaxi started roaming in Phoenix’s streets [16]. The general public viewed the development positively and was excited by AI’s boundless applications.  However, AI Ethics development lagged behind AI technology development. As mentioned in the historical background section, AI regained popularity and broadened its scope to include any computer science disciplines that enabled human-like intelligence in 2012. Machine learning, machine perception, text analysis, natural language processing (NLP), logical reasoning, game-playing, decision support systems, data an"
  },
  {
    "arxiv_id": "2403.05551",
    "title": "A Bibliometric View of AI Ethics Development",
    "chunk": "g, game-playing, decision support systems, data analytics, and predictive analytics became AI’s upstream or supporting disciplines. Robotics (including autonomous vehicles) was a fast-developing field that was enabled by AI. AI became an aggregator.  AI Ethics keywords during this phase reflected many human-like features, for example, “accountability\", “care\", “compassion\", “empathy\", “fairness\", “justice\", “transparency\", “trust\", and Explainable AI (XAI). The AI Ethics community wanted to make this intelligent technology accountable, caring, compassionate, empathetic, fair, unbiased, transparent, and trustworthy, just like an ethical human.  3) Phase III: Make AI Human-centric Machines (2020- present)  In Phase III, while continuing its rapid ascension, AI had shown aspects that were far"
  },
  {
    "arxiv_id": "2403.05551",
    "title": "A Bibliometric View of AI Ethics Development",
    "chunk": "apid ascension, AI had shown aspects that were far from angelic. The AI Ethics community focused on grounding AI into an explainable, responsible, and trustworthy machine that serves humans instead of being a runaway alien technology. That was the reason we titled this phase \"Make AI Human-centric Machines\".  By 2020, AI had surpassed humans in handwriting recognition, speech recognition, image recognition, read comprehension, and language understanding [17]. In the meantime, Deep Fakes exacerbated online misinformation and undermined basic human trust. In May 2021, the United States National Security Commission urged the US to win the AI arms race against China, reminiscing the costly and dangerous Cold War [18]. In July 2022, Google fired an engineer who claimed that its LaMDA language m"
  },
  {
    "arxiv_id": "2403.05551",
    "title": "A Bibliometric View of AI Ethics Development",
    "chunk": " an engineer who claimed that its LaMDA language model was sentient, exacerbating the general public’s suspicion of AI [19]. In November 2022, Open AI released Chat GPT 3.0 to the public and triggered an all-out AI race. It looked increasingly like the AI companies were racing to the bottom to win but put safety and security on the backburner [20]. In 2023, ChatGPT became the second Large Language Model to pass the Turing Test [21] [22]. In 2023, Goldman Sachs estimated that 300 million jobs could be displacement by AI [23]. Scientists, entrepreneurs, and public officers started to alarm the general public about the consequences of unconstrained AI development. Public distrust of AI surged.  During this phase, precautionary keywords showed up very frequently in the literature, such as “alg"
  },
  {
    "arxiv_id": "2403.05551",
    "title": "A Bibliometric View of AI Ethics Development",
    "chunk": "up very frequently in the literature, such as “algorithm bias\", “AI bias\", “gender bias\", and “discrimination\" in 2020, and “opacity\", “responsibility gap\", and “social justice\" in 2021. Meanwhile, keywords such as “explainability\" and “trustworthy AI’ popped up in 2020. “Human-centric\", “responsible AI\" showed up in 2021, and “interpretability\" and “sustainable AI\" showed up in 2022. The AI ethics community wanted to make AI responsible, explainable, and trustworthy to humans. AI Ethics entered a phase to make AI \"human-centric\". \n\nD. The future of AI Ethics   AI technology is disruptive in nature. AI Ethics is pivotal in the benign and benevolent rollout of AI technology. With the current development pace, it is almost inevitable that AI and robotics will match or surpass humans both phy"
  },
  {
    "arxiv_id": "2403.05551",
    "title": "A Bibliometric View of AI Ethics Development",
    "chunk": "and robotics will match or surpass humans both physically and intellectually. AI can become near-human. AI ethicists may need to explore how to make these intelligent AI embodiments “machine-like humans”: machines that are intelligent and capable, but never achieve the same status as full humans. Another challenge is the confluence of AI, robotics, and biotechnology. Machine-augmented humans and machine-augmented non-human (animals or newly created species) could blur the definition of humans. AI ethicists may need to study the ethical ramifications of this development and draw redlines on socially acceptable creations of alien beings.  Superintelligence, a form of AI that is superior to human intelligence, can be a concern. Even if the risks are extremely low, if indeed it happens, the co"
  },
  {
    "arxiv_id": "2403.05551",
    "title": "A Bibliometric View of AI Ethics Development",
    "chunk": "ks are extremely low, if indeed it happens, the consequences could be incredibly serious. AI Ethicists may need to develop a framework to prevent Superintelligence from remotely happening. V. LIMITATION Our bibliometric analysis has a few limitations:  1. We rely heavily on SCOPUS for data sources. Literature unlisted on SCOPUS would be excluded. 2. We relied heavily on VOSviewer to generate keyword co-occurrence data. Errors in VOSviewer could be carried into our final analysis.  3. Literature search was based on literature titles and keywords. That could result in the addition of unwanted papers. Some legitimate AI ethics articles might be excluded.   4. The majority of literature included in this study was in English (97%). It is highly probable that some non-English literature was miss"
  },
  {
    "arxiv_id": "2403.05551",
    "title": "A Bibliometric View of AI Ethics Development",
    "chunk": "probable that some non-English literature was missed.  Despite these caveats, we believe that our bibliometric analysis remains highly valuable to the scientific and engineering community since most AI and AI Ethics research is published in English-language journals and conferences that are indexed by Scopus. VI. CONCLUSION The bibliometric analysis of AI Ethics literature has pointed to a 3-phase AI Ethics development, namely incubation, making AI human-like machines, and making AI human-centric machines. AI ethicists may need to get ahead of the AI technology development and research on making AI machine-like humans, prohibit unethical development of machine-augmented non-humans, and prevent the development of malicious or malevolent Superintelligence.  VII. ACKNOWLEDGEMENT The work repo"
  },
  {
    "arxiv_id": "2403.05551",
    "title": "A Bibliometric View of AI Ethics Development",
    "chunk": "rintelligence.  VII. ACKNOWLEDGEMENT The work reported herein was supported by the National Science Foundation (NSF) (Award #2246920). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF.   [1] Dartmouth University, “Dartmouth workshop,” 1956. https://home.dartmouth.edu/about/artificial-intelligence-ai-coined-dartmouth [2] John McCarthy, “What is ai?,” 2023. http://jmc.stanford.edu/artificial-intelligence/what-is-ai/index.html [3] James Lighthill, “Lighthill report: Artificial intelligence: a paper symposium,” Science Research Council, London, 1973. [4] John Markoff, “Behind artificial intelligence, a squadron of bright real people,” 2005. New York Times. [5] Patty Tascarella, “Ar"
  },
  {
    "arxiv_id": "2403.05551",
    "title": "A Bibliometric View of AI Ethics Development",
    "chunk": ",” 2005. New York Times. [5] Patty Tascarella, “Are you talking to me?” 2005. https://www.economist.com/technology-quarterly/2007/06/09/are-you-talking-to-me [6] CNN, “Ai set to exceed human brain power,” 2006. CNN.com.  [7] Mike Hale, “Actors and their roles for $300, hal? hal!” New York Times, 2011 [8] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton, “Imagenet classification with deep convolutional neural networks,” Communications of the ACM, vol. 60, 2017. [9] Dave Gershgorn, “The inside story of how ai got good enough to dominate Silicon Valley,” Quartz, 2018. [10] Bruce Weber, “Swift and slashing, computer topples kasparov,” 1997. https://www.nytimes.com/1997/05/12/nyregion/swift-and-slashing-computer-topples-kasparov.html [11] Steve Russell, “Darpa grand challenge winner: Sta"
  },
  {
    "arxiv_id": "2403.05551",
    "title": "A Bibliometric View of AI Ethics Development",
    "chunk": " Steve Russell, “Darpa grand challenge winner: Stanley the robot!” 2006. https://www.popularmechanics.com/technology/robots/a393/2169012/ [12] John Markoff, “Computer wins on ‘Jeopardy!’: Trivial, it’s not!,” 2011. https://www.nytimes.com/2011/02/17/science/17jeopardy-watson.html [13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei, “Imagenet: A large-scale hierarchical image database,” in 2009 IEEE Conference on Computer Vision and Pattern Recognition, 2009, pp. 248–255. [14] Adrian Cho, “‘Huge leap forward’: Computer that mimics human brain beats professional at game of go,” Science, 2016. [15] Alexa Lardieri, “Ai beats doctors at cancer diagnoses,” 2018. https://www.usnews.com/news/health-care-news/articles/2018-05-28/artificial-intelligence-beats-dermatologists-at"
  },
  {
    "arxiv_id": "2403.05551",
    "title": "A Bibliometric View of AI Ethics Development",
    "chunk": "28/artificial-intelligence-beats-dermatologists-at-diagnosing-skin-cancer [16] Jon Fingas, “Waymo launches its first commercial self-driving car service,” 2019. https://en.wikipedia.org/wiki/Waymo#:~:text=In%20December%202018%2C%20Waymo%20launched,and%20request%20a%20pick%2Dup [17] Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel, Zeerak Waseem, Pontus Stenetorp, Robin Jia, Mohit Bansal, Christopher Potts, and Adina Williams, “Dynabench: Rethinking benchmarking in nlp,” 2021. [18] National Security Commission on Artificial Intelligence, “National security commission on Artificial Intelligence - final report,” 2021. https://www.nscai.gov/wp-content/u"
  },
  {
    "arxiv_id": "2403.05551",
    "title": "A Bibliometric View of AI Ethics Development",
    "chunk": " report,” 2021. https://www.nscai.gov/wp-content/uploads/2021/03/Full-Report-Digital-1.pdf [19] Miles Kruppa, “Google parts with engineer who claimed its ai system is sentient,” 2022. https://www.wsj.com/articles/google-parts-with -engineer-who-claimed-its-ai-system-is-sentient-11658538296?ns=prod/accounts-wsj [20] Samantha Lock, “What is ai chatbot phenomenon chat-gpt and could it replace humans?,” 2022, Accessed: 2022-12-05. https://www.theguardian.com/technology/2022/dec/05/what-is-ai-chatbot-phenomenon-chatgpt-and-could-it-replace-humans [21] Celeste Biever, “Chatgpt broke the Turing test - the race is on for new ways to assess ai,” 2023. Nature, vol. 619, 2023 \n[22] Will Oremus, “Google’s ai passed a famous test — and showed how the test is broken,” 2022. The Washington Post.  [23] Be"
  },
  {
    "arxiv_id": "2403.05551",
    "title": "A Bibliometric View of AI Ethics Development",
    "chunk": "st is broken,” 2022. The Washington Post.  [23] Beatrice Nolan, “AI systems like chatgpt could impact 300 million full- time jobs worldwide, with administrative and legal roles some of the most at risk, goldman sachs report says,” https://www.businessinsider.com/generative-ai-chatpgt-300-million-full-time-jobs-goldman-sachs-2023-3. \n"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\nSMEs\nMohammed Alnajjar\nSparkka Oy\nVantaa, Finland\nfirstname@sparkka.comKhalid Alnajjar\nRootroo Ltd\nHelsinki, Finland\nfirstname@rootroo.comMika Hämäläinen\nMetropolia University\nof Applied Sciences\nHelsinki, Finland\nfirst.lastname@metropolia.fi\nAbstract\nThis study examines AI adoption among\nFinnish healthcare SMEs through semi-\nstructured interviews with six health-tech com-\npanies. We identify three AI engagement\ncategories: AI-curious (exploring AI), AI-\nembracing (integrating AI), and AI-catering\n(providing AI solutions). Our proposed three-\nfold model highlights key adoption barriers,\nincluding regulatory complexities, technical ex-\npertise gaps, and financial constraints. While\nSMEs recognize AI’s potential, most rem"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "nts. While\nSMEs recognize AI’s potential, most remain\nin early adoption stages. We provide action-\nable recommendations to accelerate AI inte-\ngration, focusing on regulatory reforms, talent\ndevelopment, and inter-company collaboration,\noffering valuable insights for healthcare organi-\nzations, policymakers, and researchers.\n1 Introduction\nThe healthcare industry spans multiple sectors, in-\ncluding pharmaceuticals, diagnostics, medical pro-\ncedures, and wellbeing services. Artificial Intelli-\ngence (AI) has demonstrated significant potential\nin transforming healthcare by assisting in tasks\nsuch as medical imaging (Suzuki, 2017), speech\nprocessing (Partanen et al., 2020), and personalized\ntreatment plans (Vahedifard et al., 2023). Recent\nadvances in neural models have further enhanced\nAI’s "
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "ances in neural models have further enhanced\nAI’s ability to improve diagnostics and patient care\nwhile reducing the workload on healthcare profes-\nsionals (see Javaid et al. 2023).\nHealth-tech companies play a crucial role in AI-\ndriven innovation, continuously developing new\ntools and services to enhance medical outcomes\nand operational efficiency. Governments and pri-\nvate enterprises invest heavily in AI-driven medical\nresearch, yet adoption remains complex due to chal-\nlenges such as regulatory restrictions (e.g., GDPR),\ndata privacy concerns (c.f. Hämäläinen 2024), and\nthe risks of computational errors in diagnosis (see\nDave et al. 2023).One notable example of AI’s impact in health-\ncare is Google’s model (Nabulsi et al., 2021), which\ndemonstrated high sensitivity in detecting abnor-"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "\ndemonstrated high sensitivity in detecting abnor-\nmal chest conditions, including COVID-19, from\nX-ray scans. The model’s success, despite not be-\ning trained on COVID-specific data, highlights AI’s\npotential in identifying unseen diseases—an essen-\ntial feature for future medical advancements.\nWhile Finnish health-tech companies acknowl-\nedge AI’s transformative potential, their adoption\nremains limited due to data accessibility, compli-\nance barriers, and the need for extensive valida-\ntion. This study investigates how Finnish SMEs\nin healthcare integrate AI into their operations, the\nchallenges they face, and pathways to overcoming\nthese obstacles. We introduce a threefold model\ncategorizing AI adoption among SMEs and provide\nactionable recommendations to support AI integra-\ntion in he"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": " recommendations to support AI integra-\ntion in healthcare settings.\nAI in healthcare extends beyond clinical appli-\ncations into digital humanities, where natural lan-\nguage processing (NLP) plays a crucial role in an-\nalyzing medical texts, patient records, and health-\ncare policies. Understanding AI adoption in health-\ncare SMEs contributes to the broader discourse on\nhow AI, NLP, and computational tools shape inter-\ndisciplinary research and real-world applications.\nOur main contributions in this paper are:\n•Conduct interviews with small and medium-\nsized healthcare enterprises to assess AI adop-\ntion challenges and opportunities.\n•Perform qualitative analysis to evaluate AI\nmaturity levels among Finnish SMEs.\n•Introduce a threefold model of AI adoption in\nbusiness.\n•Provide strategic "
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "el of AI adoption in\nbusiness.\n•Provide strategic recommendations to en-\nhance AI utilization in healthcare.\nBy bridging AI research and practical healthcare\napplications, this work contributes to the ongoingarXiv:2503.14527v1  [cs.CY]  15 Mar 2025\ndialogue on AI’s role in healthcare, policy, and\ndigital humanities, offering insights for both re-\nsearchers and industry practitioners.\n2 Background\nIn recent years, artificial intelligence (AI) has\nsignificantly impacted various industries, and the\nhealthcare industry is no exception. Reddy et al.\n(2019) explored the incorporation of AI in health-\ncare delivery, identifying challenges and opportu-\nnities for large-scale use while addressing issues\nsuch as medical responsibilities and data access.\nThey utilized a qualitative method based on ob"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "ss.\nThey utilized a qualitative method based on obser-\nvations of existing AI technologies and predictions\nof future developments.\nIn another research, Garbuio and Lin (2019) an-\nalyzed the complexity of value-users in healthcare\nand emerging business models in AI-driven health-\ncare startups. By examining archetypes of business\nmodels used by entrepreneurs worldwide, they con-\nducted a quantitative analysis of 30 healthcare star-\ntups that deploy AI. They concluded that designing\neffective business models is crucial for bringing\nbeneficial technologies to the market.\nAI definitions and deployment status for\nmedium-sized companies were investigated by Ul-\nrich and Frank (2021), particularly German SMEs,\nand the opportunities of AI in supply chain op-\ntimization. They collected both quantit"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": " chain op-\ntimization. They collected both quantitative and\nqualitative data through an open and closed sur-\nvey questionnaire from 12,360 German companies’\nemails via the Nexis database. Their findings high-\nlighted the relevance of technologies for compa-\nnies, AI opportunities in SMEs, and barriers to AI\nadoption.\nThe research conducted by Bettoni et al. (2021)\nfocused on the challenges of applying AI in com-\npanies and AI maturity models. They conducted\nface-to-face interviews and reviewed state-of-the-\nart literature, examining two SMEs in Poland and\nItaly. Their research resulted in a conceptual frame-\nwork to support AI adoption in SMEs.\nBunte et al. (2021) studied the application of AI\nin manufacturing and its utilization in the industrial\nenvironment, particularly in measuring the"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "ustrial\nenvironment, particularly in measuring the finan-\ncial impact of AI. They employed a mixed-methods\napproach, using open-ended online questionnaires\nto collect data from 441 participants across 68 com-\npanies in Germany, Austria, and Switzerland. Their\nresearch suggested potential strategies to support\nAI usage in SMEs and identified two best practicesolutions.\n3 Methodology and Data\nThis section explains the research strategy and data\ncollection methods used. This work employs a\ncase study research strategy, focusing on health-\ntech companies in Finland as the unit of analysis.\nThe case study methodology allows us to explore\ncomplex phenomena and gain insights into the un-\nderlying dynamics and mechanisms that drive them\n(Yin, 2009). Through semi-structured interviews\n(Saunders et "
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": ". Through semi-structured interviews\n(Saunders et al., 2009), rich and detailed data from\nvarious stakeholders can be collected within the\nhealth-tech industry.\nTo apply the case study methodology in this re-\nsearch, first, a literature review was conducted to\nidentify relevant theories and concepts that would\ninform the research questions. Then, data was col-\nlected through semi-structured interview methods.\nThe data were analyzed using a qualitative data\nanalysis software, following a systematic approach.\nThe results of the analysis informed discussions\nand recommendations for further research and prac-\ntice.\nThe research methodology consists of qualita-\ntive approaches that include four parts: the first is\nresearching the existing healthcare and wellbeing\nSMEs in Finland to analyze thei"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "care and wellbeing\nSMEs in Finland to analyze their products and ser-\nvices offerings to build a general understanding of\nhealth-technology applications in the market. Then,\nin the second phase, the companies that have digital\nproducts or services have been contacted to request\ninterviews with them. The third part involves ana-\nlyzing the findings from the conducted interviews\nwith health-tech Finnish companies about their AI\nusage level with a focus on Small- and Medium-\nsized Enterprises (SMEs). Finally, the development\nsection provides insights and recommendations to\nsupport the use of AI in Finnish health-tech busi-\nnesses, based on the interviews conducted, and an\noverview of the future possible uses of AI in the\nsector.\n4 Results\nIn this section, the results of interview analysis\nrep"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "his section, the results of interview analysis\nreports will be presented comprehensively, in a\nscientific manner that allows understanding and\nanalysis of the results. As well, the most impor-\ntant phrases that were mentioned in the interviews\nabout the research topic. In addition, the common\nobservations, and trends related to the use, benefits,\nand challenges of embedding AI in the healthcare\nand wellbeing sector.\nThe key findings of the results are based on the\nanalysis of the data collected through the interviews\nwith the respondents. Appendix 1 includes some of\nthe respondents’ citations to provide further insight\ninto the themes that emerged. However, not all key\nfindings are represented in the appendix, as some\nwere not explicitly stated by the respondents but\nwere inferred from the"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "ated by the respondents but\nwere inferred from the overall interview. Therefore,\nit is important to read the entire interview transcript\nto fully understand the key findings.\nThis section is divided into subsections accord-\ning to the key findings categories. In each subsec-\ntion, it will cover the main findings by dividing\nthe answers into groups. Also, a synthesis for the\nfindings for each question is provided.\n4.1 AI in Products and Services\nCompanies have considered using AI or are already\nembracing it in very different ways. Based on the\ninterviews, it’s possible to identify three different\nways companies are using or considering AI in\nthe health care sector. AI can be used as a tool\nfor analysis, seen as a possible future solution or\nprovided as their core product.\nCompanies 2, 3 and"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "provided as their core product.\nCompanies 2, 3 and 5 reported that they use AI\nto conduct analysis on health data. Company 5\nused AI to automatically detect anomalies in ECG\nanalysis results whereas the other two companies\nused AI in a less autonomous way to analyse data\nfor medical professionals to make better judgments.\nCompany 2 believed firmly that they could autom-\natize even the step where a medical professional\nneeds to take a look at the results and have an AI\ndiagnose and interpret the data as well.\nCompanies 1 and 6 are looking into using AI in\ntheir work. Company 6 has identified that their\nproblem of working with brain data related to\nepilepsy is often predictable. They envision em-\nbracing AI in the future to automatically identify\nwhen an epileptic seizure is about to happen."
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "tify\nwhen an epileptic seizure is about to happen. Com-\npany 1 has taken some steps towards processing\nfundus images automatically by shortlisting poten-\ntial companies whose technology is mature enough\nto detect anomalies in such data. However, Com-\npany 1 points out the issue arising from a limited\namount of data for training AI models, which might\nmake their AI aspirations unfeasible.\nCompany 4 stands out from the crowd by being\nthe only company that provides AI services as theirmain product. They are primarily a machine learn-\ning company and their task is to cater for health\nAI related needs of their clients. They provide AI\nsolutions for diagnostic needs.\n4.2 How AI is Defined\nAI is quite a flexible notion as it can consist of\nmany different aspects of computing starting from\nsimple "
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "fferent aspects of computing starting from\nsimple programming to machine learning. This sec-\ntion will describe how the interviewed companies\nunderstand the word AI. The companies defined\nAI as a learning algorithm, quality of life enhancer\nand human-level anomaly detector.\nMost of the companies (1, 2, 3 and 4) had a mod-\nern definition for AI, that is that it is some sort of an\nalgorithm that ends up learning predictions based\non data. Company 2 highlighted the importance of\nspeed and that AI can be used to partially replace\na costly medical specialist, however, they pointed\nout that medical doctors do not easily accept their\nAI colleague but refer to issues like privacy con-\ncerns. Company 3 also pointed out the problem of\nprivacy by mentioning EU regulations on the use\nof medical data. "
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "ioning EU regulations on the use\nof medical data. Company 1 defined AI narrowly\nfrom the point of view of a learning classifier. They\nsaw the lack of clean training data as an issue and\na hindrance in developing AI. Company 4 wanted\nto point out that AI is such a large field that from\ntheir point of view, they are dealing with machine\nlearning rather than AI.\nCompany 6 had not yet embraced AI, which is\nsomething reflected in the way they understood AI.\nFor them it is a question of a quality-of-life im-\nprovement over not using an AI at all. Company\n5 had the highest hopes for AI by defining it as a\nhuman-level anomaly detector. This answer differs\nfrom the majority in the sense that the company\nsees AI as an unsupervised tool that can detect ten-\ndencies from data without being an actively"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "t ten-\ndencies from data without being an actively learn-\ning agent.\n4.3 Perceived Level of AI Maturity\nThis section will describe how companies per-\nceived their own level of AI maturity following\nthe levels established by Gartner. The interviewed\ncompanies self-identified as being in categories 1,\n2 and 3. These are well in line with the discus-\nsion in the earlier sections which means that their\nself-reporting is rather honest in terms of how they\ndescribed they actually used AI tools.\nCompanies 1 and 6 reported level 1 as their own\nlevel. Company 6 highlighted the issue of costs re-\nlated to transitioning from an AI-aware level into a\nlevel where AI is actively used. There are costs not\nonly related to development but also related to con-\nforming with all regulations that are in place."
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "n-\nforming with all regulations that are in place. Com-\npany 1 reported that their own level is currently\n1, but they estimated the level of their short-listed\nfuture collaborators to be 3.\nCompanies 2, 3 and 5 reported their level to be\n2, that is the level in which AI is applied mostly for\ndata science needs. Company 2 also identified that\nthey are envisioning a medical head instrument that\nis currently on the level 1 of AI maturity.\nCompany 4, which is the one relying solely on\nAI in their business model, was the only one report-\ning their level to be as high as 3. This is the level of\nAI in production where new value is being created\nthrough AI. The company does not have any aspi-\nrations to climb higher on the AI maturity levels\nbecause they are a small company and cannot reach\nthe st"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "e they are a small company and cannot reach\nthe stars.\n4.4 AI Application Areas\nThis section will describe how the interviewed com-\npanies for this study use AI outside of the main ap-\nplication area that has been described in the earlier\nsections. Mostly none of the companies really uses\nAI for any other business applications. Companies\n1, 3, 4 and 6 failed to give any example on how\nthey would utilize AI in other areas.\nCompany 2 identified that they do use AI in\nmarketing. They host an AI-powered chatbot on\ntheir website. Apart from this, the company did not\nidentify other uses for AI in their business.\nCompany 5 pointed out an unintentional use of\nAI. They only use AI in other areas because it is\nalready baked in the software they use on a daily\nbasis such as Microsoft and Atlassian to"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "n a daily\nbasis such as Microsoft and Atlassian tools.\nThese insights provide an overview of how\nFinnish SMEs in the health-tech sector are utiliz-\ning AI and their perspective on its maturity and\napplication areas. The next sections will continue\ndiscussing the challenges faced and the perceived\nimpact of AI adoption in the healthcare industry.\n4.5 Perceived Impact of AI\nAI is hardly used just because it is trendy but be-\ncause it has a tangible impact on how business is\nconducted. This section describes what the inter-\nviewed companies had to say about the impact AI\nhas had on their work. The interviewed companies\nthought rather unanimously that AI is indispens-\nable for their operations. Only Company 6 reportedthat AI had no impact thus far, but this was due to\nthe fact that the company"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "far, but this was due to\nthe fact that the company had not started to use AI\nyet. Interestingly, even Company 1, which does not\nyet use AI, reported that AI is a must-have, which\nexplains why they are actively seeking a suitable\nAI collaborator.\nCompanies 1, 2, 3, 4, and 5 stated that AI is\nessential. Company 2 identified that conducting\nthe level of analysis they need to do would be im-\npossible without AI methods. Company 4, which\nis purely an AI-based company, stated that they\nwould not have any market value without AI. Fur-\nthermore, Company 4 indicated that embracing AI\ngave them an advantage in acquiring funding.\n4.6 Data Source\nIt is no secret that AI relies heavily on data. Just as\nthe definitions for AI suggested by the interviewed\ncompanies, modern AI is mainly about learning\nfro"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "\ncompanies, modern AI is mainly about learning\nfrom data. This section describes the findings on\nwhat data sources the companies rely on. Data is\neither collected in-house or obtained from external\nproviders.\nCompanies 2, 4, 5, and 6 report using in-house\ndata. In the case of Company 6, it is stated as a\npossible hypothetical data source. For other compa-\nnies, they report that their data comes from differ-\nent measuring devices that monitor patients, such\nas ECG and EEG. The aforementioned companies\nhave not considered the need for additional comple-\nmentary data from other companies or open reposi-\ntories.\nCompanies 1 and 3 use external providers. Com-\npany 1 stated that they collaborate with manufactur-\ners of different fundus cameras to gain more data.\nCompany 3 has access to big data;"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": " gain more data.\nCompany 3 has access to big data; however, they\nare still looking for ways to benefit from it. This\nis understandable given that big data may conceal\nanswers to many questions one does not even think\nof initially.\n4.7 Computing Environment\nGiven that AI relies heavily on data, another issue\nneeds to be taken care of: the computing envi-\nronment. AI models need to be trained on data,\nwhich might require high usage of computational\nresources. This section describes the computing\nenvironments the companies used. The companies\nhad either outsourced AI tools entirely, used a pri-\nvate server, or a public cloud.\nCompanies 1, 2, and 6 stated that either their AI\ntools are provided by third parties or that they will\nbe provided by third parties. Company 2 further\nmentioned that th"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "third parties. Company 2 further\nmentioned that their team is too small to handle\ntheir own computing environment for AI needs.\nCompanies 4 and 5 use public cloud providers,\nAmazon AWS and Microsoft Azure, respectively.\nCompany 3 uses public clouds for training AI mod-\nels and private servers to handle personal data.\nCompany 6 envisions that they will start with a\nprivate server and, if needed, move to a public\ncloud.\n4.8 Challenges in AI\nNew technology might seemingly come with all\nthe bells and whistles, but embracing it is not al-\nways straightforward. This section describes the\nchallenges the informants faced when implement-\ning AI and when using it. These challenges can\nbe categorized into three main areas: regulations,\nmarket acceptance, and talent acquisition.\nIf all the companies w"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "ce, and talent acquisition.\nIf all the companies were interviewed simulta-\nneously, they would likely have said in unison that\nthe EU has regulations that are too strict for health-\nrelated AI. Companies 2, 3, 4, and 6 all stated that\nthe USA has more lenient laws on many aspects.\nCompanies 2 and 4 had issues with personal data\nregulations in the EU. Additionally, Company 4\nmentioned facing legal challenges when trying to\nget approval for their technology. Company 6 faced\nissues with strict medical certification requirements\nthat, again, are more relaxed in the USA and China.\nFinally, Company 3 mentioned that the US medical\nauthority FDA allows the use of AI models that are\ncontinuously learning from data whereas the EU\nallows models that are trained once and tested on at\nleast 200,000 sam"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "re trained once and tested on at\nleast 200,000 samples. Thus, their challenge was\nrelated to the inflexibility of the regulations.\nCompanies 1 and 2 had issues with market accep-\ntance. Both companies reported that it was difficult\nto get approval from medical professionals on the\ncustomer side to start using the AI in production.\nNew technology is often met with a degree of re-\nsistance and skepticism, which might explain these\nfindings.\nCompanies 5 and 6 reported a more concrete\nissue of being able to find competent members of\nstaff. Both companies struggle to find people with\na suitable medical background and a necessary set\nof R&D skills in the field of AI. Perhaps this is\nexplained by the fact that machine learning and\nmedicine are taught as very different subjects in\nmany universitie"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "ght as very different subjects in\nmany universities with little to no overlap.4.9 Perceived Benefits of AI\nIn terms of benefits, the interviewed companies saw\ntwo main advantages: speed and accuracy, and in-\ndispensability. This section briefly describes what\nthe companies had to say about these, although\nthere are probably many more benefits that the in-\nformants did not consider during the interview.\nCompanies 2, 3, 5, and 6 stated that the main ben-\nefit of AI is that it can perform laborious analysis\nwork faster than a human being and do so with high\naccuracy. This means that the problems the com-\npanies deal with are also defined well enough that\nthe AI models have learned not to err frequently.\nCompanies 1 and 4 continued to see AI as a ne-\ncessity. In the case of Company 4, AI truly"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": " a ne-\ncessity. In the case of Company 4, AI truly is their\nlifeline given that their entire operations revolve\naround providing AI services. Company 1 also\nstated that there is a lot of room in the market and a\nlot of unsatisfied innovation potential, especially in\nthe EU for health-related AI tools, unlike in Asia,\nwhere the market is already oversaturated.\n4.10 Wishes for Third Parties\nThis section describes what needs the companies\nreported they would have for third-party services to\nsupport their AI ventures. Interestingly, Companies\n4, 5, and 6 reported absolutely nothing. For the\nother companies, the needs can be classified into\naccess to resources and budget solutions.\nCompanies 1 and 2 stated that they would be\ninterested in having access to more data from ex-\nternal providers. Gi"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": " access to more data from ex-\nternal providers. Given that AI runs on data, it is\nno surprise that such a need might emerge. As de-\nscribed in earlier sections, many companies relied\nheavily on their in-house data, but even so, in the\nworld of AI, more is always better.\nCompanies 2 and 3 also expressed a need for low-\ncost access to AI. Especially Company 2 stated that\nthe typical price tag of C300,000- C400,000 for an\nAI project developed by an external company is\nway too high for a small business. Company 2 sug-\ngested either lower prices or access to funding as a\nsolution. Company 3 advocated for cheaper access\nto high-performance computing so that AI models\ncan be trained in a more cost-efficient manner.\n4.11 Future Concerns\nThe field of AI is currently in an ever-changing\nstate with c"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "f AI is currently in an ever-changing\nstate with continuous innovations taking place in\nall areas of AI. This section describes how the in-\nformants see what the future holds for their compa-\nnies in relation to AI. The interviewed companies\nhad many different ideas for the future: solutions\nfor staff shortages, positive changes in healthcare,\nuse in other business aspects, changes in regula-\ntions, better AI models, and higher computational\nrequirements.\nCompany 2 sees AI as one possible solution for\nstaff shortages that result from a variety of factors\nsuch as an aging population. They also believe\nthat AI will bring a positive change to how health-\ncare services are provided. For example, a patient\nwould not need to wait a long time to see a neurol-\nogist if an AI model could diagnose t"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "ee a neurol-\nogist if an AI model could diagnose the symptoms\nautomatically.\nCompany 4 foresees a clear regulatory need for\nintroducing standards to healthcare data and AI\nmodels. The current situation is a Wild West with\nno cohesive practices. Meanwhile, Company 1\npresents a practical issue still challenging for mod-\nern AI techniques: detecting more than one symp-\ntom at a time accurately.\n4.12 Advice for Other Companies\nWhen the companies were asked about the advice\nthey would give to another company that has not\nyet considered AI at all, they provided responses\nthat fit into the following categories: gathering data,\ndefining the problem, hiring competent people, and\nstarting experimentation. These steps, combined,\nform a practical roadmap for companies looking to\nintegrate AI into thei"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "ap for companies looking to\nintegrate AI into their business operations.\nCompanies 2 and 3 emphasized the importance\nofgathering data . As Company 2 puts it, it is bet-\nter to start collecting data sooner rather than later,\neven if AI plans are not in the near future. Data\nis the foundation of AI, and having access to well-\nstructured datasets ensures a smoother transition\nwhen the company is ready to implement AI solu-\ntions. Company 3 also noted that it is important to\nanalyze the collected data to understand what value\ncan be extracted from it.\nCompanies 1 and 2 discussed the significance of\ndefining the problem . The sooner a company has\nclarity regarding the problem it wants to solve, the\nsooner it will know what type of data is required,\naccording to Company 2. Company 1 pointed out\n"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "ed,\naccording to Company 2. Company 1 pointed out\nthat taking extra care in specifying goals correctly\nfrom the beginning is essential, as unclear objec-\ntives can lead to inefficient AI implementation and\nwasted resources.\nCompany 6 recommended that businesses hirecompetent people with the right mix of skills. They\nhighlighted the challenge of finding professionals\nwho possess both AI expertise and knowledge in\nthe healthcare sector. The integration of AI in\nhealthcare requires interdisciplinary collaboration\nbetween AI experts, medical professionals, and\nbusiness strategists.\nCompanies 4 and 5 encouraged businesses to\nstart experimenting with AI. They emphasized the\nneed for companies to test AI solutions in small,\ncontrolled environments before fully committing\nto large-scale implementa"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": " before fully committing\nto large-scale implementations. Company 5 sug-\ngested that businesses should begin by playing\naround with their data to gain a deeper understand-\ning of potential AI applications. Company 4 also\npointed out that many AI tools and frameworks are\nreadily available, making it easier for businesses to\nstart experimenting with AI-driven solutions.\n5 The Threefold Model of AI in Business\nTo complete the development task of this research,\na collaborative brainstorming development method\nwas utilized. This method involves generating a\nlarge number of ideas and then selecting the most\npromising ones to pursue further (Wilson, 2013). A\nbrainstorming session with two members from the\ncommissioner was organized to generate solutions\non what services can pave the way for health"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "tions\non what services can pave the way for health-tech\nSMEs to adopt and develop the use of AI, and how\nhealth-tech companies can cooperate together to\nelevate the level of AI in the sector. The ideas\nwere then grouped and analyzed to identify the\nmost relevant and feasible ones. This collaborative\nmethod allowed the identification of potential gaps\nin AI utilization in the health-tech sector and to\ncome up with a state-of-the-art framework.\nBased on the findings during the interviews, a\nthreefold model on the use of AI in business has\nbeen elaborated. The following three categories\nhave been identified for AI in business: AI Curious,\nAI Embracing, and AI Catering companies. This\nsection will shed more light on each of these cat-\negories and how they differ from each other. The\ncategoriza"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "nd how they differ from each other. The\ncategorization is based on how AI is operational-\nized in different companies.\nThe model is useful when trying to understand\nand better analyze the use of AI from a grassroots\nlevel. This can help companies better locate them-\nselves in terms of AI and business. One company\ndoes not need to fit in only one of the categories\neither, but a company can, for example, be AI Cater-\nFigure 1: Threefold Model of AI in Business\ning in providing a specific solution for healthcare\nand AI Curious when planning on integrating AI\ninto marketing practices.\nIt is important to note that the highest levels of\nAI maturity are not part of this framework because\nthe interviewed companies would not place them-\nselves that high in the hierarchy. This tells us also\nsomethin"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "high in the hierarchy. This tells us also\nsomething about the paradigm shift in the field of\nAI, where hardcore AI research and development\nis in the hands of larger companies such as Google,\nMeta, or OpenAI, while the field-specific use of AI\nis often handled by companies that do not have mas-\nsive resources for core AI research. Revolutionary\nAI methods such as word embeddings (Mikolov\net al., 2013) and the Transformer model (Vaswani\net al., 2017) have been developed by Google, mod-\nels such as ChatGPT and DALL-E (Ramesh et al.,\n2022) by OpenAI, and audio embeddings (Baevski\net al., 2020) by Meta. In short, there is no room for\na small player to compete in the space of new AI\nrevolutions.\n5.1 AI Curious\nAn AI Curious company is still in the process of\nplanning. Such a company can be curr"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "he process of\nplanning. Such a company can be currently identi-\nfying possible problems where AI can be used or\ncan already be in talks with AI-providing compa-\nnies about solving a particular problem. AI curious\ncompanies may engage in practices of collecting\ndata and analysing it to uncover its potential in the\nfuture when the company is ready to start using AI\nin their day-to-day operations.\nAI curiosity can thus be a time of great explo-\nration of both AI and its added value to the target\nmarket. AI curious companies could benefit from\ncost-efficient consultants, external R&D funding,and existing tools and datasets. The first stop for\nan AI curious company might thus be an open data\nplatform such as Zenodo or Kaggle or an open\nAI model platform such as Huggingface or Model\nZoo.\nAt this"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "platform such as Huggingface or Model\nZoo.\nAt this stage, it is important that the company\nhas a clear idea of what the actual AI problem is\nbefore moving to the next stage of embracing AI.\nMoving forward with an ill-defined problem might\nhave costly consequences or poor market adapta-\ntion. This calls for a degree of understanding of\nthe current limitations and possibilities of what can\nand cannot be done with AI. This understanding\ncan be acquired internally or externally.\n5.2 AI Embracing\nAI Embracing companies have already started to\nuse AI in their business operations. However, they\neither do not develop AI by themselves but buy it\nas a service from an external provider or if they\ndevelop AI in-house, it is not their main product\nbut rather an auxiliary tool for their actual product\nt"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "ather an auxiliary tool for their actual product\nthat could be provided without AI as well.\nAn AI Embracing company has identified one\nor a few targeted problems that they can optimize\nwith AI. They, however, are not fully relying on\nAI because AI is used as a functional part of their\nbusiness pipeline that also consists of manual tasks\nsuch as the final analysis or diagnosis of the num-\nbers crunched by AI.\nA strong collaboration between an AI Embracing\ncompany and their AI provider is advised. Modern\nAI is entirely data-driven, and thus better results\ncan be obtained if the AI Embracing company is ca-\npable and willing to share their own data with their\nAI provider. An AI Embracing company might run\nAI models on their own servers or on an external\ncloud over an API access.\n5.3 AI Caterin"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": " external\ncloud over an API access.\n5.3 AI Catering\nCompanies that are AI Catering provide AI services\nto other companies that are currently only embrac-\ning AI or in the AI Curious stage. AI products and\nservices are the core offerings of AI Catering com-\npanies. AI Catering companies do not necessarily\ndevelop their own cutting-edge AI solutions, but\nthey can rather use existing AI methods, such as\nTransformers (Wolf et al., 2019), Datasets (Lhoest\net al., 2021), PyHFST (Alnajjar and Hämäläinen,\n2023), Gensim (Rehurek, n.d.), and SciKit (Pe-\ndregosa et al., 2011), that they train on in-domain\ndata to solve the business problems their customers\nhave.\nAI Catering companies can provide and train AI\nmodels on their own servers or outsource the heavy\ncomputation to a cloud provider such as AW"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "e heavy\ncomputation to a cloud provider such as AWS or\nAzure. While AI is typically provided as a service,\nAI Catering companies may provide their solutions\nso that their clients can run the AI models on their\nown machines.\nBecause real state-of-the-art AI development has\nmoved beyond the reach of smaller companies, AI\nCatering companies can only truly compete against\neach other with data. The more and better-quality\ndata an AI Catering company has, the better their\nAI models will be and the more advantage they will\nhave in the market. Access to computational re-\nsources plays an important role here as well. Large\namounts of data require more computational power\nto be harnessed in use.\nThis threefold model provides a structured way\nto understand AI adoption in SMEs, helping busi-\nnesses na"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "stand AI adoption in SMEs, helping busi-\nnesses navigate their AI journey more effectively.\nThe next section will discuss inter-categorical busi-\nness opportunities and how companies within these\nthree categories can collaborate to maximize the\nbenefits of AI.\nFigure 2: Interdependence of Companies in Different\nCategories of Business AI\n6 Discussion\nMost companies are still in the early stages of AI\nadoption, either experimenting or in the initial im-\nplementation phase, with none reaching high matu-\nrity yet. Despite recognizing AI’s potential to im-\nprove healthcare, they face three major challenges:\nlegal, technical, and financial.\nAll interviewees highlighted that regulatory com-\npliance is the most significant barrier. The legal\nframework governing AI in healthcare has not keptpace wi"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "ork governing AI in healthcare has not keptpace with technological advancements, creating\nhurdles for innovation (Powles and Hodson, 2017).\nIn Finland and the EU, medical licensing is a com-\nplex, evolving process, making AI implementation\ndifficult (Petersson et al., 2021). For instance, con-\nducting clinical trials for a new AI-powered med-\nical device requires national authority approval,\nwhich can take months and necessitate 180,000\ntests to ensure safety (Jiang et al., 2017). Even\nafter approval, further ethical committee clearance\nis required, adding time and financial strain.\nThese findings underscore the urgent need for\nregulatory improvements to support AI adoption\nin Finland’s health-tech sector. Streamlining medi-\ncal licensing and approval processes would reduce\ndelays and cost"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "nd approval processes would reduce\ndelays and costs, allowing companies to focus on\ninnovation and deployment of AI solutions.\nTwo-thirds of interviewees also cited difficulties\nin finding qualified AI professionals with both med-\nical expertise and programming skills. \"There are\ncurrently few AI experts in the health-tech indus-\ntry,\" stated one company leader. Other technical\nbarriers include collecting and cleaning reliable\ndata, integrating AI with hospital systems, and over-\ncoming resistance from medical professionals, who\noften require years to accept new technologies.\nFinancial constraints further limit AI adoption.\nHigh upfront costs, delayed return on investment,\nlimited access to training data, and expenses related\nto regulatory compliance present significant barri-\ners. Privacy"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "compliance present significant barri-\ners. Privacy restrictions and legal requirements\nfurther complicate AI implementation for SMEs in\nthe sector.\nBased on interviews and analysis, this study rec-\nommends several measures to enhance AI utiliza-\ntion in health-tech SMEs, particularly in Finland\nand the EU. Regulatory frameworks should be re-\nformed to facilitate AI integration while ensuring\ndata privacy and patient safety. Simplifying re-\nsearch processes, streamlining medical licensing,\nand reducing bureaucratic barriers will allow com-\npanies to focus on innovation rather than compli-\nance hurdles.\nDeveloping AI talent is crucial, as many SMEs\nstruggle to find qualified professionals. A solution\nis to invest in continuous training programs tailored\nto healthcare AI, lowering entry barri"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "ms tailored\nto healthcare AI, lowering entry barriers while en-\nhancing expertise. Additionally, attracting skilled\nforeign professionals with competitive salaries and\nbenefits can bridge the talent gap.\nFinancial barriers remain a major obstacle for\nAI adoption, given the high initial investment and\ndelayed returns. Establishing funding mechanisms,\nincluding government grants and private invest-\nments, would provide SMEs with the resources\nneeded to integrate AI solutions. Collaboration\nplatforms can further support AI adoption by con-\nnecting SMEs with research institutions, AI ser-\nvice providers, and industry partners, enabling\nknowledge-sharing and joint innovation.\nEnsuring data security is another priority. A cer-\ntification system for AI data privacy should be intro-\nduced, verifyi"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "or AI data privacy should be intro-\nduced, verifying that companies handling sensitive\nmedical data comply with strict security standards.\nThis would enforce encrypted storage, controlled\naccess, and detailed audit logs to safeguard patient\ninformation while enabling responsible AI deploy-\nment.\nThese recommendations offer practical strate-\ngies for policymakers and industry stakeholders to\nfoster AI adoption in the Finnish health-tech sector,\nbalancing innovation with regulatory compliance\nand data security.\n7 Conclusion\nThis study examined AI adoption among Finnish\nhealth-tech SMEs, identifying key challenges and\nopportunities. While AI holds immense potential\nfor enhancing healthcare efficiency and patient out-\ncomes, most SMEs remain in early adoption stages\ndue to regulatory barriers,"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": " early adoption stages\ndue to regulatory barriers, limited AI expertise, and\nfinancial constraints. A more flexible legal frame-\nwork, improved access to AI talent, and increased\nfunding opportunities are necessary to accelerate\nAI integration in healthcare.\nAddressing these challenges will enable SMEs\nto leverage AI for innovation, ultimately benefit-\ning the healthcare sector and society. Future work\nshould explore collaborative AI development mod-\nels, interdisciplinary training programs, and policy\nreforms to foster AI adoption. By streamlining\nregulations and promoting industry partnerships,\nAI-driven solutions can be more effectively imple-\nmented, ensuring sustainable growth and improved\npatient care.\nReferences\nKhalid Alnajjar and Mika Hämäläinen. 2023. Pyhfst:\nA pure python implem"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "ika Hämäläinen. 2023. Pyhfst:\nA pure python implementation of hfst. In Lightning\nProceedings of NLP4DH and IWCLUL 2023 , pages\n32–35.A. Baevski, H. Zhou, A. Mohamed, and M. Auli. 2020.\nwav2vec 2.0: A framework for self-supervised learn-\ning of speech representations.\nA. Bettoni, D. Matteri, E. Montini, B. Gladysz, and\nE. Carpanzano. 2021. An ai adoption model for smes:\nA conceptual framework. IFAC-PapersOnLine ,\n54(1):702–708.\nA. Bunte, F. Richter, and R. Diovisalvi. 2021. Why it\nis hard to find ai in smes: A survey from the practice\nand how to promote it. In ICAART 2021 - Proceed-\nings of the 13th International Conference on Agents\nand Artificial Intelligence , volume 2, pages 614–620.\nTirth Dave, Sai Anirudh Athaluri, and Satyam Singh.\n2023. Chatgpt in medicine: an overview of its ap-\npl"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "23. Chatgpt in medicine: an overview of its ap-\nplications, advantages, limitations, future prospects,\nand ethical considerations. Frontiers in artificial in-\ntelligence , 6:1169595.\nM. Garbuio and N. Lin. 2019. Artificial intelligence as\na growth engine for health care startups: Emerging\nbusiness models. California Management Review ,\n61(2):59–83.\nMika Hämäläinen. 2024. Legal and ethical considera-\ntions that hinder the use of llms in a finnish institution\nof higher education. In Proceedings of the Workshop\non Legal and Ethical Issues in Human Language\nTechnologies@ LREC-COLING 2024 , pages 24–27.\nMohd Javaid, Abid Haleem, and Ravi Pratap Singh.\n2023. Chatgpt for healthcare services: An emerging\nstage for an innovative perspective. BenchCouncil\nTransactions on Benchmarks, Standards and Ev"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "uncil\nTransactions on Benchmarks, Standards and Evalua-\ntions , 3(1):100105.\nF. Jiang, Y . Jiang, H. Zhi, Y . Dong, H. Li, S. Ma,\nY . Wang, Q. Dong, H. Shen, and Y . Wang. 2017.\nArtificial intelligence in healthcare: past, present and\nfuture. Stroke and Vascular Neurology , 2(4):230–\n243.\nQ. Lhoest et al. 2021. Datasets: A community library\nfor natural language processing. In EMNLP 2021\n- 2021 Conference on Empirical Methods in Natu-\nral Language Processing: System Demonstrations ,\npages 175–184.\nT. Mikolov, K. Chen, G. Corrado, and J. Dean. 2013.\nEfficient estimation of word representations in vector\nspace. In 1st International Conference on Learn-\ning Representations, ICLR 2013 - Workshop Track\nProceedings .\nZ. Nabulsi et al. 2021. Deep learning for distinguishing\nnormal versus abnormal "
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "earning for distinguishing\nnormal versus abnormal chest radiographs and gen-\neralization to two unseen diseases tuberculosis and\ncovid-19. Scientific Reports , 11(1):1–15.\nNiko Partanen, Mika Hämäläinen, and Tiina Klooster.\n2020. Speech recognition for endangered and\nextinct samoyedic languages. arXiv preprint\narXiv:2012.05331 .\nF. Pedregosa et al. 2011. Scikit-learn: Machine learning\nin python. Journal of Machine Learning Research ,\n12:2825–2830.\nL. Petersson, I. Larsson, J. M. Nygren, M. Neher, J. E.\nReed, D. Tyskbo, and P. Svedberg. 2021. Challenges\nto implementing artificial intelligence in healthcare:\na qualitative interview study with healthcare leaders\nin sweden. Health Services Research , 22:850.\nJ. Powles and H. Hodson. 2017. Google deepmind\nand healthcare in an age of algorithms."
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "e deepmind\nand healthcare in an age of algorithms. Health and\nTechnology , 7(4):351–367.\nA. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and\nM. Chen. 2022. Hierarchical text-conditional image\ngeneration with clip latents.\nS. Reddy, J. Fox, and M. P. Purohit. 2019. Artificial\nintelligence-enabled healthcare delivery.\nRehurek. n.d. Gensim–python framework for vector\nspace.\nM. Saunders, P. Lewis, A. Thornhill, S. Lewis, and\nThornhill. 2009. Research methods for business stu-\ndents fifth edition .\nKenji Suzuki. 2017. Overview of deep learning in med-\nical imaging. Radiological physics and technology ,\n10(3):257–273.\nP. Ulrich and V . Frank. 2021. Relevance and adoption\nof ai technologies in german smes - results from\nsurvey-based research. Procedia Computer Science ,\n192:2152–2159.\nFarzan Vahedifar"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "Computer Science ,\n192:2152–2159.\nFarzan Vahedifard, Atieh Sadeghniiat Haghighi, Tirth\nDave, Mohammad Tolouei, and Fateme Hoshyar\nZare. 2023. Practical use of chatgpt in psychiatry for\ntreatment plan and psychoeducation. arXiv preprint\narXiv:2311.09131 .\nA. Vaswani et al. 2017. Attention is all you need. Ad-\nvances in Neural Information Processing Systems ,\n30.\nC. Wilson. 2013. Brainstorming and beyond: a user-\ncentered design method . Newnes.\nT. Wolf et al. 2019. Huggingface’s transformers: State-\nof-the-art natural language processing.\nR. K. Yin. 2009. Case study research: Design and\nmethods , 4 edition. Sage Publications.\nA Questions asked in the interview\nTable 1 lists the questions asked during the inter-\nviews.B Summary of key findings\nThe key findings from the interviews are summa-\n"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "s\nThe key findings from the interviews are summa-\nrized in the table below. These insights provide\na clear view of how AI is being integrated into\nFinnish health-tech SMEs, the challenges faced,\nand the opportunities for future growth are shown\nin Table 2.\nInterview Theme Question Why was the question asked?\nCompany’s current\nposition on AICould you tell us about yourself and your com-\npany’s activities?Answering these questions will con-\ntribute to building knowledge about the\ncompany’s current position on the use\nof AI in the healthcare context, thus will\ncontribute to answering RQ1.\nDo you consider AI use in your business solu-\ntions? (& why?)\nAt what level of Gartner AI Maturity Model is\nyour company currently?*\nWhat impact has your company experienced\nfrom the use of AI?\nChallenges as"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "pany experienced\nfrom the use of AI?\nChallenges associated\nwith the adoption and\nuse of AI in healthcare\nindustryWhat problems did you face at the initial stage\nof adopting AI in healthcare?To explore and search the challenges\nthat Finnish health-tech SMEs face in\nthe adoption and usage of AI, and that\nis critical to answer RQ2 and to develop\nreal-world practical solutions for them.\nHow did your company resolve these issues?\nWhat are the current challenges the company\nis facing in applying AI in healthcare?\nWhat future concerns do you expect to exist\naround the use of AI in healthcare?\nReliable\nrecommendations from\nthe field expertsIn your opinion, what are the actions that can\nresolve these challenges?To collect informative opinions from in-\ndustry leaders about action plans that\ncan lead"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "n-\ndustry leaders about action plans that\ncan lead to elevate the AI-Maturity level\nin the sector.\nWhat services do you wish to be provided by\nAI-solution providers to facilitate the emer-\ngence of AI among health-tech companies?\nWhat is your advice to start-ups in the health-\ntech sector on the use of AI?\nTable 1: A summary of the research questions asked in the interviews.\n* Gartner AI Maturity model is briefly explained to the interviewee before being asked the question.\nQuestion is asked if it’s valid and logical to be asked, thus interviews are semi-structured.\nCategory Key Findings\nAI in Products & Services Used for analysis, potential future solutions, core AI products.\nDefinition of AI Seen as a learning algorithm, quality-of-life enhancer, anomaly detector.\nAI Maturity Levels Comp"
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "nhancer, anomaly detector.\nAI Maturity Levels Companies mostly at levels 1, 2, and 3 of Gartner’s AI Maturity Model.\nAI Application Areas Mainly used for health analysis; some use in marketing and operational tools.\nPerceived Impact Indispensable for most companies; non-users acknowledge AI’s potential.\nData Source Companies use in-house and external data sources, with privacy concerns.\nComputing Environment AI tools outsourced, private and public cloud services used.\nChallenges Strict regulations, market resistance, difficulty hiring AI talent.\nBenefits of AI Improved speed and accuracy, operational necessity.\nWishes for 3rd Parties Lower-cost AI solutions, better access to data.\nFuture Concerns Increased computational requirements, regulatory standardization.\nAdvice for Companies Gather "
  },
  {
    "arxiv_id": "2503.14527",
    "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare\n  SMEs",
    "chunk": "tory standardization.\nAdvice for Companies Gather data, define problems, hire skilled professionals, experiment with AI.\nTable 2: Summary of key findings on AI adoption and challenges.\n"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "Towards a Capability Assessment Model for the Comprehension and Adoption \nof AI in Organi sations  \n \nAuthors  \nTom Butler, Angelina Espinoza -Limóna and Selja  Sepp älä \nBIS Department, University College Cork, Ireland  \nAbstract  \nThe comprehension and adoption of Artificial Intelligence (AI) are beset with practical and ethical \nproblems. This article presents a 5 -level AI Capability Assessment Model (AI -CAM) and a related AI \nCapabilities Ma trix (AI -CM) to assist practitioners in AI comprehension and adoption. These practical tools \nwere developed with business executives, technologists, and other organisational stakeholders in mind. \nThey are founded on a comprehensive conception of AI compare d to those in other AI adoption models \nand are also open- source artefacts. Thus, the A"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": " \nand are also open- source artefacts. Thus, the AI -CAM and AI -CM present an accessible resource to help \ninform organisational decision- makers on the capability requirements for (1) AI -based data analytics use \ncases base d on machine learning technologies; (2) Knowledge representation to engineer and represent \ndata, information and knowledge using semantic technologies; and (3) AI -based solutions that seek to \nemulate human reasoning and decision- making. The AI -CAM covers t he core capability dimensions \n(business, data, technology, organisation, AI skills, risks, and ethical considerations) required at the five \ncapability maturity levels to achieve optimal use of AI in organisations. The AI -CM details the related \nindividual and team -level capabilities needed to reach each le"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "d team -level capabilities needed to reach each level in organisational AI capability; it, \ntherefore, extends and enriches existing perspectives by introducing knowledge and skills requirements at \nall levels of an organisation. It posits three levels of AI  proficiency: (1) Basic, for operational users who \ninteract with AI and participate in AI adoption; (2) Advanced, for professionals who are charged with \ncomprehending AI and developing related business models and strategies; and (3) Expert, for computer \nengineers, data scientists, and knowledge engineers participating in the design and implementation of AI -\nbased technologies to support business use cases. In conclusion, the AI -CAM and AI -CM present a valuable \nresource for practitioners, businesses, and technologists, looking to"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "tioners, businesses, and technologists, looking to innovate using AI technologies and \nmaximise the return to their organisations.b  \nKeywords   \nArtificial Intelligence , Capability Assessment Model , AI adoption, AI skills,  AI capabilities , AI literacy  \n1 Introduction  \nOnce the domain of science fiction, A rtificial Intelligence (AI) now permeate s societ y: It powers \napplications in our phones, offices, the factories that manufacture our goods, and increasingly the \ndigitalisation of business services . The current ubiquity of  AI was made possible by (a) The digit al \ntransformation of business and society; and (b) recent advances in data science , machine learning, and \nnatural language processing  that permit \"pattern recognition and smarter ways of automating data \nusage \". 1  "
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": " and smarter ways of automating data \nusage \". 1  \n                                                           \na Contact Email: aespinoza -at- xanum.uam.mx   \nb The AI -CAM and AI -CM are also available on GitHub: https://github.com/tgbutler/AI -Capability -Assessment -\nModel -. \nThe huge increase in computer processing power at all endpoints, from smartphones to cloud computing \nservers, saw  BigTech firms  invest heavily in  AI R&D to advance its application .2 Take, f or example : (a) in \nfinance, Fin Tech and RegTech  firms emerged with powerful AI-based solutions for smart fraud detection3,4 \nand financial regulation risk  management ;5 (b) in health -care, AI supports patient diagnostic ;6,7,8 (c) in \nenergy, AI is used for smart consumption ;9,10 and,  (d) in retailing, for identify"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "umption ;9,10 and,  (d) in retailing, for identifying customer consumption \npatterns .11,12  \nAI comprehension and adoption  capabilities and business use cases differ widely across business  domains  \nand industry sectors . Market research illustrates that 80% of large organisations plan to adopt or have \nadopted some form of AI, whereas only 8% of organisations have adopted AI to support core activities .13,14 \nOthers have focused on  single -pilot  projects to assess business u se cases .15  \nResearch by MIT and The Boston Consulting Group16 reveals that managers in 70% of 3000 firms studied \nsay they  underst and the role of AI in generating business value . In comparison,  59% of those firms stated \nthey had an AI strategy  in their organisation , and 57% were  already implementing AI"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "ganisation , and 57% were  already implementing AI -based solution s. \nThese figures show a significant increase in AI adoption since their previous report in 2017 .17  \nHowever, our unpublished research indicates  that many firms engage in AI -washing, so the actual use of \nAI in business may be much less than admitted. Then, it appears that AI means different things to different \npeople, indicating a comprehension problem. Most  business use cases of AI involve sophisticated digital \npattern matching, not reasoning and complex problem -solving.  Classifying all such applications like AI, \ntherefore,  requires clarification.   \nThe route to successful innovation using AI technologies is complex. Swanson and Ramillier state that \"Even if an organization provisionally adopts the innovation,"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": " organization provisionally adopts the innovation, charting a successful course for \nimplementation demands continued vigilance in maintaining and updating its conceptual framework, in \npart through reflection on its own practical experience, but also by monitoring closely the further \nevolution of the technology, its envisioned applications, and its marke tplace. All this creates in the adopter \norganization an intensive need for information \".\n18 They argue for the development by senior management \nof an 'organising vision ' whose purpose is to reveal organisation al opportunities for technology \nexploitation thr ough (1) the ' interpretation ' of the technology 's capabilities to achieve an understanding \nof how to innovate using it; (2) ' legitimation ' of the need for its adoption and"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": " ' legitimation ' of the need for its adoption and role in enabling or supporting  \nbusiness processes and decision making; and (3) ' mobi lisation ' of resources, financial, technical and \nhuman, internal and external, to help the adoption, implementation and use of the technology. These are \nimportant considerations and inform this paper's approach and contribution.  \nIn business contexts , AI comprehension and adoption is the remit of professionals  who comprehend, plan \nand adopt  AI to support well- defined business use cases . However, a question arises  as to whether  \nbusiness and technology professionals  have the required AI knowledge and skills (i.e.,  capabilities) to \naddress this challenge.   \nAccording to  a 2017 report by MIT Sloan Management Review ,19 one of the main barr"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "T Sloan Management Review ,19 one of the main barriers  to AI \nintroduction in organisation s relates to  the paucity of business  and technology capabilities . The report \nidentifies  specific challenges for executives to (1) Understand  AI features and functions , (2) Understand  \nsuitable business use cases for the application of AI, and (3) Develop  AI-based competitive strategies.  It \nalso show s a disparity in AI knowledge and skill leve ls, particularly  extant understandings of the full \nspectrum of AI technologies and how these can be  applied in business use cases  to yield a return on \ninvestment (ROI) .20  \nIn response to the AI business imperative, s everal problems  confront firms: (1)  Should the y upskill \nemployees ? Or (2) employ or contract in  AI professionals or capab"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": ") employ or contract in  AI professionals or capabilities ? Or (3) outsource AI projects  to \nprofessional services firms of AI vendors ? This means , business leaders , professionals  and technologists  \nneed to properly comprehen d AI to ensure an optimal fit between AI technologies and business uses \ncases.  Thus, business professionals and technologists in firms need the capabilities to : (1) Design AI \nadoption  strategies;  and ( 2) Enable employees to acquire  AI skills  commensurate with their role and level \nof involvement in AI projects  (e.g., managerial and technical) . This capabilities requirement  applies \nwhether solutions are provided in -house o r are  outsourced .  \nThe challenge  for firms is : how do they  guide AI knowledge and skills  acquisition  in an organisation "
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "ledge and skills  acquisition  in an organisation al \ncontext ? Understanding digital technology is a major problem for business executives —for example, \nmany are not aware of the difficulties in sourcing and deploying AI expertise .21 Furthermore, they are \nunaware of the range of AI knowledge and skills required across several disciplines , from data and  \ncomputer science to knowledge representation , system engineering , and business subject matter expert \n(SME) expertise.  However, to understand why AI capabilities are multidiscipli nary , it is necessary to \nunderstand what A rtificial Intelligence is—Section 2 sets the stage by addressing this fundamental issue .  \nThis article 's contribution is a  Capability Assessment Model  for AI adoption in organisation s: This specifies \nthe"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "AI adoption in organisation s: This specifies \nthe capabilities required to achieve desired organisation al outcomes. Ceteris paribus, the higher the level \nof capabilities demonstrated across the various dimensions, the greater the success  in AI  innovation.  \nTherefore, an organisation can  determine if a capabilities gap exists and use the model and the related AI \nCapabilities Matrix to close the gap in line with achieving the strategic and operational benefits of AI \nadoption.   \nThe remainder of this paper is structured as follows: As indicated, Section 2  presents contrasting \ndefinitions of AI to inform the development and understanding of the AI -CAM and the AI -CM. Section 3  \npresents the AI Capability Assessment Model . Section 4 presents  the AI Capabilities Matrix , which \ni"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": " 4 presents  the AI Capabilities Matrix , which \nindicates the knowledge and skills that need  to be acquired by the organisation 's staff  according to  \nrequired proficiency levels and the organisation 's AI capability maturity level  (1-5). Section 5 briefly \ndiscusses the contribution of this paper and offers some concluding thoughts . \n2 Underst anding AI and its Capabilities  \nThere are different conceptions and understandings of Artificial Intelligence and its capabilities .22 The Alan \nTuring Institute23 defines AI as \" the science of making computers do things that require i ntelligence when \ndone by  humans  \".24 A broader definition is that  \"AI is the theory and development of computer systems \nable to perform tasks normally requiring human intelligence, such as visual percepti"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "uiring human intelligence, such as visual perception, speech \nrecognition, decision -making, and translation between languages \" .25 Another definition  is that  \"AI is a set  \nof tools and technologies that has the ability to augment and enhance organisational performance. This \nis achieved by creating artificial systems to solve complex environmental problems, with I ntelligence being \nthe simulation of human -level intelligence \" .26  \nAI is also considered a general- purpose technology (GPT) with a unique learning capability that provides \norganisations with potentials for wide -ranging improvements, as well as entirely new business \nopportuni ties.27 To achieve AI's attributed outcomes involves applying concepts and techniques from \nmachine learning, deep learning, natural language pr"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "chine learning, deep learning, natural language processing, knowledge representation, expert systems, \nand robotics .28,29  \nThus, the following all- inclusive definition is adopted:  \"AI refers to multiple technologies that can be \ncombined in different ways to :\" sense, comprehend, and act, with \" the ability to learn from experience \nand adapt over time .\"30 Sense  refers to the capability to perceive the world aroun d the system (images, \nsound and speech) through technologies such as computer vision and audio processing; Comprehend  \nrefers to the capability to analyse and understand the data and information through inferencing; Act \nrefers to a capability to take action  through decision -making, choosing among competing courses of \naction, and so on. In the round, this  is considere"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "ction, and so on. In the round, this  is considered as weak or narrow AI .31 \nNarrow AI is all that is currently possible , at least commercially . As a computing paradigm , it has three \nimportant foci:32  \n1. Perceptual computing : This involves capturing and interpreting signals from the environment. It is \nrelated to pattern recognition and classification for numerical data and digitised visual, audio or \nanalogue text. Perceptual computing includes a family of machine learning (ML) , including deep \nlearning  (DL), algorithms requiring more or less  human intervention for their implemen tation , such \nas supervised (when all data is labelled to train algorithms), semi- supervised (when data is partially \nlabelled), and unsupervised (algorithms need to discover the data structure witho"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "gorithms need to discover the data structure without  human  \nsupervision)  learning . Some ML and DL applications focus  on natural language processing ( NLP) and \nimage recognition (vision computing).  \n2. Semantic  computing : This involves captur ing the meaning of concepts and the relations between \nthem. Thus, it is the knowledge representation of the business entities express ed as data. Semantic \ncomputing is mostly used to represent and organise data  into a knowledge base (KB), which is a \ngraph of connected nodes through  links, where the nodes are the business entities and the links , \nthe relationships between the entities.  A KB  also allows powerful inference if its underlying model \n(e.g. , an ontology) is defined in an ontology language.  \n3. Cognitive computing : This i n"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "logy language.  \n3. Cognitive computing : This i nvolves building upon and integrating percept ual and semantic  \ncomputing to perform complex computing tasks ba sed on algorithms that can learn, extrapolate, \nand explain how and why particular decisions/actions were taken .  \nThese three AI categori es are used to drive the development of the capability  assessment  model  \npresented in the next section.  \n3 Towards an AI Capability Assessment Model ( AI-CAM ) \nFor AI adoption  to be successful  in organisation s, senior decision -makers need to have an organising vis ion \nthat informs actors on the requisite structures and processes required to ensure the successful \ncomprehension, adoption, implementation and use of digital technologies ,33 including AI. Logically, this \nmeans that an o"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": ",33 including AI. Logically, this \nmeans that an organisation  needs to have the requisite capabilities , or the capability to acquire them , \nthrough  organisation al learning.  As this is an incremental pro cess that requires elaboration and guidance , \nthat is the purpose of this paper.   \nThe AI Capability Assessment Model (Table 3) presented herein provides guidance on  the core capability \ndimensions ( i.e., business, data, technology, organi sation, AI skills, risks, and ethical considerations)  \nrequired at different stage s and levels in AI comprehension and adoption. Thus , an organisation  will be \nable to align its goals  and objectives  and the capabilities required at each level.   \nThe AI-CAM  is inspired by two well- known maturity models:  The Capability Maturity Model Int"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "aturity models:  The Capability Maturity Model Integrationc \n(CMMI ) V2.0 and the Maturity Model for the Enterprise Knowledge Graphd (EKG/MM). The CMMI is a \nmodel that focuses on matur ing the software development process in organisation s. The EKG/MM is an \nindustry standard model definition of the capabilities that an  organisation  needs  for creating an  enterprise \nknowledge graph . The latter model focuses on  certain aspects of the adoption of AI's semantic computing \n                                                           \nc CMMI:  https://cmmiinstitute.com/cmmi  \nd EKG/MM:  https://www.ekgf.org/maturitymodel   \napproach ; however , the AI-CAM  focuses on AI comprehension and adoption in a broader sense , including  \nperceptual , sema ntic, and cognitive computing.  \n3.1 AI-Cap"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": ", sema ntic, and cognitive computing.  \n3.1 AI-Capability Assessment Levels  \nEach AI-Capability Assessment Level (Table 1) defines the capability requirements that an organisation  \nshould fulfil to achieve its AI comprehension and adoption objectives. It d escribe s five levels of \ndevelopment in data management aspects, business readiness, the AI knowledge and skills organisational \nactors  should possess, along with the ethic al considerations and risks associated with  AI solutions.  Based \non our review of the literature and experience, the AI-CAM  levels are as follows:  \nLevel 1 : Organisation s operating at this level possess basic capabilities to  implement entry -level  AI \nsolutions . Data governance and management capabilities are immature  and poor: Much time and effort \nare "
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "are immature  and poor: Much time and effort \nare expended on data cleansing prior to analysis. S mall- scale  pilot projects will be underway ; however, \nthey may not be aligned with business strategy goals  and objectives . There  may also be a lack of exemplars \nor case studies  for decision -maker s to draw on . The  required knowledge and skill s for the compr ehension \nand adoption of AI technologies  may not exist. Organisation s operating at this level need to begin building \nor acquiring  AI knowledge and skills n ecessary for desired business use cases related to their  business \nmodel and competitive strategies. I t is also important for organisation s to begin evaluat ing the ethic al, \nregulatory  and social responsibilities along with risks —reputational, regulatory, operatio"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "ong with risks —reputational, regulatory, operational risks  and so \non. \nLevel 2 : Organisation s operating at this level  possess the capabilities to  identify  the contribution of AI \ntechnologies  to achieve business objectives. The organisation  has an ad hoc approach to  data \nmanagement, governance, architecture, technology and quality on the pilots : Data must  be manually \ncleaned in some cases . An AI-technology p ilot is fully deployed  in at least one area of the organisation . \nThe ROI for the piloted AI solution  is measured to quantify benefits  in terms of potential cost reduction , \nincrease in profits or market share. The strategic importance of AI -based  solution s is also identified.  AI \nknowledge and skills acquisition by technical staff involved in pilots  are at ex"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "n by technical staff involved in pilots  are at expert levels . The ethic al \nconsequence s and organisation al and business risks of the piloted AI solutions are being carefully \nevaluated : Data pro tection  measures in pilot areas are managed well.  \nLevel 3 : Organisation s operating at this level  will have completed a number of AI -based pilot projects and \nare brining one or more pilots to scale.  The organisation  will have achieved and manages  the requisite \ndata management capabilities at the enterprise level across all relevant categories.  They will be engaging \nin the  quantitative evaluat ion of  a range of  AI-based  solution s in terms of competitive market share, ROI, \ncost/benefit, and operation al efficiency and effectiveness. Business and IT managers acknowledge that \n"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "eness. Business and IT managers acknowledge that \ndata management capabilities are immature and have  developed an integrated strategy for data \nmanagement, data governance, data architecture, data technology and data quality dimensi ons. Key \norganisation al staff in strategic and operational business areas are receiving education in AI technologies \nto enable them to identify, assess and manage opportunities to employ these technologies to \noperationalise or support the organisation 's business model and achieve strategic objectives . Policies and \nplan s to address the ethic al issue s and organisation al risks (including data protection ) are also in place.  \nLevel 4 : Organisation s operating at this level will have one or more AI-based applications  in operation. \nRequisite data mana"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "d applications  in operation. \nRequisite data management capabilities are achieved in the target business use case area(s) and defined \nfor the organisation  as a whole. There is a move from siloed structured and unstructured data to a \nfederated and integrated a rchitecture, initially with virtualisation an objective enabled through semantic \ncomputing' s knowledge representation. AI- based applications ' ROI, cost/benefit, operational efficiency , \neffectiveness,  and strategic impact are measured and known. The organis ation  will be evaluating new \nbusiness case s, both strategic and operational. Managers and staff in core business areas  will have \nachieved  high  levels of knowledge  and skills on the organisation 's existing AI applications . The \norganisation  will have a plan for "
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "cations . The \norganisation  will have a plan for knowledge and skills  acquisition  in novel or untried AI technologies . \nPolicies and plans to address the ethic al issue s and organisation al risks (inc luding data protection) are in \nplace and continuously updated in light of novel AI technologies .  \nLevel 5: Organisation s operating at this level will have several AI -based application(s) in operation across \nbusiness units and business line s. Organisation s are proficient in measuring and assessing the ROI, \ncost/benefit, operational efficiency , effectiveness,  and strategic impact of AI technologies. The \norganisation  continuously evaluates  new business cases, both strategic and operational , seeking \nopportunities to leverage its AI knowledge and skills across these areas . Ma"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "ts AI knowledge and skills across these areas . Managers and staff in core \nbusiness areas  will have achieved high levels of knowledge and skills on the organisation 's existing AI \napplications . They  will be actively transferring these capabilities to other areas . The organisation  will \ncontinue to acquire  new knowledge and skills in novel or untried AI technologies. Policies to address the \nethic al issue s and organisation al risks (inc luding data protection) are continuously reviewed and updated \nto maintain compliance with existing regulations.  \n \nTable  1: AI -Capability Ass essment Levels  \nAI-CAM  Level  Criteria  \nLevel 5: Quantitatively \nManaged  The organisation  is/has : \n• Socialised and managed its organising vision for AI to provide the framework for \ninterpreting AI"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "r AI to provide the framework for \ninterpreting AI, legitimising the adoption and use of AI technologies, and \nmobilising  different communities from IT to business to innovate using it.  \n• Achieved and is managing the requisite data management capabilities at the enterprise level.  \n• Enabling or supporting core business processes using AI -based applications.  \n• Proficient in measuring and assessing the ROI, cost/benefit, operational efficiency \nand effectiveness and strategic impact of AI technologies.  \n• Identif ying additional busines s case s to apply AI technologies.   \n• Routinely a cquiring and transferring new and existing AI -related knowledge and \nskills throughout the organisation . \n• Continuously reviewing policies to address the ethical issues and organisation al \nrisks "
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "ess the ethical issues and organisation al \nrisks to maintain compliance with existing regulations.  \n• Maintaining and managing compliance with data protection regulation s. \nLevel 4: Defined  The organisation  is/has : \n• Implementing a strategic plan for AI -enabled applications  and defining its \norganising vision . \n• Operating AI -based application(s) to enable or support a core business process or \nactivity.  \n• Achieved and is managing the requisite data management capabilities in the target \nbusiness use case area (s). \n• Measuring and evaluating ROI, cost/benefit, operational efficiency and effectiveness and strategic impact of AI -based technologies.   \n• Evaluating new business cases, both strategic and operational.  \n• Achieving and transferring high levels of knowledge and sk"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "g and transferring high levels of knowledge and skills on the organisation 's \nexisting AI applications.  \n• Planning for knowledge and skills acquisition in novel or untried AI technologies.  \n• Maintaining and continuously updating policies and plans to address the ethical \nissues and organisation al risks (inc luding  data protection).  \nLevel 3: Strategic  The organis ation  is/has:  \n• Developed  a strategic plan and organised  vision for AI -enabled applications to \ndigitally transform core business operations and processes.   \n• Recognised that it does not have the requisite data management capabilities and \nhas developed a strategy for data management, data governance, data \narchitecture, data technology and data quality dimensions.  \n• Completed several AI -based pilot projects an"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "  \n• Completed several AI -based pilot projects and is bring ing one or more pilots to \nscale . \n• Engaging in the quantitative evaluation of a range of AI -based solutions in terms of \ncompetitive market share, ROI, cost/benefit, and operational efficiency and \neffectiveness.   \n• Capable of evaluating the capabilities of third- party AI technology vendors.  \n• Educating key organisation al staff in strategic and operatio nal business areas in AI \ntechnologies to enable them to identify, assess and manage opportunities to employ these technologies to operationalise or support the organisation 's business \nmodel and achieve strategic objectives.  \n• Drafting policies and plans to address the ethical issues and organisation al risks \n(including  data protection) relating to the strategic us"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "ing  data protection) relating to the strategic use of AI . \nLevel 2: R &D The organisation  is/has:  \n• Underst anding of  the business and competitive potential of the AI solutions  to \ninformate and automate core business processes.   \n• An ad hoc approach to data management, governance, architecture, technology \nand quality on the pilots.  \n• Implemented b usiness use case p ilots  for AI solutions  in at  least one area of the \norganisation .  \n• Evaluated AI's potent ial to increase competitive market share , and the \norganisation can measure ROI, cost/benefit and operational efficiency and \neffectiveness.   \n• Acquiring and building AI knowledge and skills through AI pilots  and other sources .  \n• Evaluated ethic al and AI -related risks of pilot s, considering the impact on humans"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "risks of pilot s, considering the impact on humans , \ndata protection and regulat ory compliance.  \nLevel 1: Initial  The organisation  is/has:  \n• Poor or basic  understanding of the business and competitive potential of the AI \nsolutions.  \n• Experimented and/or adopted basic  AI technologies  in, for example , areas such as \ndata analytics . \n• Poor data governance across the organisation , which operates a siloed data \narchitecture  and where manual data cleansing is the norm.  \n• Poor alignment of AI -based solutions with  business goals and strategic objectives.  \n• Engaged in k nowledge acquisition for  understanding  and envision ing the potential \nof AI -based business  solutions.   \n• Considered general ethic al issues  and risks of AI solutions .  \n \n4 AI Capabilities Matrix (AI"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "of AI solutions .  \n \n4 AI Capabilities Matrix (AI -CM) \nThe proposed AI Capabilities Matrix ( Table 4) describes the AI knowledge and skills organisation al actors \nshould possess  at different levels  of AI adoption . The matrix  aims to provide a general view of the AI \nknowledge and skills landscape across an organisation and complement  the resources that decision -\nmakers can use in their path to AI adoption. The capabilities are matched to types of role s within an \norganisation , grouped according to three levels of AI proficiency, as defined in Table 2.  \nEach  AI proficiency level builds on the knowledge and skills required at lower levels for each type of \norganisation al actor. For example, the knowledge and skills of chief executive officers (CEOs)  and chief \ntechnology offic"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "utive officers (CEOs)  and chief \ntechnology officers (CTOs), who decide, plan, and drive an AI solution, differ from the knowledge and skills \nrequired from those who implement AI solutions  (for instance, project leaders, systems administration \nleaders, system architects, computer and data scientists, knowledge and data engineers, and \nprogrammers ), as well as from those who participate  in other ways in AI projects , such as subject matter \nexperts . \nTable 2: AI Proficiency Levels  \nAI Proficiency \nLevel  Description  Examples of Roles34-40  \nBasic  Basic level of AI proficiency for users within \nthe organisation  who interact with AI  Subject -matter expert, trader, administration staff, factory \nworker, driver, quality control inspector, etc.  \nAdvanced  Advanced level of AI profic"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "ctor, etc.  \nAdvanced  Advanced level of AI proficiency for \ndecision -makers who design the \norganisation 's AI strategy  CIO, Head of Compliance, CMO, Head of eCommerce, director, \nsupervisor, senior -management personnel, executive, general \nmanager, etc.  \nExpert  Expert level of AI proficiency for technical \nstaff who implement the AI technology  Data scientist, data analyst, machine learning engineer, deep \nlearning engineer, machine learning researcher, deep learning \nresearcher, software engineer, predictive mode ller, corporate \nanalytics manager, information strategy manager, \ncomputational linguist, computer vision engineer, knowledge \nengineer, semantic -web consultant, ontologist, ontology \nmanager, applied science manage r, ontology expert, \nknowledge engineering manager, etc"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "tology expert, \nknowledge engineering manager, etc.  \n \nTable 3: AI Capability Assessment Model \nAI Capability Assessment Model (AI -CAM): Core Areas  \n             Core Areas  \nLevels  Business  Data  Technology  Organi sation  AI Knowledge and \nSkillse Ethics  and Risks  \nLevel 5: \nQuantitatively \nManaged  -An organising vision is socialised \nand managed to provide the \nframework for interpreting AI, \nlegitimising the adoption and \nuse of AI technologies, and \nmobilising  different communities \nfrom IT to business to innovate \nusing it.  \n-The organisation continuously \nevaluates, measures, and \nassesses the ROI, cost/benefit, \noperational efficiency and \neffectiveness , and strategic \nimpact of AI technologies.  \n-Additional business use cases \nare considered in alignment with \nbusiness"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": " cases \nare considered in alignment with \nbusiness model and strategy . \n -The following data \nmanagement (DM) \ncapabilities are fu lly in place:  \n1. DM Strategy  \n2. DM Business Case and Funding  \n3. DM Program  \n4. Data Governance  \n5. Data Architecture  \n6. Data Technology \n7. Data Quality Program  \n-Data is identified, defined \nand governed .  \n-An enterprise data model \nexists.  \n-Data collection s and silos are \nfederated, integrated and \nvirtualised , and completely \nmanaged.  -Methodology and m etrics for \nmeasuring the performance, \nefficiency and effectiveness \nof AI techn ology are \ndeveloped and  applied.  \n-Technological innovation \nusing  AI solution s are \nassessed and managed  in line \nwith  business metrics.  \n-Technologies from the \nperceptual, semantic or \ncognitive com"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "s from the \nperceptual, semantic or \ncognitive computing paradigms are integrated to \nleverage the full power of AI.  -Senior management make \ninformed decisions at the \nboard level.  \n-Business and technical \npersonnel have the \ncompetencies to provide an \nAI-based solution to internal \nfunctions and external  clients . \n-The IT function has internal \ncompetencies and external \nlinks to acquire or plan, \ndesign, develop and deploy \nnovel AI solutions.  -The organisation  possesses \ncomprehensive knowledge \nand skills in all AI computing \ntechnologies:  \n1. Perceptual  \n2. Semantic  \n3. Cognitive   \n-AI knowledge and s kills are \nevidenced  to varying degrees \nacross the organisation . -Ethics policies to address \nthe ethical issues and \norganisation al risks to \nmaintain compliance with \n"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "ganisation al risks to \nmaintain compliance with \nexisting regulations are \ncontinuously monitored.  \n-Compliance with data \nprotection regulations is \nmaintained and managed \nthrough policies and \ncontrols.  \n \nLevel 4: Defined  -The organisation  has deployed \nan AI -based application(s) to \nenable or support a core \nbusiness process or activity.  \n-It is implementing its  strategic \nplan and has defined its \norganising vision to deploy AI -\nenabled solutions in key business \nprocesses . \n-It define s how to measure and \nevaluate ROI, cost/benefit, \noperational efficiency and \neffectiveness , and strategic \nimpact of AI -based technologies.   \n-It is evaluating new business \ncases, both strategic and \noperational . -Data management (DM) \ncapabilities are defined, with a DM S trategy unde"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "capabilities are defined, with a DM S trategy under \nimplementation, as is the DM \nBusiness Case and F unding.  \n-The DM Program is also \nbeing implemented . \n-The following capabilities \nand structures are fully \ndefined for the initial AI \ndeployment, but not across \nthe organisation :  \n• Data Governance  \n• Data Architecture  \n• Data Technology \n• Data Quality Program  -AI technolog ies are deployed \nin production  to support a \nbusiness use case . \n-Perceptual and semantic \ncomputing paradigms are \nintegrated.   \n \n -Senior management has \ndefined the need for AI at the board level.  \n-The business team are \nleveraging the power of AI to \ninform operational decisions \nand actions.  \n-The technical team is \nadministering the AI solution in production.  -The business team in the \nareas "
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": " in production.  -The business team in the \nareas where AI is deplo yed \nhas the requisite capabilities \n(knowledge and skills) and \nsocialises  the defined skillset \nacross the organisation .  \n-The technical team has the \nrequired AI knowledge and \nskills for deploying and \nmanaging the  selected AI \nsolution.  \n -Ethics policies to address \nthe ethical issues and \norganisation al risks to \nmaintain compliance with \nexisting regulations are \ndefined.  \n-Compliance with data \nprotection regulations is achieved through policies \nand controls in target \noperational areas.  \n \n                                                           \ne See also the AI Capabilities Matrix ( Table 4 ) for more detailed descriptions.  \nAI Capability Assessment Model (AI -CAM): Core Areas  \n             Core A"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": " Model (AI -CAM): Core Areas  \n             Core Areas  \nLevels  Business  Data  Technology  Organi sation  AI Knowledge and \nSkillse Ethics  and Risks  \nLevel 3: Strategic  -The organisation  has d eveloped \na strategic plan and organising \nvision for AI -enabled \napplications to digitally transform core business \noperations and processes.   \n-It has c ompleted several AI -\nbased pilot projects and is \nbringing one or m ore pilots to \nscale in an operational context.  \n-It is e ngaging in the quantitative \nassessment of measures to \nevaluate ROI, cost/benefit, \noperational efficiency and \neffectiveness , and strategic \nimpact of AI -based technologies.  -Data management (DM) \ncapabili ties are the subject of \na DM S trategy , which is \nunder development, as is the \nDM Business Case and \nF"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "er development, as is the \nDM Business Case and \nFunding.  \n-The DM Program is also \nunder development, but the \nfollowing capabilities and \nstructures are not fully \ndefined for the initial AI \ndeployment  except for pilot \nprojects :  \n• Data Governance  \n• Data Architecture  \n• Data Technology \n• Data Quality Program  -An AI solution selected for \nits strategic impact is \nimplemented and in \nproduction in an operational \ncontext.  \n-The AI technology solution is \nbased on either the perceptual or semantic \nparadigm.   \n \n -Senior management has the \nrequisite knowledge and skills to develop a business \nstrategy around AI.   \n-The business team are \nenacting AI's capabilities to \ninform operational decisions and actions.  \n-The technical team has \nincluded business areas to \nimplement th"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "team has \nincluded business areas to \nimplement the selected AI \nsolution  in an Agile project \nenvironment . -The business team in the \nareas where AI is deployed \ncontinue to develop the \nrequisite capabilities (knowledge and skills) from \nthe learnings and outcomes \nof the pilot projects and \napplying these in operational \ncontexts.  \n-The technical team \ncontinues to develop and \nhone the required AI skills to \ndevelop the selected AI \nsolution or evaluate  \ncontractors.  \n -The strategic impact of \nethical issues and v arious \norganisation al risks are \nconsidered.  \n-The implications for \ncompliance with data protection regulations are \nexplored , and policies and \ncontrols in target operational \nareas prepared.  \n \nLevel 2: R&D  -The organisation  understands \nthe business and compe"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": " organisation  understands \nthe business and competitive \npotential of AI solutions to \ninformate and automate core \nbusiness processes.  \n-Business use case pilots for AI \nsolutions have been \nimplemented in at least one area \nof the organis ation .  -Data management (DM) \ncapabili ties are recognised as \nnot being sufficient .  \n-The required DM capabilities \nand structures to enable \nmature AI deployment are \nnot in place, except in the \npiloted area.   \n -The infrastructure for the AI \nsolution implementation is set up.  \n-The technology architecture \nfor developing the AI solution is set up.  \n-Otherwise, if not internally \ndeveloped, the evaluation of \ncontractors for external \ndevelopment is in place.  \n -The technical team is \ncreated for performing the \npilot with the selected AI "
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "ed for performing the \npilot with the selected AI \nsolution.  \n-The project management \nmethodology is set  up.  -The business team in the \npilot areas engages in \nsensemaking on the \nnecessary capabilities \n(knowledge and skills) \nrequired for the pilot's \nsuccess .  \n-The technical team receives \non-the-job training from \nconsultant experts  to acquire \nthe required AI skills for \ncompleting  the pilot.  -The impact of ethical issues \nand various organisation al \nrisks are considered  in the \ncontext of the pilot . \n-The implications for \ncompliance with data protection r egulations are \nexplored , and policies and \ncontrols in target operational areas prepared.  \n \nAI Capability Assessment Model (AI -CAM): Core Areas  \n             Core Areas  \nLevels  Business  Data  Technology  Organi"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "Areas  \nLevels  Business  Data  Technology  Organi sation  AI Knowledge and \nSkillse Ethics  and Risks  \nLevel 1: Initial  -The organisation  has a basic \nunderstanding of the business \nand competitive potential of AI \nsolutions.  \n-It has e xperimented and/or \nadopted basic AI technologies  \n(e.g., in data an alytics).  \n-AI-based isolutions are poorly \naligned with business goals and \nstrategic objectives.  \n-The organisation  is engag ed in \nknowledge acquisition for \nunderstanding and envisioning \nthe potential of AI -based \nbusiness soluti ons.  -Requirements for data \ncollection infrastructure and \ndata quality are identified for \na potential AI solution.  \n- Data management \ncapabilities are poor and \nrequire significant \ndevelopment.  -The infrastructure for \nadopting an AI solutio"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": ".  -The infrastructure for \nadopting an AI solution is \nlimited or absent . \n-Ad hoc development of \ntechnical requirements \nawareness for potential AI \nsolution implementation.  -A business and technology \nteam are created to begin \ncomprehension of AI \ntechnologies and business \nbenefits and evaluate the \nbusiness c ases for adopting  \nan AI solution.  \n \n -The business team have a \nrudimentary knowledge of AI \nand some skills in data \nanalytics.  \n-AI basics/general skills \nacquisition is underway in \nthe IT function .  \n-Internal and external search \nis in place to locate the \nexpertise and required AI \nknowledge based on profiles.  \n -The strategic impact of \nethical issues and various \norganisation al risks are \nunknown.  \n-The implications for \ncompliance with data \nprotection regul"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "ations for \ncompliance with data \nprotection regulations are \nunknown and unconsidered.  \n \nThe contents of the AI Capabilities Matrix  are derived from a synthesis  of the capabilities (knowledge and \nskills ) identified in specialist professional reports ;41-45 professional and academic course books, \ndescriptions , and syllabi;46-49 academic publications ;50-54 governmental webpages ;55,56 job advertisements ; \nand the authors ' experience.  \nThe main contributions of this matrix are that: (1) it includes knowledge and skills in all the areas identified \nin Section  2 as being part of AI technologies  (i.e., perceptual, semantic and cognitive computing \ntechnologies ); and ( 2) it is cross -functional  as its contents a re not exclusively concerned with the technical \nskills for experts"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "y concerned with the technical \nskills for experts or the skills involved in AI adoption at the managerial level, but encompass AI skills \nspecifications for all actors involved in AI adoption across an organisation ; and ( 3) it is linked to an AI \ncapabilities  assessment model , the proposed AI -CAM .  \nThe skillsets described in the matrix are not meant to be exhaustive and could indeed be further  extended, \nspecified , and linked to specific tasks and tools that would need to be mastered to perform these tasks . \nHowever, such  specifications would be too context- specific  and go beyond this general guide  to assist in \nAI adoption . Note also that the technical skills at the Expert level are not tied to specific roles, such as data \nscientist or mac hine learning engineer, as they "
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "\nscientist or mac hine learning engineer, as they can  (in principle)  be acquired irrespective of one's original \ntechnical expertise or role within the organisation . Finally, soft skills (e.g., communication, interpersonal \nskills , time management, independence) and business -level competencies (e.g., understanding business \nrequirements) often  required at the Expert level are omitted from the matrix.  \nThe first column , \"Capabilities and Technical Areas \", lists general competencies (see points A. to E.) with \ntheir description. M ore specific competencies and technical areas relevant to AI are listed under the \ngeneral competencies (see numbered points). The next three columns specify the skillsets corresponding \nto each AI proficiency level (Basic, Advanced and Expert). In most ca"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "ncy level (Basic, Advanced and Expert). In most cases, it is assumed that the skillsets on \nthe left -side columns also apply to the columns on the right, meaning that someone at the Expert level \nalso possesses the skills at the Basic level. The last column links the  skillsets to the AI -CAM levels.  \n \nTable 4: AI Capabilities  Matrix  \nCapabilities and \nTechnical Areas  AI Proficiency Level  AI-CAM Level (L)  Basic  Advanced  Expert  \nA. General \nknowledge of AI  Possessing general knowledge of the main questions underlying AI as a discipline and its technologies .  \n a. Understand ing the differences between Artificial Intelligence and human or animal intelligence.  \nb. Recognising artefacts that use AI and distinguishing them from those that  do not.  \nc. Recognising that AI encompas"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "se that  do not.  \nc. Recognising that AI encompasses various areas of activity that make AI technology possible (machine learning, data science, \nknowledge representation, natural language processing, computer vision, etc.).  \nd. Recognising that developing AI technology in volves humans with technical expertise to program and calibrate the AI systems, \nincluding supporting technologies.  L1: \n• Advanced  \n• Expert  \nL3: \n• Basic  \nB. AI Applications  Identifying the ways in which AI can be used within the organisation  (at individual job and overall organisation al levels), its advantages \nand limits.   \n1. AI's strengths \nand weaknesses  a. Identifying the technologies \nthat are involved in the relevant AI case studies, and \ntheir weaknesses and \nstrengths.  a. Identifying tasks and are"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "sses and \nstrengths.  a. Identifying tasks and areas within \nthe organisation  that could benefit \nfrom AI, where human skills are \nmore advantageous, and where \nhuman- AI interaction might work \nbest.  \nb. Identifying the AI adoption time \nand costs for the relevant case \nstudies.  a. Identifying the AI technology and algorithms most \nsuitable for addressing the task.  \nb. Identifying the weaknesses/strengths of the \nselected AI technologies.  L1: \n• Advanced  \nL3: \n• Basic  \n• Expert  \n2. AI capabilities \nand scope  a. Identifying the AI scenarios \nthat may impact the nature of \nthe work and if this  requires  \ntraining in new areas.  \nb. Understanding that some \ntasks can be automated with \nAI, fully or partially, with some \nhuman interaction.  a. Understanding what AI can do, to \ndevel"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "ction.  a. Understanding what AI can do, to \ndevelop business applications with \nrealistic expectations (e.g., an AI \nchatbot can emulate a human \ndialogue but with limits).  \nb. Identifying which AI sub- fields \napply to the organisation 's \nactivities/AI projects: machine \nlearning, natural language \nprocessing (NLP), expert systems, \ncomputer vision, speech, planning, \nand robotics.  a. Advising on the technical feasibility of AI business \nproposals and/or suggesting new ideas.  \nb. Identifying the capabilities of the selected AI \ntechnologies.  L1: \n• Advanced  \n  \nL3: \n• Basic  \n• Expert  \nCapabilities and \nTechnical Areas  AI Proficiency Level  AI-CAM Level (L)  Basic  Advanced  Expert  \nC. Technical skills \nfor AI  Possessing different levels of technical knowledge and skills to par"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "nt levels of technical knowledge and skills to participate in the adoption of AI within an organisation .  \nPerceptual computing   \n1. Machine \nlearning (ML) \ncapabilities  a. Being aware that AI systems \nare programmed to learn \nautomatically  (e.g., to filter \nspam email, identify images and consumption patterns, \netc.).  a. Identifying the different algorithm \ntypes and their main usage in \npractical applications.  \nb. Knowing that ML systems learn \nfrom data. \nc. Explaining ML in non- technical \nterms.  \nd. Assessin g the relevance of ML \nmethods to the organisation 's \napplications.  a. Understanding and implementing ML algorithms: \nfeature engineering, error metrics selection and \nanalysis. \nb. Using diagnostic tests and interpreting their \noutputs (e.g., learning curves) to gain ins"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "their \noutputs (e.g., learning curves) to gain insight into \nthe performance and improve it.  \nc. Mastering the mathematical knowledge for ML \n(e.g., linear algebra, multivariate calculus).  \nd. Applying ML to one or more AI appl ication areas \n(e.g., NLP, computer vision) and possessing the necessary disciplinary knowledge in the underlying \ndomains (e.g., linguistics, sensors) . L1: \n• Advanced  \n \n \nL2: \n• Basic  \n• Expert  \nSemantic computing   \n2. Knowledge \nrepresentation \n(KR) capabilities  a. Understanding the concept of \nKR and its main practical \nusages.  \nb. Identifying how subject matter \nexperts ( SMEs ) support KR.  \nc. Interacting with technical staff to develop KR models.  a. Knowing the concept of KR and the \ndifferent KR models (e.g., taxonomies, ontologies, \nknowledge gr"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "odels (e.g., taxonomies, ontologies, \nknowledge graphs, etc.).  \nb. Identifying the semantic applications of KR in relevant \ndomains (e.g., health -care, finance, \nenergy, etc.).  \nc. Identifying that KR plays an \nimportant role in data \nharmonisation and integration in \nan organisation . \nd. Identifying and ensuring that SMEs \nfully support the technical staff in \nKR. a. Understanding requirements for KR modelling, \ndomain analysis and conceptual analysis.  \nb. Interacting with SMEs to develop KR models.  \nc. Using KR languages (e.g., OWL, RDF, UML), \nformats (e.g., RDF/XML,  Turtle, N -Triples) and \nquery languages (e.g., SPARQL) . \nd. Evaluating NLP techniques for performing KR -\nrelated tasks (e.g., ontology learning and \npopulation, entity linking, information extraction, \netc.).  \ne."
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "tity linking, information extraction, \netc.).  \ne. Knowing logics and the logical properties of KR languages . \nf. Using reasoners for consistency checking, \nclassification and deductive inference.  L1: \n• Advanced  \n \nL2: \n• Basic  \n• Expert  \nCapabilities and \nTechnical Areas  AI Proficiency Level  AI-CAM Level (L)  Basic  Advanced  Expert  \nCognitive computing   \n3. Deep learning \n(DL) capabilities  a. Being aware that DL is a sub -\narea of ML.  \nb. Identifying practical \napplications of DL, for \nexample, in voice recognition \n(e.g., Siri, Alexa), in \nautonomous driving (e.g., self -\ndriving cars), etc.  \n a. Being aware that DL systems are \nblack boxes, which might present \nrisks for given applications (e.g., \nhealth- care diagnostic systems).  \nb. Knowing that DL systems learn \nfrom d"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "stems).  \nb. Knowing that DL systems learn \nfrom data. \nc. Identifying potential DL -based \nsolutions for the organisation . \n a. Understanding and using neural network \narchitectures (e.g., fully connected networks, CNNs, RNNs), methods for training and error \nanalysis. \nb. Selecting evaluation metrics, and diagnostic \ntesting for interpreting the outputs.  \nc. Possessing the necessary mathematical knowledge.  \nd. Assessing the relevance of DL methods to the \norganisation 's applications.  \ne. Applying DL to one or  more AI application areas \n(e.g., NLP, computer vision) and possessing the \nnecessary disciplinary knowledge in the underlying \ndomains (e.g., linguistics, sensors) . L1: \n• Advanced  \n L2: \n• Basic  \n• Expert  \nSupporting Capabilities  \n4. Data engineering \ncapabilities  a. S"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "bilities  \n4. Data engineering \ncapabilities  a. Supporting the data engineers \nin collecting and annotating \nthe data in case studies.  \nb. Interacting with technical staff to define annotation schemas \nand annotate data.  a. Knowing that AI systems use data, \nand the difference between \nstructured and unstructured data.  \nb. Recognising the importance of data quality to train and test AI \nsystems, and of data \nharmonisation and integration.  \nc. Recognising that data may be biased or contain private \ninformation and handling it.  \nd. Ensuring SMEs ' support in the \ndata -production processes.  \ne. Identifying that data is a valuable \nand monetisable asset and brings  \ncompetitive advantages to the organisation . \nf. Integrating data protection plans \ninto the AI strategy.  a. Finding or "
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "ction plans \ninto the AI strategy.  a. Finding or compiling datasets from internal or \nexternal sources.  \nb. Evaluating and a ssessing data sources.  \nc. Exploring, cleaning, transforming, integrating, debiasing, and anonymising data . \nd. Interacting with SMEs to develop data annotation projects and define annotation schemas.  \ne. Setting up an annotation platform for SMEs . \nf. Possessing solid scientific foundations and \nstatistical skills.  \ng. Using data management tools with appropriate access and security levels.  L1: \n• Advanced  \n L2: \n• Basic  \n• Expert  \nCapabilities and \nTechnical Areas  AI Proficiency Level  AI-CAM Level (L)  Basic  Advanced  Expert  \n5. Software \nengineering \ncapabilities   a. Identifying potential software \napplications, which use AI technologies to facilit"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "applications, which use AI technologies to facilitate the AI \ndelivery to end- users and \ncustomers.  \nb. Including the software \napplications in the AI strategy.  \nc. Planning the software \ndevelopment capability and \nmaturity models for ensuring \nquality software applications.  \n a. Managing the whole  development life cycle and \nmodels, from requirements elicitation to \ndeployment in production.  \nb. Integrating the AI technologies in the software applications.  \nc. Mastering programming languages for internet, mobile and desktop applications.  \nd. Handling software development methods, both agile and traditional.  \ne. Managing software implementation in different \nparadigms, such as object -oriented, aspect \nprogramming, and internet protocols (e.g., HTML, \nXML, HTTP request).  \nf. Pre"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "otocols (e.g., HTML, \nXML, HTTP request).  \nf. Preparing software documentation.  L1: \n• Advanced  \n \nL2: \n• Expert  \n6. Configuration \nmanagement \n(versioning)   a. Including the configuration \nmanagement plan in the AI solution for securing the code of \nthe organisation 's software.  a. Setting  up and managing the configuration \nmanagement for collaborative team  development \nof the AI solution.  L1: \n• Advanced  \nL2: \n• Expert  \n7. Continuous \nintegration    a. Setting  up and managing a continuous integration \nplatform for delivering the AI solution for \ndeployment in an ambitious frequency (even daily).  L2: \n• Expert  \n8. Cloud computing   a. Identifying and  planning a cloud \nplatform for supporting the AI solution development.  a. Selecting a cloud platform for boosting the storag"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "Selecting a cloud platform for boosting the storage \nand processing capabilities of the organisation  for \ninnovative AI solutions.  \nb. Setting  up and managing the AI solution in a cloud \nplatform for providing collaborative development.  L1: \n• Advanced  \nL2: \n• Expert  \n9. Virtual server \ncomputing   a. Identifying and evaluating if a \nvirtual server strategy is applicable \nor acquiring the required \nhardware for the AI solution.  a. Selecting a virtual server platform vendor or \nproprietary  solution to boost the organisation's \nstorage and processing capabilities for innovative AI \nsolutions.  \nb. Setting  up and managing the AI solution in a virtual \nserver configuration.  L1: \n• Advanced  \n L3: \n• Expert  \nCapabilities and \nTechnical Areas  AI Proficiency Level  AI-CAM Level (L)  B"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "l Areas  AI Proficiency Level  AI-CAM Level (L)  Basic  Advanced  Expert  \n10. System \nadministration   a. Planning the system \nadministration strategy and staff \nfor administering the AI solution \nin production.  a. Including the AI solution into the system \nadministration routine tasks for production.  \nb. Knowing the required processing, storage and installation softwar e for setting up the AI solution in \nproduction.  \nc. Monitoring the proper functionality of the \nproductive AI solution.  L1: \n• Advanced  \n \nL3: \n• Expert  \nD. Ethical Issues \nand Risks of AI  Identifying societal and ethical implications of AI technology and their potential risks for the organisation .  \n1. Capabilities  \nregarding ethical \nissues and other risks  a. Being aware that AI raises \nsocietal and ethical is"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "eing aware that AI raises \nsocietal and ethical issues.  \nb. Understanding that AI technologies and solutions \nwon 't necessarily make an \nemployee redundant.  \nc. Identifying how AI can help \nthem in their  daily work.  a. Understanding the societal and ethical questions raised by AI and taking appropriate (business \nor technical ) mitigation measures: privacy issues, surveillance concerns, employment issues \n(e.g., job destruction), misinformation (e.g., fake news/video generation), singularity/concern about harm to people.  \nb. Considering ethical issues raised by decisions automation with AI, e.g., singularity/concern about harm to people.  \nc. Recognising system development biases resulting from lack of workforce diversity and from biases in the data.  \nd. Recognising risks inherent t"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "ses in the data.  \nd. Recognising risks inherent to black -box systems and the possible need for transparent sys tems \n(explainable AI) to ensure accountability.  L1: \n• Advanced  \n• Expert  \nL3: \n• Basic  \ne. Developing plans to manage \nethical risks and data protection . e. Deploying technical solutions to address ethical issues \nand mitigate risks . \n5 Discussion and Conclusions  \nThis paper 's contribution is  a 5-level AI Capability Assessment Model ( AI-CAM ) and a related AI Capabilities  \nMatrix  (AI-CM) which are tools aimed at assisting organisation s in AI comprehension and adoption. These \ntools were devel oped with executives and leading stakeholders in mind. They are based on a wider \ndefinition of AI than usually accounted for in AI adoption  models and thus include requirem"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "r in AI adoption  models and thus include requirements not only \nregarding data governance and management issues, and the use of machin e learning technologies, but \nalso semantic technologies based on knowledge representation resources. These models encompass  \ntechnologies for emulating human -like reasoning . Therefore, these tools are particularly suited for \nseeking to automate tasks involv ing, for example, querying and reasoning over very large amounts of \ntextual data.  \nThe AI-CAM  covers the core areas to be developed to achieve different maturity levels in AI capabilities: \nbusiness, data, technology, organisation , AI skills, business and operational risks , and ethical questions.  \nTherefore, the AI -CAM provides a valuable resource to organisations to help assess organisation"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "ource to organisations to help assess organisational AI \ncapability maturity and  determine if it can achiev e its strategic and operational objectives using A I. The \nmodel specifies the required capabilities required to achieve desired organisation al outcomes. The higher \nthe level of capabilities demonstrated across the various dimensions, the greater the potential for success \nin AI adoption and innovation. An o rganisation  can therefore determine if a capabilities gap exists and use \nthe model and the related AI-CM to close the gap in line with achieving the perceived strategic and \noperational benefits of AI adoption . \nThe AI-CM addresses the knowledge  and skills n eeded to reach each level of the AI-CAM . The AI -CM aims \nat filling the functional, disciplinary and modelling gap"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "ing the functional, disciplinary and modelling gaps  identified in our literature review  by (1) specifying \nAI capabilities and skills at different levels of AI proficiency, (2) in all disciplinary and technical areas \ncovered by AI adoption, including KR, and by (3) linking the capabilities and skills to different stages of AI \nadoption within a n organisation .  \nIt is inspired by the AI literacy  framework ,57 which identifies the general AI capabilit ies that everyone \nought to possess. The authors of this framework \"define AI literacy  as a set of competencies that enables \nindividuals to critically evaluate AI technologies; communicate and collaborate effectively with AI; and use \nAI as a tool online, at  home, and in the workplace \".58 It thus fits the whole range of professionals "
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": ".58 It thus fits the whole range of professionals involved \nin an organisation 's AI adoption path. It contributes to existing proposals by introducing knowledge and \nskills requirements at all levels of an organisation  categorised  into three levels of AI proficiency: B asic level \nfor users who interact with AI and participate in AI adoption, A dvanced level for decision -makers who \ndesign AI strategies, and Expert level for technical staff who implement the AI technology.  \nWhile seeki ng to include a wide range of core skills based on recent professional and academic literature, \nthe AI Capabilities Matrix  is by no means a  systematic and  exhaustive  inventory of relevant skills. It is \nmeant to be extended, refined and regularly updated wi th more systematic reviews of sources in "
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "dated wi th more systematic reviews of sources in the areas \nconcerned. These tools would also require empirical validation, which we leave for future work.  \nIn conclusion, the AI -CAM and AI -CM offer an open -source resource for organisation s to help maximise \nthe benefits of AI adoption and use. T he models  may also be used to assess the capabilities of AI vendors, \nto identify potential AI -washing and close the gap between the promise and reality of AI in organisation s.  \nAcknowledgements  \nThis work was partially supported  by Enterprise Ireland and the Marie Skłodowska -Curie Actions through \nthe Career -FIT Fellowships MF20180003 and  MF20180009.  Career- FIT has received funding from the \nEuropean Union’s Horizon2020 research and innovation programme under the Marie Skł odowsk"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "nd innovation programme under the Marie Skł odowska -Curie \ngrant agreement No. 713654.  \nReferences  \n1. Kruse L, Wunderlich N, Beck R. Artificial Intelligence for the Financial Services Industry: What \nChallenges Organizations to Succeed. In 52nd Hawaii International Conference on System \nSciences; 2019; USA. p. 6408 -6417.  \n2. TechAdvisor Staff. How tech giants are investing in artificial Intelligence . 2019.  \n3. BAE Systems. Net Reveal. [Online]. 2021.  \n4. Amazon Web Services. Amazon Fraud Detector. [Online]. 2021.  \n5. Butler T, Abi -Lahoud E, Espinoza A. Designing Semantic Technologies for Regulatory Change \nManagement in the Financial Industry. In International Conference on Information Systems \n(ICIS); 2015: AIS. p. 1 -11. \n6. Kaur S, Singla J, Nkenyereye L, Jha S, Prashar D, Jo"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "ur S, Singla J, Nkenyereye L, Jha S, Prashar D, Joshi GP, et al. Medical Diagno stic Systems \nUsing Artificial Intelligence (AI) Algorithms: Principles and Perspectives. IEEE Access. 2020; 8: 228049- 228069.  \n7. Sobrinho A, Da S. Queiroz ACM, Dias Da Silva L, De Barros Costa E, Pinheiro ME, Perkusich A. \nComputer -Aided Diagnosis of Chron ic Kidney Disease in Developing Countries: A \nComparative Analysis of Machine Learning Techniques. IEEE Access. 2020; 8: 25407 -25419.  \n8. El-Sappagh S, Alonso J, Ali F, Ali A, Jang JH, Kwak KS. An Ontology -Based Interpretable Fuzzy \nDecision Support System f or Diabetes Diagnosis. IEEE Access. 2018. 37371- 37394.  \n9. Voltaware. Electricity Data Intelligence. [Online]. 2021.  \n10. Espinoza A, Penya Y, Nieves JC, Ortega M, Peña A, Rodríguez D. Supporti"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "Nieves JC, Ortega M, Peña A, Rodríguez D. Supporting Business \nWorkflows in Smart Grids: An Intelligent Nodes -Base d Approach. IEEE Transactions on \nIndustrial Informatics. 2013; 9(3): 1384 -1397.  \n11. Adobe Systems. Adobe Content Intelligence. [Online]. 2021.  \n12. Amazon Web Services. Amazon Aurora Machine Learning. [Online]. 2021.  \n13. Gartner. 3 Barriers to AI Adoption. 2019 September 18.  \n14. Ghosh B, Daugherty PR, Wilson HJ, Burden A. Taking a Systems Approach to Adopting AI. \nUSA. 2019.  \n15. Fountaine T, McCarthy B, Saleh T. Building the AI -Powered Organization. USA. 2019.  \n16. Ransbotham S, Khodabandeh S, Kir on D, Candelon F, Chu M, LaFountain B. Expanding AI's \nImpact With Organizational Learning. Research report. 2020.  \n17. Ransbotham S, Kiron D, Gerbert P, Reeves M. Res"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "7. Ransbotham S, Kiron D, Gerbert P, Reeves M. Reshaping Business With Artificial Intelligence. \nUSA. 2017.  \n18. Swanson EB, Ramiller NC. The  organizing vision in information systems innovation. \nOrganization science. 1997 October; 8(5): 458 -474.  \n19. Ransbotham, Khodabandeh, Kiron, Candelon, Chu, LaFountain, ref 16. above  \n20. Ransbotham, Kiron, Gerbert, Reeves, ref 17. above  \n21. Ransbotham, K iron, Gerbert, Reeves, ref 17. above  \n22. Butler T, O’Brien L. Artificial intelligence for regulatory compliance: Are we there yet? Journal \nof Financial Compliance. 2019; 3(1): 44 -59. \n23. Alan Turing Institute. Alan Turing Institute. [Online]. 2021.  \n24. Leslie D. Understanding artificial intelligence ethics and safety: A guide for the responsible \ndesign and implementation of AI systems"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "sponsible \ndesign and implementation of AI systems in the public sector. UK. 2019.  \n25. Ransbotham, Kiron, Gerbert, Reeves, ref 17. above  \n26. Alsheibani SA, Cheung Y, Messom C. Factors Inhibiting the Adoption of Artificial Intelligence \nat organizational- level: A Preliminary Investigation. In Americas Conference on Information \nSystems (AMCIS); 2019; Cancun, Mexico: AIS. p. 1- 10. \n27. Jöhnk J, Weißert M, Wyrtki K. Ready o r Not, AI Comes - An Interview Study of Organizational. \nBusiness Information Systems Engineeing. 2021; 63(1): 5 -20. \n28. Alsheibani, Cheung, Messom, ref 26. above  \n29. Butler, O’Brien, ref 22. above  \n30. Purdy M, Daugherty. Why Artificial Intelligence Is t he Future of Growth. 2016.  \n31. Butler, O’Brien, ref 22. above  \n32. Ibid \n33. Swanson, Ramiller, ref 18. a"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "above  \n32. Ibid \n33. Swanson, Ramiller, ref 18. above  \n34. Faggella D. Critical Capabilities - The Prerequisites to AI Deployment in Business. Emerj Plus. \n2020.  \n35. Martin EJ. THE AI Skills Gap. Speech Technology. 2019; 24(2): 12 -15. \n36 Katanforoosh K. AI Career Pathways: Put Yourself on the Right Track.  \n37. NCOR -BR. Centro Nacional de Pesquisa em Ontologias para Ciência da Informação. [Online].  \n38. Xie M, Ding L, Xia Y, Guo J, Pan J, Wang H. Does artificial intelligence affect the pattern of skill \ndemand? Evidence from Chinese manufacturing firms. Economic Modelling. 2021; 96: 295 -\n309.  \n39. Fountaine, ref 15. above  \n40. Ransbotham, Khodabandeh, Kiron, Candelon, Chu, LaFountain, ref 16. above  \n41. Faggella, ref 34. above  \n42. Karjaluoto A, Peltomaa A, Lehtinen R. Bridgin"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "\n42. Karjaluoto A, Peltomaa A, Lehtinen R. Bridging the AI skills gap for machine manufacturers. \nControl Engineering. 2020; 67(9).  \n43. Katanforoosh, ref 36. above  \n44. Fountaine, ref 15. above  \n45. Woods C. Explaining Ontologies to Your Boss. Ontologies Explained. [Online]. 2020.  \n46. Oxford Artificial Intelligence Programme. Oxford Artificial Intelligence Programme \nUnderstand AI, its potential for business, and the opportunities for its implementation. \n[Onli ne]. \n47. Ng A. Machine Learning Yearning: Technical Strategy for AI Engineers, In the Era of Deep \nLearning (Draft Version): deeplearning.ai; 2018.  \n48. DeepLearning.AI. Coursera. [Online].  \n49. Imperial College London on Coursera. Mathematics for Machine Learning Specialization. \n[Online]. 2021.  \n50. Paschen U, Pitt C, Ki"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "tion. \n[Online]. 2021.  \n50. Paschen U, Pitt C, Kietzmann J. Artificial intelligence: Building blocks and an innovation \ntypology. Business Horizons. 2020; 63(2): 147- 155.  \n51. Gil D, Hobson S, Mojsilović A, Puri R, Smith JR. AI for Management: An Overview. In Canals \nJaHF. The Future of Management in an AI World: Redefining Purpose and Strategy in the \nFourth Industrial Revolution. Springer International Publishing; 2020.  \n52. Davenport TH. From analytics to artificial intelligence. Jo urnal of Business Analytics. 2018; \n1(2): 73- 80. \n53. Defize D. Developing a Maturity Model for AI -Augmented Data Management. University of \nTwente, Faculty of EEMCS, Master Business Information Technology; 2020 October.  \n54. Futia G, Vetrò A. On the Integra tion of Knowledge Graphs into Deep Learning"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "ntegra tion of Knowledge Graphs into Deep Learning Models for a \nMore Comprehensible AI —Three Challenges for Future Research. Information. 2020 \nFebruary 22; 11(2): 10.  \n55. Government of Canada. Artificial Intelligence Designer in Canada. Job Bank. [Online]. 2021 \n[cited 2021 February 25. Available from: \nhttps://www.jobbank.gc.ca/marketreport/skills/24510/ca ]. \n56. Government of Canada. Artificial Intelligence (ai) Programmer in Canada. Job Bank.  [Online]. \n2021 [cited 2021 February 25. Available from: \nhttps://www.jobbank.gc.ca/marketreport/skills/227159/ca ]. \n57. Long D, Magerko B. What is AI Literacy? Competencies and Design Considerations. \nProceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 2020: p. \n1-16. \n58. Ibid \n \ni This document is an author vers"
  },
  {
    "arxiv_id": "2305.15922",
    "title": "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations",
    "chunk": "16. \n58. Ibid \n \ni This document is an author version of the paper  in the Journal of AI, Robotics & Workplace Automation, 1 (1) , 18-\n33 (2021) .                                                            \n"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "Improving Health Professionals’ Onboarding with AI and XAI for Trustworthy\nHuman-AI Collaborative Decision Making\nMIN HUN LEE, Singapore Management University, Singapore\nSILVANA CHOO XINYI, Singapore General Hospital, Singapore\nSHAMALA D/O THILARAJAH, Singapore General Hospital, Singapore\nWith advanced AI/ML, there has been growing research on explainable AI (XAI) and studies on how humans interact with AI and XAI\nfor effective human-AI collaborative decision-making. However, we still have a lack of understanding of how AI systems and XAI should\nbe first presented to users without technical backgrounds. In this paper, we present the findings of semi-structured interviews with health\nprofessionals (n=12) and students (n=4) majoring in medicine and health to study how to improve onboarding w"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "ne and health to study how to improve onboarding with AI and XAI. For\nthe interviews, we built upon human-AI interaction guidelines to create onboarding materials of an AI system for stroke rehabilitation\nassessment and AI explanations and introduce them to the participants. Our findings reveal that beyond presenting traditional performance\nmetrics on AI, participants desired benchmark information, the practical benefits of AI, and interaction trials to better contextualize AI\nperformance, and refine the objectives and performance of AI. Based on these findings, we highlight directions for improving onboarding\nwith AI and XAI and human-AI collaborative decision-making.\nCCS Concepts: •Human-centered computing →Interactive systems and tools ;User studies ;•Applied computing →Health care\ninfo"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "User studies ;•Applied computing →Health care\ninformation systems ; •Computing methodologies →Artificial intelligence ;Machine learning .\nAdditional Key Words and Phrases: Human Centered AI; Human-AI Collaboration; Trustworthy AI; Explainable AI; Trust; Clinical\nDecision Support Systems; Physical Stroke Rehabilitation Assessment\n1 INTRODUCTION\nArtificial intelligence (AI) has been increasingly being explored to provide data-driven insights for improving various\ndecision-making tasks (e.g. health [ 5,12,15,35,67] and other social services [ 29,74]). Even if recent research has\ndemonstrated that these AI systems can have competent performance that can rival domain experts [ 12,19,33,48,59,62],\na fully autonomous approach of AI systems in high-stake contexts (e.g. health) is not desirable due"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "-stake contexts (e.g. health) is not desirable due to safety and ethical\nissues. A growing body of research has been conducted to investigate how humans and AI systems can complement\neach other’s strengths [ 12,35,62] and integrate these AI systems in practice [ 5,48,59]. However, it is still challenging\nto integrate these systems in practice [ 27,28,61,67] due to several factors, such as lack of user acceptance and trust\n[27, 28, 61] and difficulty with understanding the rationale of an AI output [12, 38, 55].\nTo address these challenges of integrating AI systems in practice, there is growing work that aims to make AI\nhuman-centered [ 5,13,34], explainable [ 1,4,31,53,68], and trustworthy [ 3,20,37]. Along this line, recent research\nworks involved stakeholders to understand their practice"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "involved stakeholders to understand their practices and needs [ 34,67,72] and socio-environmental factors [ 5] to\ndesign explainable AI techniques to provide new insights on a decision making task and study how clinicians or health\nprofessionals can make use of AI outputs [ 11,12,34]. However, previous studies [ 10,12,34] assume that users without\ntechnical backgrounds can be onboarded with an AI system and AI explanations. Some research described the failures\nof effectively using AI explanations as they might be inadvertently the most understandable for users with technical\nbackgrounds [ 60]. Additionally, there has been limited understanding of how AI systems should be introduced to users\nwithout technical backgrounds [ 13] and whether they can specify a desirable basic performance of AI"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "ey can specify a desirable basic performance of AI to consider using it.\nAuthors’ addresses: Min Hun Lee, mhlee@smu.edu.sg, Singapore Management University, Singapore, Singapore; Silvana Choo Xinyi, Singapore General\nHospital, Singapore, Singapore; Shamala D/O Thilarajah, Singapore General Hospital, Singapore.\n1arXiv:2405.16424v1  [cs.HC]  26 May 2024\n2 Lee et al.\nIn this work, we focus on the context of physical stroke rehabilitation assessment and explore how AI and AI\nexplanations should be introduced to users without technical backgrounds (e.g. health professionals and students majoring\nin medicine and health). To this end, we leveraged previous research of guidelines for human-AI interaction [ 2,50], AI\nmodel card [ 43], onboarding recommendations [ 12], and tutorials of XAI technique"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "ommendations [ 12], and tutorials of XAI techniques [ 31] to create onboarding tutorial\nmaterials of an AI-based decision support system and XAI techniques for the context of the study.\nOur onboarding tutorial materials include 1) the description of the context (Figure 1a and 1c) and primary usage\nof AI (Figure 1b), 2) the introduction of AI (i.e. Figure 2a: inputs and outputs of AI & how it can be developed and\noperate, Figure 2b: dataset, and Figure 2c performance metrics and performance of AI), and 3) the descriptions of the\nmotivation and meaning of an AI explanation (Figure 3a) and three widely used AI explanations (i.e. feature importance,\ncounterfactuals, and prototype/example-based) for the context of the study (Figure 3b, 3c, and 3d).\nUsing our onboarding tutorial materials, we co"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "d).\nUsing our onboarding tutorial materials, we conducted a semi-structured interview with 12 health professionals and 4\nstudents majoring in medicine and health. Throughout the interview, we learned their practices to build a trustworthy\nrelationship with their colleagues. We also collected participants’ feedback about the tutorial materials including their\nconfusions to understand their information needs and areas for improvement. In addition, they were asked to describe a\ndesirable performance AI to consider using it, rank the usefulness of three AI explanations for onboarding and decision\nsupport, and share suggestions on how to improve the onboarding and decision support with AI and AI explanations.\nOur findings highlight the value of tutorials on AI and AI explanations along with the"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "tutorials on AI and AI explanations along with the information needs of users without\ntechnical backgrounds (e.g. health professionals and students in medicine and health) on functional, developmental, and\nevaluation aspects of AI and how to make use of AI explanations. Specifically, participants suggested the context-specific\nrequired AI performance and evaluations to determine the usage of AI. Beyond presenting a numerical traditional\nperformance metric, they also recommended communicating the benchmark information and the benefit of AI to\ncontextualize AI capabilities and limitations. As they build a trustworthy relationship with their colleagues over time, they\nsuggested providing iterative trials to refine AI objectives and tune it with feedback for trustworthy interactions with AI.\nA"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "h feedback for trustworthy interactions with AI.\nAdditionally, our study uncovered challenges of how AI explanations can be designed and used to improve onboarding\nwith AI and support interactive communications with AI: creating a way to measure the level of understanding of AI and\nAI explanations, aligning goals between users and AI, and specifying the practices to audit AI.\nOverall, our study provides insights into how AI and AI explanations can be presented to users without technical\nbackground and contributes to design considerations and challenges to improve their onboarding with AI. Our work\nadvances ongoing discussions around onboarding and education of non-technical, domain users with AI [ 13,29,58] for\neffective human-AI collaborative decision-making in various high-stake domains."
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "ive decision-making in various high-stake domains.\n2 RELATED WORK\n2.1 Challenges of Deploying AI-based Decision Support Systems for Human-AI Collaboration\nAs AI achieves high performance to replicate expert’s decision-making [ 19,33,48,59], such as diagnosing prostate\ncancer [ 48] or assessing the quality of post-stroke rehabilitation exercises [ 33], AI has been investigated in the form of a\ndecision support system [ 12,35]. Specifically, ongoing research efforts explore to integrate AI-based decision support\nsystems that provide data-driven insights (e.g. quickly retrieving similar cases from previously diagnosed patients [ 12]\nand identify important input features [ 35]) to enhance domain experts’ accuracy and efficiency of decision making into\npractice [ 5,12,35,67]. However, it remain"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "ng into\npractice [ 5,12,35,67]. However, it remains challenging to integrate these systems into practice [ 27,28,61,67]. One\nimpediment to adopting these systems has been the lack of user acceptance and trust [ 27,28,61]. As these systems\nImproving Health Professionals’ Onboarding with AI and XAI for Trustworthy Human-AI Collaborative Decision\nMaking 3\nutilize a complex AI algorithm and often operate as a black box [ 12,55,61], users have difficulty with understanding\nwhy the system provides a certain outcome [ 12,38,55]. If domain experts (e.g. clinicians, health professionals) do not\nunderstand the intended use, functionalities, or capability of an AI-based decision support system [ 40,61], they may\nresist and abandon its usage [27].\n2.2 Towards Human-Centered, Trustworthy, and Explainab"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "Towards Human-Centered, Trustworthy, and Explainable AI\nResearchers have emphasized the importance of making AI human-centered [ 5,13,34] , trustworthy [ 3,20,26,37],\nexplainable [ 1,4,31,53,68] to make it more deployable in practical settings. In the following subsections, we summarize\nthe prior work on human-centered, trustworthy, explainable AI and describe how we build upon and differentiate with the\nprior work.\n2.2.1 Designing Human-Centered AI. For human-centered designs and evaluation of AI, increasing recent research\nworks [ 5,34,57,72] highlight the importance of involving stakeholders to understand their challenges and needs\n[34,67,72] and socio-environmental factors [ 5]. For instance, Wang et al. [ 67] conducted interviews with clinicians in\nChina and conducted observations to "
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "clinicians in\nChina and conducted observations to examine how AI-based decision support systems are used and discussed the issue\nof misalignment with local context and workflow and usability barriers. Lee et al. [ 34] interviewed and conducted a\nfocus group session with therapists to understand their challenges and needs during rehabilitation assessment to design a\nhuman-centered decision support system. Beede et al. interviewed and observed the eye-screening workflows of clinics in\nThailand, characterized the user expectations and post-deployment experiences of the AI-assisted screening process, and\ndiscussed the necessity of evaluating a system in socio-technical contexts [5].\nIn this work, we focus on the context of an AI-based decision support system for assessing physical stroke rehab"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "support system for assessing physical stroke rehabilitation\nassessment. Building upon a growing body of research that highlights the value of human-centered approaches to invite\nfeedback on designs of AI systems from the target users [ 5,34,67,72], we engaged with stakeholders without technical\nbackgrounds (e.g. health professionals) to explore how to improve their onboarding with an AI-based decision support\nsystem. As stakeholders without technical backgrounds may not provide sufficiently detailed suggestions on a narrow\ndesign aspect (e.g. the overall problem formulation) [ 29], our interdisciplinary team of a technical researcher and domain\nexperts in stroke rehabilitation has worked together to create onboarding materials of AI and AI explanations and\nconducted semi-structured intervi"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "explanations and\nconducted semi-structured interviews with health professionals to collect their critiques and suggestions on how to\nimprove onboarding with AI and AI explanations.\n2.2.2 Efforts on Framework for Trustworthy AI. Trust is considered as a critical component of the successful\ndeployment of AI and increasing research discusses about creating trustworthy AI [ 20,63,65,70]. Although there has\nbeen little common understanding of what constitutes trust or trustworthy AI [ 22], researchers have discussed several\ndefinitions and frameworks of trustworthy AI. For instance, Vashney [ 65] builds upon the definition of trust and describes\nfour attributes of trustworthy artificial intelligence: 1) technical competence that refers to the basic performance and\naccuracy of an AI model, 2) re"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "sic performance and\naccuracy of an AI model, 2) reliability and fairness that indicates maintaining good and correct performance across\nvarying operating conditions, 3) understandability that describes whether users can comprehend the pipeline and lifecycles\nof an AI model, and 4) personal attachment/benevolence, which refers whether the purpose of an AI model can be aligned\nwith a society’s wants. In addition, Toreini et al. [ 63] based on the widely accepted principles of trust, ABI (Ability,\nBenevolence, Integrity) [ 41] and described a framework of trustworthy AI that includes a temporal dimension from initial\ntrust to continuous trust [36, 63] and four technologies: fairness, explainability, auditability, and safety.\n4 Lee et al.\nAlthough there are increasing efforts to make framework"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "ugh there are increasing efforts to make frameworks for trustworthy AI, there are still remaining questions\non how these frameworks can be applied to create a new application of trustworthy AI (e.g. how we can effectively\nbuild initial trust with AI? what would be desirable basic performance of trustworthy AI and role of explainable AI\ntechniques?). In addition, most prior work of designing human-centered, AI-based clinical decision support systems\nassumed that clinicians or health professionals onboard with AI and then studied how they can interact with these AI-based\nsystems [ 5,34,57,67,72]. The problem of how users without technical backgrounds can be onboarded with AI [ 13] (i.e.\nunderstandability aspects of trustworthy AI [65]) is underexplored.\nBuilding upon previous research of tru"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "erexplored.\nBuilding upon previous research of trustworthy AI [ 20,63,65,70] and guidelines of human-AI interaction [ 2,13,\n14,50] including AI model cards [ 43], we explored how onboarding tutorial materials of AI and XAI can be created\nand presented to users without technical backgrounds for trustworthy AI. Among various aspects and components of\ntrustworthy AI, our work focuses on exploring how to build initial trust [ 63] and effectively onboard with AI while\nunderstanding the information needs of health profesionals on AI and XAI and exploring the possibility of defining the\nuser’s notion of basic performance of an AI model to start using it.\n2.2.3 Technically Oriented AI Explanations. To address the user’s difficulty with understanding the rationales of an\nAI output/recommendation, r"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "g the rationales of an\nAI output/recommendation, researchers have explored techniques to make AI interpretable and explainable [ 4,31,53,68].\nThese explainable AI techniques can be broadly categorized into 1) inherently interpretable models (e.g. rule-based models\nor linear regressions) whose internal mechanisms are directly interpreted and 2) post-hoc explainable AI (XAI) techniques\nthat provide explanations of a complex algorithm (e.g. a deep learning model) [ 31]. Various post-hoc XAI techniques\ncan be further classified into explaining the model’s overall or instance-specific behavior [ 31]. Among various post-hoc\nXAI techniques, this work focuses on three widely used local XAI techniques: feature importance, counterfactual, and\nprototype/example-based explanations. A feature importanc"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "pe/example-based explanations. A feature importance explanation describes how much input features contribute to\na model output [ 23,46,56]. A counterfactual explanation describes how input features should be changed to update an\nAI output [ 25,45,66]. A prototype/example-based explanation aims to identify samples that are the most relevant and\ninfluential to an AI output [11, 23].\nExplainable AI techniques that generate rationales of an AI output aim to serve a variety of users: technical AI/ML\ndevelopers, who monitor and debug an AI model, or a non-technical, domain users, such as clinicians or health pro-\nfessionals, who review AI explanations as relevant evidence and outcomes on a decision-making task. However, prior\nresearch has shown that these AI explanations are not useful for peopl"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "hat these AI explanations are not useful for people without technical background (e.g. clinicians or\nhealth professionals in clinical practice) [ 10,52]. These failures of effectively using AI explanations might have occurred\nbecause these AI explanations are not designed for specific end-users or tasks [ 60]. These AI explanation methods\nmight be inadvertently the most understandable to people with technical backgrounds who build and debug an AI model.\nAs the end-users might have different needs, goals, and tasks when interpreting and reacting to AI model outputs and\nexplanations, it is critical to engage with the end-user and make AI explanations user-centered.\nIn this work, we utilized three widely used XAI techniques and explored how these techniques can be used to improve\nusers’ onboa"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "ese techniques can be used to improve\nusers’ onboarding with AI for their AI-assisted decision-making.\nThe most relevant research to our work is research by Cai et al. [ 13] that describes pathologists’ information needs on\nan AI model (i.e. known strengths and limitations and its design objective). Although the previous work [ 13] provides\nseveral suggestions on onboarding with AI, it remains unclear how we can introduce users without technical backgrounds\nto the functionality, strengths and limitations, and design objectives of AI and AI explanations. Building upon this previous\nresearch [ 13], our research further investigates the usefulness of an AI model card [ 43] to communicate the competence\nImproving Health Professionals’ Onboarding with AI and XAI for Trustworthy Human-AI Collabo"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "g with AI and XAI for Trustworthy Human-AI Collaborative Decision\nMaking 5\nof an AI model [ 50] for user’s onboarding with AI. Specifically, we studied whether users without technical backgrounds\n(i.e. health professionals) can leverage a traditional performance metric from an AI model card to understand the strengths\nand limitations of an AI model and determine whether an AI model can be used in practice. In addition, we conducted a\ndeeper examination of aspects to faciliate users’ onboarding with AI and AI explanations and how three widely used\nAI explanations can be used for onboarding and decision-making with AI. Our work further discusses considerations to\nimprove onboarding with AI and AI explanations and human-AI collaborative decision-making.\n3 STUDY DESIGN\nThis work aims to unders"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "on-making.\n3 STUDY DESIGN\nThis work aims to understand how an AI-based decision support system can be introduced to medical practitioners for its\ntrustworthy usage. Specifically, we focused on studying (1) how well medical practitioners can understand the onboarding\ntutorial materials of an AI decision support system and (2) whether they can leverage a traditional evaluation metric\nwhich is commonly used by AI/ML researchers to indicate how well an AI/ML model can classify/predict ground truth\nscores (e.g. F1-scores) to determine the usage of the system, (3) the usefulness of three AI explanations for onboarding\nand decision support, and (4) informing the design of onboarding tutorial materials and considerations for trustworthy\nusage of an AI-based decision support system.\nTo address thes"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": " AI-based decision support system.\nTo address these research questions, we leveraged existing guidelines for human-AI interaction [ 2,50] and onboarding\nwith AI [ 13], an AI model card [ 43], tutorials of AI explanations [ 31] to create the onboarding tutorial materials of an\nAI-based decision support system for physical stroke rehabilitation assessment (Figure 1, 2, 3). In addition, we had\niterative online synchronous and asynchronous discussions with domain experts in stroke rehabilitation to inform a set\nof semi-structured interview questions and refine onboarding tutorial materials of an AI-based decision support system\nfor physical stroke rehabilitation assessment. During the online synchronous discussions, the leading researcher with a\nbackground of human-AI interaction and machine l"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "a\nbackground of human-AI interaction and machine learning presented the draft of interview questions and onboarding\nmaterials and collected feedback on areas to be improved and revised questions, onboarding materials, and scripts for the\nfollow-up discussions. After refining and finalizing the interview questions and onboarding materials, we conducted a\npilot interview session with a student who majors in law and does not have technical backgrounds to check the length\nof a session and whether onboarding materials are understandable for people without technical backgrounds. Then, we\nconducted a semi-structured interview with healthcare professionals (i.e. therapists and a medical social worker) and\nstudents majoring in medicine and healthcare (e.g. nursing, therapy). This study including on"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "e (e.g. nursing, therapy). This study including onboarding tutorial materials,\nprotocol, and recruitment methods was approved by the Institutional Review Board.\n3.1 Onboarding Tutorial Materials\nOur onboarding tutorial materials are composed of three parts: introducing 1) the context and AI applications for physical\nstroke rehabilitation, 2) the development and evaluation of an AI model (e.g. how it is trained and operates on new\ndata, dataset, evaluation metrics, and performance), 3) AI explanations (e.g. the motivation of AI explanations, feature\nimportance, counterfactual, and example/prototype-based explanations).\nFirst, building upon guidelines [ 43,50], we described the context and challenges that an AI-based system aims\nto address (Figure 1a), the primary applications for the users "
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "igure 1a), the primary applications for the users of this study (i.e. therapists and post-stroke survivors)\n(Figure 1b), and envisioned use cases of quantitative stroke rehabilitation assessment (i.e. assessing the range of motion,\nsmoothness, and the presence of compensatory motions) (Figure 1c). The descriptions of tutorial materials (Figure 1) to\nintroduce the contexts of the study can be found below:\n6 Lee et al.\n(a)\n(b)\n(c)\nFig. 1. Onboarding Tutorial Materials of an AI that introduce (a) an (c) the context of physical stroke rehabilitation assessment\nand (b) AI applications of physical stroke rehabilitation assessment and therapy.\nImproving Health Professionals’ Onboarding with AI and XAI for Trustworthy Human-AI Collaborative Decision\nMaking 7\nFigure 1a: “When stroke occurs, post-st"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "n\nMaking 7\nFigure 1a: “When stroke occurs, post-stroke survivors will have paralyzed and limited functional abilities. They\ntypically involve therapy sessions to regain their functional and cognitive abilities. During therapy sessions, therapists\nassess the functional & cognitive status of a patient and prescribe a set of exercises to practice. In this work, we focus\non the functional assessment of post-stroke survivors. During rehabilitation therapy, therapists often prescribe a set of\nexercises to a patient due to their limited availability. A therapist and a patient have regular follow-up meetings to\ndiscuss the patient’s status and progress and adjust the rehabilitation program accordingly. During the follow-up\nmeeting, there is limited quantitative information on the patient’s status "
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": " quantitative information on the patient’s status for therapists to make informed decision\nmaking. ”\nFigure 1b: “To address this challenge, there have been increasing explorations on an AI-based system for rehabilitation.\nThis system typically utilizes a vision-based or wearable sensor to estimate body joint positions and extract various\nkinematic features to quantify the patient’s quality of motion. This quantitative assessment can be provided to\ntherapists as a decision support system for improving their rehabilitation assessment or a virtual or exoskeleton\nsystem to improve patients’ engagement in rehabilitation. ”\nFigure 1c: “For the rehabilitation assessment, therapists assess patient’s quality of motion in the following three\naspects: Range of Motion, Smoothness, Compensation. ROM in"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": " Range of Motion, Smoothness, Compensation. ROM indicates whether a patients can achieve a specific\ntarget motion. Smoothness indicates whether a patients can coordinate their motion smoothly. Compensation checks\nwhether a patient involves any unnecessary joint motion; here this patient compensates with his shoulder and trunk to\nmove his arm that is affected by stroke. ”\nIn the second part of the onboarding tutorial materials, as suggested by the guidelines [ 43,50], we explained the inputs\nand outputs of an AI-based system and how a typical AI-based system for rehabilitation assessment can be developed\nand operated (Figure 2a) and described the dataset [ 33] (i.e. how it is collected and labeled) (Figure 2b). In addition,\nwe elaborated on performance metrics and the performance of an AI m"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "performance metrics and the performance of an AI model [ 43,50] (Figure 2c). For reporting the\nAI performance, we utilized the dataset [ 33] and followed the previous research on quantitative stroke rehabilitation\nassessment [ 33] to implement a feed-forward neural network model from using Pytorch libraries [ 51]. The implementation\ndetails of the AI model can be found in the Appendix. A. We then reported how well an AI model can assess three\ncommon performance components of rehabilitation assessment (i.e. range of motion, smoothness, and compensation).\nThe descriptions of tutorial materials (Figure 2a) to introduce the development, operation, and evaluation of AI can be\nfound below:\nFigure 2a: “Here, we describe the pipeline of developing an AI model in more detail. When the system estima"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "an AI model in more detail. When the system estimates body\njoints, it extracts kinematic features, such as elbow flexion, how much each joint moves in a certain direction, and\ncomputes the overall statistics during an exercise, such as the maximum value of elbow flexion and the corresponding\nlabels of exercises, whether an exercise has the full range of motion or not; smooth or not; involves any compensations\nor not;\nFor training an AI model, we collect samples of patients’ exercises, extract features, and collect labels. Here, we have\nonly 5 kinematic features, but in a real case of the development, we have a lot more features and samples. Given these\npaired features and labels, an AI model learns a function that maps features and corresponding labels as closely as\npossible.\nGiven new tes"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "nding labels as closely as\npossible.\nGiven new test data and extracted features from a patient, the AI model generates an outcome whether an exercise\nhas full ROM or not, smooth, or involves any compensations or not. Here, the elbow flexion angle is similar to the\n8 Lee et al.\n(a)\n(b)\n(c)\nFig. 2. Onboarding Tutorial Materials of an AI: (a) a diagram of how AI is developed and operated, (b) dataset, and (c)\nevaluation metrics and performance\nnormal case. And the distance of shoulder up is also small and close to the normal and along with a similar ratio of\ntrembling. Thus, we have AI outputs of normal ROM, smooth, and no compensation. ”\nImproving Health Professionals’ Onboarding with AI and XAI for Trustworthy Human-AI Collaborative Decision\nMaking 9\nFigure 2b: “For an explorative study, we"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "\nMaking 9\nFigure 2b: “For an explorative study, we collected 300 samples of exercises, in which 15 post-stroke survivors\nperformed 10 trials on their unaffected and affected sides. We collected labels of these exercises from a therapist, who\nhas 5 years of practicing stroke rehabilitation and conducted a fugl meyer assessment on 15 post-stroke survivors.\nUsing this dataset, we developed an AI model to replicate therapist’s assessment on ROM, Smoothness, Compensation\nof patient’s exercises. ”\nFigure 2c: “Any AI model that we build is guided by a reward function, which the AI model uses to determine ‘right’\nor ‘wrong’ outcomes. We should consider how we specify this reward function that the system will optimize for [ 50].\nWhen an AI generates outcomes of whether an exercise is correctly cond"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": " outcomes of whether an exercise is correctly conducted or not, there are four possible outcomes\n[50]: True Positive: indicates AI outputs an ‘Incorrect’ motion given an ‘Incorrect’ motion; True Negative: indicates\nAI outputs \"correct\" motion given a ‘correct’ motion; False Positive: indicates AI outputs an ‘Incorrect’ motion given\na ‘correct’ motion; False Negative: indicates AI outputs ‘correct’ motion given an ‘Incorrect’ motion\nIf you want fewer false positives, you can consider optimizing precision; If you want fewer false negatives, you can\nconsider optimizing recall; If you want fewer false positives & negatives, you can consider optimizing the F1-score\nGiven our dataset, we optimize an AI model to have fewer false positives & negatives. An AI model achieves a 82%\nF1-score on ROM; 7"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "ves. An AI model achieves a 82%\nF1-score on ROM; 79% on Smoothness; 77% on compensation;\nTo understand the competence of an AI model to replicate a therapist’s assessment, we computed how well a\nsecondary therapist agrees with the therapist, who generated annotations. Overall, our AI model can achieve\ncomparable performance with a secondary therapist. ”\nIn the third part of the tutorial materials, we first described the motivation and meaning of an AI explanation [ 31]\nusing an image classification task [ 56]. In addition, we explained three commonly used local AI explanations (i.e. feature\nimportance, counterfactuals, and prototypes/example-based) [ 18,31] for the context of the study. The descriptions of\ntutorial materials (Figure 3) to introduce the AI explanations can be found below:\nF"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "ntroduce the AI explanations can be found below:\nFigure 3a: “There has been increasing research on AI explanations, which aims to improve users’ understanding of\nhow the AI-based system works and determine when to trust an AI output\nHere, we have an AI model that classifies the type of animals from an image; Given an input image, an AI model\nclassifies it as Siberian Husky. Here I give you one example of an AI explanation; An AI explanation describes which\nparts of an image an AI model relied on for its output. We can find that an AI model focuses on snow parts and identify\nthe limitation of an AI model. Thus, we need to be careful of using this AI model. ”\nFigure 3b: “Feature importance describes the overall importance of different features on AI model outcomes; Once\nwe identify important"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "s on AI model outcomes; Once\nwe identify important features, we can pick the top most important features and show the comparison of these feature\nvalues on patients’ unaffected and affected sides to check the threshold/boundary between unaffected/affected side\nmakes sense or not and determine whether to trust AI or not. ”\nFigure 3c: “Counterfactuals describe how input features need to be changed to generate an opposite outcome. By\nreviewing which inputs lead to different outcome, we can understand how an AI operates on specific inputs and\ndetermine whether we can trust AI or not. For instance, given an AI output of detecting compensation (1), the\ncounterfactual explanations describe that an AI model will generate the output of no compensation if the feature value\nof max. leaningbackward-sh"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "on if the feature value\nof max. leaningbackward-shouldervalue is decreased to 0.35. ”\nFigure 3d: “Prototype/Example-based Explanation shows which sample data points are the most similar to input\ndata. By reviewing relevant samples and the outputs of AI models on these samples, we can identify whether an\n10 Lee et al.\n(a)\n(b)\n(c)\n(d)\nFig. 3. Onboarding Tutorial Materials of an AI explanations: (a) motivation of an AI explanation, (b) a feature importance\nexplanation, (c) a counterfactual explanation, and (d) prototype/example-based explanations.\nImproving Health Professionals’ Onboarding with AI and XAI for Trustworthy Human-AI Collaborative Decision\nMaking 11\nAI model has the right outputs or not. Also, we can summarize the input feature into two key dimensions and use\nthese representation"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "to two key dimensions and use\nthese representations to identify which samples are closely aligned and similar with each other. Specifically, we can\nproject the features representation of each patient in the visualization. For instance, by reviewing this Figure, we can\nunderstand how each patient has been represented and which patients are represented/considered similar by an AI\nmodel and check whether an AI model utilizes correct feature representations or not. ”\nFor a feature importance explanation, we utilized the SHAP library that support consistency and local accuracy [ 39] to\ncompute importance scores (i.e. SHAP values) of each feature (Figure 3b). In addition, we utilized the top three most\nimportant features to show the difference between patient’s unaffected and affected sides by s"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "tween patient’s unaffected and affected sides by stroke using a radar chart\n[2, 35], following the practices of therapists to compare patient’s unaffected and affected side [34].\nA counterfactual explanation indicates what changes in feature values will lead to updating an AI output in a certain\nway [ 31,32,45]. For a counterfactual explanation, we utilized the DiCE library [ 45] to apply a genetic algorithm [ 49]\nto find counterfactuals close to the query point. We specified the features to be changed in the DiCE library using the\nidentified salient features by the SHAP library and their desired range using patients’ held-out normal data to avoid\ngenerating varying and unfeasible explanations. After identifying counterfactual explanations, we generated textual\ndescriptions of the changes "
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": " we generated textual\ndescriptions of the changes in feature values and AI outputs (Figure 3c). For example, Figure 3c describes that the\nvalue of ‘Max.LeaningBackward-Shoulder’ should be decreased to 0.35 to update the AI output from 1 (i.e. noticeable\ncompensation) to 2 (i.e. no compensation).\nA prototype/example-based explanation describes representative or similar samples of a current instance along with AI\noutputs and ground truths on those samples [ 11,12,31] (Figure 3d). We utilized the kinematic features of a patient’s\nmotion [ 33] and computed the cosine similarity score to identify similar samples, which assist a user in understanding\nand validating an AI model output. In addition, we utilized a Principal Component Analysis (PCA) [ 71] to reduce\nthe dimension of a feature represe"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "[ 71] to reduce\nthe dimension of a feature representation. PCA was utilized because it does not require hyperparameter tuning and is\ndeterministic unlike another widely used dimensional reduction technique, t-Distributed Stochastic Neighbor Embedding\n(t-SNE) [ 64]. We then visualized this reduced feature embedding space [ 7] for a user to check whether the feature\nrepresentation of an AI model is valid or not (Figure 3d).\nAlthough we utilized a particular technique/library to identify important features, select counterfactual explanations,\nand reduce the dimension of a feature representation, this work does not intend to communicate the usage of these specific\ntechniques to create tutorial materials without any considerations. Alternative techniques/libraries can be explored for\ndifferent "
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "echniques/libraries can be explored for\ndifferent applications.\n3.2 Recruitment and Demographics\nWe recruited sixteen participants for our study (Table 1). The detailed demographics can be found in Appendix. Table 2.\nOur participants were mostly therapists who had experience in stroke rehabilitation ( P1 - P10 ). Among ten therapists\nin stroke rehabilitation, six of them are physiotherapists, who promote and maintain patient’s physical impairments from\nbio-mechanical perspectives and four of them are occupational therapists, who assist patients to better engage in their\ndaily activities. Therapists work in various settings: four participants are from outpatient clinics, three participants are\nfrom inpatient rehabilitation, two participants are from home care, and two participants are from "
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "are from home care, and two participants are from skilled nursing facility\n(Appendix. Table 2). As some outpatient clinics have an interdisciplinary team to support and manage a patient (e.g.\nphysio/occupational therapists, speech therapists, nurses, doctors), we also included health professionals ( P11 and P12 )\n(e.g. speech therapist and a medical social worker) and students majoring in medicine and health (e.g. therapy and nursing)\n12 Lee et al.\nTable 1. Demographics of Participants: Therapists who have experience in stroke rehabilitation ( P1 - P10 ) and other health\nprofessionals ( P11 - P12 ) and students majoring in medicine or health (e.g. therapy, nursing) ( P13 - P16 ).\nPID Occuptation # of yrsQ. Tech\nExperienceQ. ML\nOutputsPID Occuptation # of yrsQ. Tech\nExperienceQ. ML\nOutputs\n"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "cuptation # of yrsQ. Tech\nExperienceQ. ML\nOutputs\nP1 PhysioTherapist 7 4.8 out of 7 3 out of 3 P11 Speech Therapist 5 4.8 out of 7 2 out of 3\nP2 PhysioTherapist 2 5.2 out of 7 1 out of 3 P12 Medical Social Worker 5 4.0 out of 7 1 out of 3\nP3 PhysioTherapist 8 4.4 out of 7 2 out of 3 P13 Student in Occupational Therapy n/a 3.8 out of 7 3 out of 3\nP4 PhysioTherapist 11 5.8 out of 7 2 out of 3 P14 Student in Speech Therapy n/a 3.8 out of 7 2 out of 3\nP5 PhysioTherapist 9 5.4 out of 7 2 out of 3 P15 Student in Medicine n/a 3.8 out of 7 2 out of 3\nP6 PhysioTherapist 30 5.8 out of 7 2 out of 3 P16 Student in Nursing n/a 4.0 out of 7 0 out of 3\nP7 Occupational Therapist 14 5.4 out of 7 2 out of 3\nP8 Occupational Therapist 11 6.2 out of 7 0 out of 3\nP9 Occupational Therapist 6 4.4 out of 7 2 out o"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "3\nP9 Occupational Therapist 6 4.4 out of 7 2 out of 3\nP10 Occupational Therapist 5 3.2 out of 7 3 out of 3\n(P13 - P16 ). The student in occupational therapy (P13) and the student in speech therapy (P14) had an experience\nof working as an occupational/speech therapy assistant for stroke rehabilitation. Participants were recruited through\nadvertisements sent to the hospital staff, the mailing lists, and the contacts of the research team.\nWe asked the participants to respond to a set of technical experience questions on recent technologies , which were\nbased on questions designed by the Center for Research and Education on Aging and Technology Enhancement (CREATE)\n[17]. Specifically, they were asked to rate their experiences with recent technologies (i.e. computer/laptop, activity tracker,\nvi"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "logies (i.e. computer/laptop, activity tracker,\nvirtual voice assistant, unmanned convenient store, and autonomous vehicle) on a 7-point scale (1 = strongly disagree, 2 =\ndisagree, 3 = somewhat disagree, 4 = neutral, 5 = somewhat agree, 6 = agree, and 7 = strongly agree). A high score on\ntechnology experience (e.g. 7) indicates that a participant self-reported to be highly experienced with a recent technology.\nOverall, participants expressed a diverse level of experience with recent technologies, in which they had an average score\nof 4.31 out of 7.0. In addition, the column of ‘Q.ML Outputs’ in Table 1 describes how many times a participant correctly\nguess an AI output after introducing how an AI model is developed and operated (Figure 2a). In Section 4.2, we described\noverall results of p"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": " In Section 4.2, we described\noverall results of participants and how well the scores of guessing AI outputs are correlated with the scores of technology\nexperiences.\n3.3 Protocol\nWe conducted semi-structured interviews with 16 participants: 12 healthcare professionals (11 therapists and 1 medical\nsocial worker) and 4 students majoring in medicine and healthcare (e.g. therapy, nursing). Our interview protocol is\ncomposed of five main parts. The list of interview questions for a semi-structured interview can be found in Appendix.\nTable 3\nFirst, we asked participants to describe their work environment, how they build a trustworthy relationship with their\ncolleagues, and how they discuss uncertain cases. We included this first question to understand any practices or aspects\nthat should be con"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "rstand any practices or aspects\nthat should be considered for them to have trustworthy interaction with an AI system (Appendix. Table 3). P13 and P14\nshared the responses based on their experience of working as a therapy assistant in a hospital. P15 elaborated on the\nexperience of working with medical students and P16 described the experience of working as a part-time nurse.\nNext, we introduced the context and primary application of this study (i.e. physical stroke rehabilitation and a decision\nsupport system) (Figure 1a, 1b, and 1c) and explained inputs and an output of an AI-based decision support system for\nImproving Health Professionals’ Onboarding with AI and XAI for Trustworthy Human-AI Collaborative Decision\nMaking 13\nrehabilitation assessment and how it can be developed and operate"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "assessment and how it can be developed and operated on a new case using onboarding tutorial materials\n(Figure 2a). Also, a participant was asked to review inputs of a new case to an AI model and guess expected AI outputs.\nThird, we described the dataset (Figure 2b), evaluation metrics, and AI performance along with therapists’ agreement\nlevels (Figure 2c). The therapists’ agreement levels indicate how well annotations of a secondary therapist are aligned\nwith ground truth scores and could provide a reference on how well our AI model performs. After describing the dataset,\nevaluation metrics, and AI performance, we asked the participants without experience of using AI systems and AI\nexplanations for their practices (1) if they have a particular baseline performance of an AI model that is re"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "lar baseline performance of an AI model that is required and\n(2) if they have any specific conditions or edge cases [ 13] that should be included in the dataset or which an AI model\nshould be evaluated or good for considering using AI or for their trust usage (Appendix. Table 3). Note that we did not\nask the second question to participants ( P11 - P12 and P13 - P16 ), who do not have extensive experience in stroke\nrehabilitation as a therapist.\nFourth, after introducing the motivation of an AI explanation (Figure 3a) and three widely used AI explanations\n(i.e. feature importance (Figure 3b), counterfactual (Figure 3c), and prototype/example-based (Figure 3d) explanations),\nwe asked participants to rank which AI explanations are useful to support onboarding (i.e. when a user initially start"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "pport onboarding (i.e. when a user initially starts\nreviewing and understands AI performance) and decision support (i.e. when a user starts reviewing AI outputs for their\ndecision making) (Appendix. Table 3). Finally, we asked them to share any comments on how to improve onboarding\nwith an AI system and validating an AI output during decision support (Appendix. Table 3).\nThroughout the semi-structured interviews, after reviewing onboarding tutorial materials, we asked the participants\nwhether they followed the tutorial materials or had any clarification questions and asked them to provide any feedback on\nmaterials. All interviews were conducted remotely on a video conference platform and recorded for data analysis. Each\ninterview lasted between 60 to 80 minutes. The participants were compe"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "ween 60 to 80 minutes. The participants were compensated for their participation based on the rate\nrecommended by the domain experts.\n3.4 Data Analysis\nWe transcribed all 17.23 hours of interview recordings into text data for thematic analysis [ 9]. We utilized both deductive\nand inductive thematic analysis approaches [ 9,21] to analyze our interview data. We first selected codes based on our\ninterview questions for our research problem: practices of rehabilitation and building a trustworthy relationship, comments\non AI outputs, a development pipeline, the dataset, and the minimum performance, and comments on the AI explanations,\nonboarding with AI, and AI-assistive decision-making. Three researchers including one who facilitated interviews then\nindependently coded the transcript data to g"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": " then\nindependently coded the transcript data to generate 519 codes. We refined the initial codes while discussing disagreements\nand ambiguities in the codes [ 9,21,42] through iterative sessions. Following the practice of a reflective analysis that\ncollaboratively shapes codes through discussion for consensus of codes [ 9,42], we did not calculate inter-rater reliability.\nAfter coding, we grouped similar codes to identify and conceptualize higher-level themes through affinity diagramming.\nOverall, this process yielded five high-level themes, twenty one second-level themes, and eighty five third-level themes\n(Appendix. Table 4). In Section 4, we summarize our findings that broadly corresponds to the higher-level themes that we\nidentified from our interview data.\n14 Lee et al.\n4 RESULTS\n4.1"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "om our interview data.\n14 Lee et al.\n4 RESULTS\n4.1 Clinical Practices of Therapy & Trustworthy Relationships\n4.1.1 Clinical Practices. Our participant therapists were from various settings: outpatient clinic (4); inpatient rehabili-\ntation (3); skilled nursing facility (2); and home care (2). For a holistic understanding of a patient (P5, P7, P14), these\nsettings typically have different sizes of interdisciplinary teams ranging from 2 to 24 team members (e.g. doctors, nurses,\noccupational therapists, physiotherapists, therapist assistants, and speech therapists). In most cases, they work in pairs or\nwith a team of colleagues to “brainstorm how to assess patient’s status” (P9) and “understand the needs of a patient to\ncoordinate therapy sessions” (P8).\nWhen handling a case, participants mig"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "ions” (P8).\nWhen handling a case, participants might get referrals of experienced colleagues (P4) for discussions. Otherwise, they\ndetermine which colleagues will be adequate by checking the following aspects through their previous “interactions with\nthem” (P8): (i) experiences and knowledge (all participants), (ii) quality of work (P10), and various soft skills (P5,\nP10, P12,P14,P15). Specifically, they will approach more senior and experienced colleagues with hard skills, such as a\ncorresponding specialty (P1,P5,P7,P8,P10,P14,P15,P16) or experience with similar cases (P3,P6) for “receiving task-\noriented advice” (P16). In addition, another hard skill that they can check is whether colleagues provide client-centered\nquality work or not (P10,P11,P13). Lastly, they also highly consider soft"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "0,P11,P13). Lastly, they also highly consider soft skills of their colleagues, such as whether their\ncolleagues are “more approachable” (P10) and comfortable to share and discuss (P14,P16) to determine appropriate\npeers or colleagues for discussion.\nFig. 4. Characteristics and skills of a trustworthy colleague and a process to build a trustworthy relationship with colleagues:\na trustworthy relationship requires soft and hard skills to have interactive communications over time to understand colleagues’\nroles, skills, and goals and add a value on their goals.\n4.1.2 Process to Build a Trustworthy Relationship. Participants commented that building a trustworthy relationship\n(Figure 4) requires soft and hard skills to have interactive communications over time , in which one understands\ncolleagu"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "ions over time , in which one understands\ncolleagues’ roles, skills, and goals , but also checks whether a colleague adds a value on common goals (P7,P9,P11,P12).\nFor having a common goal, participants mentioned the importance of interacting with each other to know educational\nbackgrounds and strategies (P4,P9) and “understand each other’s needs and roles” (P12). As they “learn from each other”\nImproving Health Professionals’ Onboarding with AI and XAI for Trustworthy Human-AI Collaborative Decision\nMaking 15\n(P3) and “show that they can help and do the work well” (P5) over multiple interactions or sessions (P1,P2,P6), they\nbuild “a deeper trustworthy relationship with colleagues” (P1).\n4.1.3 Skills & Characteristics of Trustworthy Colleagues. Participants shared several skills and charact"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "es. Participants shared several skills and characteristics of\ntrustworthy colleagues for interactive communication. First, soft skills, such as being honest, approachable & friendly\n(P1,P7,P5,P10), and timely (P8,P15) are important to have iterative communications and identify common goals for\nbuilding a trustworthy relationship. Given common goals, trustworthy colleagues also require hard skills to demonstrate\none’s expertise through “provid[ing] sound, consistent, reliable facts” (P7). In addition, having good reputation with\nothers (P5,P15) and keeping the confidentiality of information (P12) are important to build a relationship over time.\n4.2 Understanding and Information Needs of AI\nOverall, participants understood the high-level ideas of an AI system for rehabilitation after present"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "s of an AI system for rehabilitation after presenting the tutorial\nmaterials. However, participants “are not clear about how the statistical methods work in detail” (P12).\nWhen participants were asked to review inputs of an AI-based system and guess its outputs, participants had diverse\nranges of correct guesses: among 16 participants, three participants (P1,P10,P13) correctly guessed all three outputs,\nnine participants (P3,P4,P5,P6,P7,P9,P11,P14,P15) correctly guessed two out of three outputs, two participants (P2,P12)\ncorrectly guessed one out of three outputs, and two participants (P8,P16) incorrectly guessed all outputs. When we\nanalyzed the correlation between participants’ normalized scores on technology experiences and their normalized scores\non guessing AI outputs by computing Pea"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "zed scores\non guessing AI outputs by computing Pearson’s correlation coefficient ( 𝑟). We found that the coefficient value ( 𝑟) of -0.31\nand the p-value of 0.23, which indicates a weak correlation between normalized scores on technology experiences\nand guessing AI outputs .\nWhile going through the tutorial materials, participants asked questions about functional, operational, development,\nand evaluation aspects of AI .\n4.2.1 Functional & Operational Aspects. For functional aspects, participants (P1,P2,P4,P5,P6,P9) questioned how\nAI processes data and they could interact with AI. Specifically, participants wondered if AI can automatically identify\nincorrect data without therapists’ input (P1,P5). In addition, participants questioned how AI will operate if input data is\nslightly off from nor"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "ill operate if input data is\nslightly off from normal ones (P1,P3,P9,P10). Participants also asked “whether AI can be evolved with new data” (P7)\nand“inputs from therapists” (P10) to have “an adaptive goal/normality” (P10).\nIn addition, participants inquired about the easiness of setting a device/system (P1,P4), “how fast the system can\nprocess data to provide an assessment” (P5), and “how it might be repaired and maintained” (P7).\n4.2.2 Development & Evaluation Aspects. Participants questioned about how to interpret a confusion matrix\n(P11,P16), “any benchmark to determine whether AI is trustworthy or not” (P7), “whether AI can become more accurate\nas it has a larger sample size” (P15), and an evaluation setting (e.g. “whether AI is being piloted in a hospital” - P14).\nWhen it comes to th"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "piloted in a hospital” - P14).\nWhen it comes to the dataset, participants suggested additional factors that can be considered to expand the dataset.\nOverall, participants considered that the usage of the clinically validated functional assessment scores (e.g. fugl meyer\nassessment [ 24]) is good to characterize and recruit post-stroke survivors for data collection. In addition to this functional\nassessment score, participants mentioned about distinguishing post-stroke survivors by other clinically relevant factors:\nthe stage and severity of stroke (P2,P4,P5,P7,P10,P14), spasticity and muscle tone (P3,P7,P8), “the status of finer motor\nfunctions” (P10), “cognitive status” (P9) that a lot of post-stroke survivors struggle with.\n16 Lee et al.\nIn addition, participants suggested expanding the "
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "In addition, participants suggested expanding the dataset to make a more balanced distribution of sex (P2,P4,P6,P11,P12,P12),\nage (P2,P5,P8,P9,P11,P13), race (P4) as different sex, ages, and races might have “different characteristics and factors\nthat affect their recovery” (P2) and consider how to address “possible bias on labels” (P1).\n4.3 Context-specific Required AI Performance & Evaluations\nMost participants had difficulty with enumerating how much percentage is sufficient as they are not familiar with this\nmetric (P4,P5,P10,P15) and “wondered if there is any industry standard to determine a good score” (P3). “When AI has\na lower performance, we[they] won’t likely to use it” (P5). Thus, participants still described that it would be good to\nachieve high accuracy (P3,P15) ranging from 8"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "d to\nachieve high accuracy (P3,P15) ranging from 80% (P2,P8,P12), 85% (P3,P9), to 90% (P6,P7,P10,P13) even if they “do\nnot know whether it is even possible” (P10).\nParticipants mentioned the importance of having context-specific required performance and a way to interpret the\nmeaning of numbers . P11 suggested a context-specific, desirable performance of AI: “when the AI is used as a reference,\n70 - 80% might be enough as therapists would make final assessment” and“when the AI is being used independently, I\nexpect a much higher score at least 95%” .\nParticipants recommended (i) presenting a benchmark performance and (ii) having contextualized and iterative\nevaluations .“Even if we[they] are given this numerical performance value, we[they] have difficulty to interpret an\nactual meaning abou"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "ave difficulty to interpret an\nactual meaning about what it implies” (P10). Participants considered presenting a benchmark performance score (e.g.\ntherapists’ agreement) would be useful to “build an initial guide for comparison” (P15) and check whether AI is reliable\nor not (P8,P14). Also, they suggested contextualizing evaluations by reporting how well AI will perform on the following\naspects: common symptoms (P5,P6) (e.g. “assessing the range of motion” - P1) and difficult tasks (P4,P7,P8,P9,P10,P13)\n(e.g. muscle tone, pelvis, scapular shoulder compensation, spasticity, finer finger motions, knee replacement, and gait\npatterns), “borderline cases” (P3), and uncontrolled situations and data (P3,P5), such as presentation of other people than\na patient and different setups of a system and a"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "n\na patient and different setups of a system and a camera.\nIn addition, participants mentioned that “having trials to see how AI works would be necessary” (P10) and “investigate\nhow to improve it” (P16) instead of presenting a number once as they “need time to build trust with AI as we build trust\nwith our colleagues” (P10).\n4.4 Understanding and Information Needs of AI Explanations\nOverall, participants desired descriptions on how they can leverage AI explanations . Also, a few participants inquired\nunderlying processes of identifying AI explanations and technical terms in visualizations.\nAfter asking a few clarification questions, participants understood the high-level ideas of three AI explanations\n(i.e. feature importance, counterfactual, prototype/example-based explanations). For a fe"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "l, prototype/example-based explanations). For a feature importance explanation,\nmost participants followed the high-level concept in the first place. However, when some participants (P2,P3,P10) first\nencountered a local bar plot of SHAP values for each feature [ 39] (Figure 3b), they asked how to read and interpret the\ngraph (i.e. the meaning of blue and red plots, presented values).\nFor a counterfactual explanation, some participants can follow the concept of a counterfactual explanation by correlating\ntheir similar practice (P2,P4), “‘familiarization’, in which we determine why a client cannot get a correct position” (P4).\nHowever, even if some participants understood the high-level ideas of a counterfactual explanation, they were confused\nabout how these explanations can be used to vali"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "d\nabout how these explanations can be used to validate an AI output (P3,P10,P14,P15).\nFor an example-based explanation, most participants considered that it is “easier to understand and review” (P1).\nSome participants asked questions on the procedures of generating an example-based explanation: “how similar items are\nImproving Health Professionals’ Onboarding with AI and XAI for Trustworthy Human-AI Collaborative Decision\nMaking 17\nidentified” (P2) and “if the clothing color of a patient affects which similar cases will be identified” (P2); “whether an\nAI model generates a similar sample” (P9); how many samples are needed to define a prototype or find relevant cases\n(P2,P8). In addition, after reviewing the visualization of embedding spaces of samples (Figure 3d), P3 asked about the\nmeanin"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": " of samples (Figure 3d), P3 asked about the\nmeaning of the axes to project samples.\n4.5 Usefulness Ranking of AI Explanations for Onboarding and Decision Support\nFigure 5 summarizes the overall ratio of rankings on three AI explanations for onboarding and decision support using\ndata from all participants, therapists with experience in stroke rehabilitation, and other participants.\nFor onboarding, both therapists and other health professionals and students considered an example-based\nexplanation as the most useful . Therapists considered a counterfactual explanation (35.0%) and a feature importance\nexplanation (28.3%) as the second and the third most useful. Other health professionals and students ranked a feature\nimportance explanation (34.2%) and a counterfactual explanation (26.3%) as th"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "2%) and a counterfactual explanation (26.3%) as the second and the third most useful.\nFor decision support, therapists considered both an example-based explanation and a feature importance\nexplanation as equally useful (33.9%) the most and a counterfactual explanation (32.2%) as the third most useful.\nOther health professionals and students ranked an example-based explanation as the most useful (44.4%) and both\na feature importance explanation and a counterfactual explanation as the second most useful ones (27.8%).\nFig. 5. Ratio of Rankings on the Perceived Usefulness of AI Explanations (Feature Importance, Counterfactuals, and\nExample-based) for Onboarding and Decision Support with AI from all participants, therapists with experience of stroke\nrehabilitation, and others (other health prof"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "roke\nrehabilitation, and others (other health professionals and students majoring in medicine and health).\nWhen participants ranked the usefulness of AI explanations, they considered whether “it provides clinically relevant\nand useful information” (P9) and whether it is “easy to understand and interact” (P10).\n18 Lee et al.\nAmong 16 participants, eight participants (P1,P4,P7,P9,P11,P12,P14,P15) had the same rankings on three AI explana-\ntions for onboarding and decision support. They considered that “both onboarding and decision support processes require\nthe same process of reviewing and validating AI outputs” (P7), in which they check how well “AI provides clinically\nrelevant information” (P9). Thus, they mentioned that “the usage of AI explanations would be similar” (P11) for both\nonboar"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "planations would be similar” (P11) for both\nonboarding and decision support.\nIn contrast, eight participants (P2,P3,P5,P6,P8,P10,P13,P16) had different rankings on three AI explanations for\nonboarding and decision support. They differentiated the processes of validating AI outputs for onboarding and decision\nsupport phases. For instance, they described that they want to “validate how well AI outputs are useful to support a\npatient-specific assessment” (P8) for decision support while they aim to “briefly validate the correctness of AI outputs to\ndevelop a trust with AI” (P10) for onboarding.\n4.5.1 Feature Importance Explanation. For a feature importance explanation, participants considered that it is\nuseful to review only important features as “reviewing all features can be time-consuming” "
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "as “reviewing all features can be time-consuming” (P7). However, participants\nalso described the limitation of a feature importance explanation that it might identify features that are not important\n(P3,P4,P14) and less correlated with an outcome (P2,P16), which “might not be applicable and useful for the practice”\n(P9).\nSome participants mentioned that “reviewing which features AI identifies as important” (P6) is useful to understand\nhow the AI works (P12,P13,P16) and “check the strength and limitations of AI” (P15) for onboarding. In contrast, some\nparticipants elaborated that a feature importance explanation is more useful to validate an individual assessment for\ndecision support than onboarding (P5,P6,P16).\n4.5.2 Counterfactual Explanation. For a counterfactual explanation, participant"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "ion. For a counterfactual explanation, participants described that it is useful to review\nhow to update patient’s movement features to flip an AI output (P7,P11), which is “what we want to achieve for our\npatient (e.g. how to make a patient with compensation to not have compensation)” (P9). However, participants found it\nis difficult to understand (P1,P10) and time-consuming to get used to and validate whether counterfactual explanations\nmake sense or not (P1,P5,P12,P16).\nSome participants mentioned that reviewing counterfactual explanations “how features need to be changed to flip\nan AI output” (P3) is helpful to understand how AI defines the medical conditions (e.g. compensation) (P6,P13) and\nunderstand the accuracy and performance of an AI model (P3,P8) for onboarding. Other participant"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "AI model (P3,P8) for onboarding. Other participants described that\ncounterfactual explanations “bring richer insights on both normal and abnormal conditions than other explanations\nthat provide information on a single condition” (P2) and are “useful to validate an AI output” (P2) and “applicable in\npractice” (P3) for decision support.\n4.5.3 Prototype/Example-based Explanation. For an example-based explanation, participants valued that it is\nuseful to review relevant samples to a client/patient’s condition instead of searching a whole dataset (P5,P13,P14) or\n“relying on our memory to remember all past cases” (P9) and easy to understand to validate whether AI is correct or not\n(P1,P10,P12,P16). However, some participants mentioned that as individual post-stroke survivors are very different a"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "ividual post-stroke survivors are very different and\nspecific, reviewing relevant examples might not be very useful (P7,P8,P16). They also wondered whether AI might have\nenough samples to provide relevant samples (P3,P8).\nParticipants described the usefulness of an example-based explanation for onboarding as it shows how an AI model\ndefines a clinical concept/symptom (P5,P6), and it is “easier to interpret than others” (P6). In addition, reviewing a pool\nImproving Health Professionals’ Onboarding with AI and XAI for Trustworthy Human-AI Collaborative Decision\nMaking 19\nof samples is useful to draw relevant conclusions (P2,P3) and “confirm the validity of an AI output” (P13) for decision\nsupport.\n4.6 Suggestions to Improve Onboarding and Decision-Making with AI\nFor improving the understandi"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "ision-Making with AI\nFor improving the understanding of the strengths and limitations of AI during onboarding and human-AI collaborative\ndecision-making, participants recommended (i) communicating benchmark information and the benefits of AI and\n(ii) a trial period to interact with AI for calibrating user trust, refining an objective of AI, and tuning AI with user\nfeedback .\n4.6.1 Communicating Benchmark Information & Benefits of AI. When we asked the participants to define a desired\nperformance value of an AI model, they had difficulty with enumerating what it is desirable performance. Participants\nconsidered that “presenting benchmarkable information” (P7), such as how much AI matches with therapists’ agreement\n(P2,P3,P5,P6,P14) or characterizing the performance on different medical cond"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "terizing the performance on different medical conditions (P4,P6,P14) is necessary to better\nestimate a desirable performance for trustworthy AI (P3,P5).\nParticipants also suggested that describing “the benefits of using AI” (P4). As most participants had difficulty with\ncontextualizing what a specific score of an evaluation metric means, they considered that it would be more effective to\ncommunicate the benefits that they can easily understand. The example of these benefits of AI include “how much time\ncan be saved” (P5), “how well they can improve their decision than a therapist alone” (P5), or whether it can support a\nbetter health outcome for patients (P2,P4).\n4.6.2 Interaction Trials to Calibrate User Trust, Refine AI Objective, and Tune AI with Feedback. Participants\nconsidered that d"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "e AI with Feedback. Participants\nconsidered that demonstrating the strengths of AI through presenting a numerical value on an evaluation metric is still\nimportant. However, as a numerical value might not be sufficient to show the whole picture of AI performance, they also\nsuggested providing a trial period with multiple interactions (P1,P5,P13,P14,P16) similar to how they build a trustworthy\nrelationship with their colleagues over time (Section 4.1.2). By directly interacting with AI and observing how AI performs\n(P1,P12,P14,P16), participants considered that they can “check whether AI outputs are similar to what therapists consider”\n(P7), understand the strengths and limitations of AI, and determine “whether AI is really helpful or not” (P5).\nFor a trustworthy relationship with colleagues"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "5).\nFor a trustworthy relationship with colleagues, participants described the necessity of interactive communications over\ntime to align a common goal (Table 4.1.2). Along this line, participants wondered “if AI can have an adaptive goal to\nset the notion of a correct movement based on patient’s status” (P10). In addition, participants desired a way to provide\nfeedback to AI and update it accordingly (P5,P7) when any limitations of AI are identified and periodically refine AI with\nrelevant, up-to-date data (P7,P9,P15) for more trustworthy interactions with AI.\n4.6.3 Other Considerations: Periodic Audits, Multi-sites Validations, and Easy Setups & Usage. In addition to\ncommunicating the competence and benefits of AI and interaction trials, participants described other important factors to\n"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "participants described other important factors to\nconsider using AI in practice and make AI more trustworthy. First, participants considered that it is necessary to have\nperiodic, internal & external audits (P2,P14,P16) “even after passing the onboarding or a trial period” (P12). These\naudits refer to whether “AI can effectively provide relevant information” (P7), “therapists correctly use the system” (P12),\nand refining AI (Section 4.6.2), but are not limited to these [44, 47, 54].\nParticipants also considered that having validations with multiple colleagues in multiple sites (P1,P12) would be helpful\nto consider using AI. As “each therapist has different educational backgrounds” (P16) and “each hospital has different\ncultures” (P14), participants wondered how an AI-based system can be de"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "icipants wondered how an AI-based system can be deployed and applicable in various settings.\n20 Lee et al.\nFinally, as “time-consuming logistics and setups would be a big barrier” (P8) to consider using an AI-based system,\nthese systems should be easy to set up and use as health professionals do not have much time in a clinical setting\n(P1,P5,P7,P10).\n5 DISCUSSION\nIn this section, we highlight key takeaways on the usefulness and value of tutorials on AI and AI explanations to\nimprove communicating the strengths and limitations of AI for onboarding and human-AI collaborative decision-making.\nIn addition, we discuss design recommendations and future research for more effective onboarding with AI and AI\nexplanations and human-AI collaborative decision-making: 1) improving tutorials on AI and "
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "decision-making: 1) improving tutorials on AI and AI explanations, 2) AI\nexplanations for onboarding and interactive communications with AI, 3) measuring the level of understanding of AI and\nAI explanations, 4) beyond presenting a numerical, traditional performance metric, 5) goal alignment between users and\nAI and refining AI, and 6) audits to build a reputation of AI.\n5.1 Values & Limitations of Tutorials on AI and AI Explanations\nAn AI model card [ 43] aims to provide a useful way to provide the essential facts of AI models in a structured way.\nHowever, our results showed that an AI model card is not sufficient to support onboard health professionals with AI. Even\nbefore presenting an AI model card, as most of our participants haven’t used much AI, they are clueless about AI (e.g.\nhow i"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "ed much AI, they are clueless about AI (e.g.\nhow it works and helps or what the limitations are) (P1,P5,P6). Without any tutorials, “it will be difficult for the first-time\nuser without a technical background to use it” (P4). Thus, they described the importance of having tutorials in simple\nlanguages (P6,P16) so that they can “make use of AI in a short time” (P1).\nMuch in the way that participants (i.e. health professionals and students majoring in medicine and health) desired the\ncharacteristics of being \"friendly\" and \"approachable\" as a trustworthy colleague (Figure 4), our findings echo the needs\nto contextualize technical terminologies and make them \"friendly\" and \"approachable\" to improve the understanding of\nAI for AI-assisted decision-making [13, 29, 58].\nOur participants mentioned"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "on-making [13, 29, 58].\nOur participants mentioned that they learned a lot about what’s behind AI and AI explanations (P8,P10) through\nour onboarding tutorial materials. They highlighted the importance of educating and “introduce[ing] about AI and AI\nexplanations so that we[they] can have a better understanding on these to make the effective usage” (P10). However,\nwe found that some participants still asked clarification questions on AI or/and AI explanations after introducing\nour onboarding materials. In addition, even if participants developed understanding of AI and AI explanations, some\nparticipants were not clear how they can determine when AI is ‘ready’ to be used. “We[Users without technical\nbackgrounds] do not necessarily think about inspecting whether AI is trustworthy or not” (P2"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "t inspecting whether AI is trustworthy or not” (P2) and they “do not have any\nexperiences to check the validity of AI” (P1). Thus, it is necessary to educate and present a way (e.g. AI explanations) for\nthem to effectively onboard with AI but also inspect AI outputs (e.g. “communicating why AI generates a certain output\nand how to validate it” - P3).\nWe expect participants might need additional interactive tutorials on the development pipeline of an AI model (e.g.\nhow it processes data and metrics) and how to interact with and make use of AI and an AI explanation. Along this line,\nfurther research could explore how onboarding materials can be effectively delivered through interactivity, the choice of\nvisual cues, and data visualizations [16].\nImproving Health Professionals’ Onboarding with"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "].\nImproving Health Professionals’ Onboarding with AI and XAI for Trustworthy Human-AI Collaborative Decision\nMaking 21\n5.2 Design Recommendations\n5.2.1 AI Explanations for Onboarding and Interactive Communications with AI.\nAI explanations that aim to describe the behavior of the entire AI model or a specific AI output [ 1,18,31,68] have\nbeen increasingly explored to allow a user to understand when an AI model is right or wrong and can be trusted for\nAI-assisted decision making [ 10,30,69]. However, these AI explanations have been mainly explored to provide insights\non an AI output during the inference phase of an AI model for user’s decision support [ 12,52,75]. There have been\nlimited explorations on how AI explanations can be used to support user’s onboarding phase before moving to AI-a"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "port user’s onboarding phase before moving to AI-assisted\ndecision making phase.\nWhen we asked the participants to rank the usefulness of three AI explanations for onboarding and decision support\nphases, some participants differentiated the characteristics of onboarding and decision support tasks. Our results (Section\n4.5 and Figure 5) show that the usefulness rankings of AI explanations are different depending on the tasks (e.g. onboarding\nvs decision support) and also participants had different strategies of using AI explanations on a task. Aligned with previous\nresearch that describes the necessity of characterizing tasks and stakeholders [ 60], our study discusses a research problem\nof how AI explanations can be designed and used for onboarding phases. In addition, as interactive commu"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "boarding phases. In addition, as interactive communications\nare critical to build a trustworthy relationship with colleagues Figure 4), a further study is required to explore how AI\nexplanations can serve as a tool/medium for health professionals to have interactive communications with AI.\n5.2.2 Measuring Understanding of AI and AI Explanations.\nOur study also uncovered challenges and needs in measuring the level of understanding of AI and AI explanations for\nmore effective onboarding with AI/ML systems. When we recruited participants, we asked participants to respond with\ntheir experience of recent technologies (i.e. computer/laptop, activity tracker, virtual voice assistant, unmanned convenient\nstore, and autonomous vehicle) based on the questions designed by the Center for Research and "
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "questions designed by the Center for Research and Education on Aging and\nTechnology Enhancement (CREATE) [ 17] that aim to measure and profile older adults’ experience with technology [ 6].\nHowever, one’s experiences and higher exposures to recent technologies including AI applications do not necessarily\nmean that he or she will also have a good understanding of AI and AI explanations. For instance, P8 rated own technology\nexperience as 6.2 out of 7 and P16 had 4.0 out of 7 but both P8 and P16 incorrectly guessed AI/ML outputs (Table 1). In\ncontrast, P10 and P13 had technology experience scores of 3.2 and 3.8 respectively, which is lower than those of P8 and\nP16. Both P10 and P13 correctly guessed all AI/ML outputs (Table 1). Along this line, we found that normalized scores\nof technology e"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "e, we found that normalized scores\nof technology experiences are not correlated with the normalized scores of guessing AI outputs (Section 4.2).\nIn addition, even if our study explores the number of correctly guessing AI/ML outputs to measure participants’\nunderstanding of AI, this measurement is still limited. For instance, we observed that participants including ones who\nguessed all AL/ML outputs correctly asked clarification questions on AI and AI explanations. (Section 4.2 and 4.4).\nOverall, our study results show that participants’ technology experiences or their number of correctly guessing AI/ML\noutputs are not necessarily linearly correlated with their capabilities to understand how an AI operates nor how they\nappreciate the AI explanations. Thus, it is important to further explore"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "anations. Thus, it is important to further explore how we can measure user’s understanding of AI\nand AI explanations in a metric similar to how the previous studies assess the computer proficiency [ 8] and investigate\nwhat level of a metric indicates when a user has sufficient understanding to interact with AI and AI explanation.\n5.2.3 Beyond a Numerical Traditional Performance Metric.\nOur study results show the limitation of presenting the value of a traditional evaluation metric to either communicate\nthe overall performance of an AI model or take a specific action (e.g. determining whether to deploy an AI-based\n22 Lee et al.\nsystem or not). Unlike participants’ practice which they have interactive communications with colleagues over time\nto align a common goal and check the abilities of "
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "to align a common goal and check the abilities of colleagues to add value to the common goal for their trustworthy\nrelationships (Figure 4), one-directional presentation of a numerical performance value does not provide the whole picture\nand understanding of AI for trustworthy onboarding and usage.\nFor onboarding with an AI-based system, Cai et al. [ 13] recommended informing the overall performance of an\nAI-based system including its particular strengths and limitations. The Google’s People + AI Guidebook [ 50] recommends\nspecifying a threshold value of a performance metric to take a specific action in the ‘User Needs + Defining Success’\nsection. Given these recommended guidelines, we hypothesized that participants without technical background could\ndiscuss with AI/ML developers to review"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "ound could\ndiscuss with AI/ML developers to review the reference performance of therapists’ agreement level and specify a\nthreshold value of a performance metric to determine whether an AI-based decision support system can be considered\nbeing used in the practice. However, even if we presented the performance of an AI model to assess three common\nperformance components of rehabilitation assessment, our participants described the difficulty with understanding the\noverall performance of an AI model by reviewing a numerical value on a performance metric. “If we are told that an AI\nmodel has 90% accuracy, it might give a wrong mental model on AI performance as its performance might be changed in\nnew cases” (P11). Thus, participants desire a better way to “understand how well AI could perform o"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "ter way to “understand how well AI could perform on new data” (P7),\nsuch as describing the benefits of using AI. However, the benefits of using AI cannot be communicated with a traditional\nevaluation metric. Thus, it is worthwhile to explore how to quantify the benefits of an AI-based system (e.g. clinical\nutilities) and describe its values to the end users in a more understandable way.\n5.2.4 Goal Alignment between Users and AI and Refining AI.\nIn addition, our study results suggest a gap between the objective of AI and the goal of a user. Although health professionals\ntypically have a goal of improving patient’s status, AI systems are trained to maximize the probability of replicating a\ntherapist’s assessment. For a trustworthy relationship with colleagues, participants described the nece"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "p with colleagues, participants described the necessity of interactive\ncommunications over time to align a common goal and add a value to it (Figure 4). Along this line, participants wondered\n“if AI can have an adaptive goal to set the notion of a correct movement based on patient’s status” (P10). In addition,\nparticipants desired a way to provide feedback to AI and update it accordingly (P5,P7). When any limitations of AI are\nidentified, they desire to periodically refine AI with relevant, up-to-date data (P7,P9,P15) for more trustworthy interactions\nwith AI. For enabling trustworthy, human-AI collaborative decision-making, it would be critical to explore how to align\nan AI’s goal with a user’s goal and refine AI with the user’s feedback [35].\n5.2.5 Audits to Build a Reputation of AI.\nAs "
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "35].\n5.2.5 Audits to Build a Reputation of AI.\nAs health professionals considered colleagues’ reputation with others is an important factor in building a trustworthy\nrelationship with their colleagues (Figure 4), they also had a similar opinion that AI becomes more trustworthy if they\nhear more positive testimonials of colleagues from multiple sites (P4,P13,P14) (e.g. “colleagues can interpret AI outputs\nand reliably make use of them” - P13). To this end, our study also highlights the values of audits on AI [ 54]. As mentioned\nin Section 4.6.3, these audits can range from simply checking whether AI provides necessary information, refining AI\nwith feedback [ 35]. Also, the audit process can be monitoring and anticipating the potential negative impact of a system,\ndesigning mitigation or inf"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "ve impact of a system,\ndesigning mitigation or informing when to abandon the development and usage of an AI technology [ 54]. For onboarding\nwith an AI-based system and integrating it in practice, our study also unveiled challenges and needs on how these AI-based\nsystems for high-stake contexts can be audited and how we can support to educate people without technical backgrounds\nto participate in this process of deploying, onboarding, using, and auditing an AI-based system.\nImproving Health Professionals’ Onboarding with AI and XAI for Trustworthy Human-AI Collaborative Decision\nMaking 23\n5.3 Limitations\nOur study provides insights on information needs for onboarding with AI and AI explanations and discusses several\nexisting gaps and areas to improve for more effective onboarding with AI a"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "to improve for more effective onboarding with AI and trustworthy human-AI collaborative\nclinical decision-making. However, our study is limited to introducing and presenting the onboarding tutorial materials to\nparticipants (i.e. health professionals and students majoring in medicine and health) by an online session. Our study does\nnot observe how health professionals would initially interact with an AI system and AI explanations in a practical setting,\nwhich would bring richer and more useful insights on information needs to effectively onboard and interact with AI.\nIn addition, our study has a limitation of generalizing the results as we mainly explore the research questions in the\ncontext of a single clinical decision-making tasks along with brief descriptions of an AI/ML model training"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "with brief descriptions of an AI/ML model training and three\nAI explanations (i.e. feature importance, counterfactual, prototype/example-based). As the primary health professionals\nfrom the context of this study (i.e. rehabilitation therapy) are mostly females (e.g. around 62.7% of therapists are female\n[73]), the participants of this study are mostly females (87.5%). Also, our study does not involve a large number of\nparticipants even if such a small sample size is not unusual in similar previous works [ 10,13]. A further in-situ study with\nother decision-making tasks and types of ML models and explanations is necessary for further generalizable insights on\nimproving onboarding with AI and AI explanations.\n6 CONCLUSION\nIn this work, we contributed to an empirical study that explored how A"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "tributed to an empirical study that explored how AI and AI explanations can be first introduced\nto health professionals and students majoring in medicine and health and identified information needs on AI and AI\nexplanations for effective onboarding and trustworthy usage of AI. Our study suggested the value of onboarding tutorial\nmaterials on AI and AI explanations and the necessity of designing AI explanations for improving onboarding and\ncommunications with AI. Also, our study highlighted the importance of exploring metrics to characterize the user’s\nunderstanding of AI and AI explanations. In addition, our study discussed other considerations for effective onboarding\nand trustworthy human AI collaborative decision-making moving beyond describing a numerical traditional performance\nmetric"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "cribing a numerical traditional performance\nmetric: presenting user-understandable benchmark information, interactive trials to communicate the practical benefits of\nAI, calibrate user trust, refine an objective of AI and AI with user feedback, and AI audits. Future research should explore\nhow various types of AI/ML models and AI explanations on different contexts/tasks can be introduced to people without\ntechnical backgrounds.\nREFERENCES\n[1] Ashraf Abdul, Jo Vermeulen, Danding Wang, Brian Y Lim, and Mohan Kankanhalli. 2018. Trends and trajectories for explainable, accountable and\nintelligible systems: An hci research agenda. In Proceedings of the 2018 CHI conference on human factors in computing systems . 1–18.\n[2]Saleema Amershi, Dan Weld, Mihaela V orvoreanu, Adam Fourney, Besmira Nushi"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": ", Mihaela V orvoreanu, Adam Fourney, Besmira Nushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori\nInkpen, et al .2019. Guidelines for human-AI interaction. In Proceedings of the 2019 chi conference on human factors in computing systems . 1–13.\n[3] Theo Araujo, Natali Helberger, Sanne Kruikemeier, and Claes H De Vreese. 2020. In AI we trust? Perceptions about automated decision-making by\nartificial intelligence. AI & society 35 (2020), 611–623.\n[4]Vijay Arya, Rachel KE Bellamy, Pin-Yu Chen, Amit Dhurandhar, Michael Hind, Samuel C Hoffman, Stephanie Houde, Q Vera Liao, Ronny\nLuss, Aleksandra Mojsilovi ´c, et al .2019. One explanation does not fit all: A toolkit and taxonomy of ai explainability techniques. arXiv preprint\narXiv:1909.03012 (2019).\n[5] Emma Beede, Elizabeth Bayl"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": ":1909.03012 (2019).\n[5] Emma Beede, Elizabeth Baylor, Fred Hersch, Anna Iurchenko, Lauren Wilcox, Paisan Ruamviboonsuk, and Laura M Vardoulakis. 2020. A human-\ncentered evaluation of a deep learning system deployed in clinics for the detection of diabetic retinopathy. In Proceedings of the 2020 CHI conference\non human factors in computing systems . 1–12.\n[6] Jenay M Beer, Cory-Ann Smarr, Tiffany L Chen, Akanksha Prakash, Tracy L Mitzner, Charles C Kemp, and Wendy A Rogers. 2012. The domesticated\nrobot: design guidelines for assisting older adults to age in place. In Proceedings of the seventh annual ACM/IEEE international conference on\n24 Lee et al.\nHuman-Robot Interaction . 335–342.\n[7]Angie Boggust, Brandon Carter, and Arvind Satyanarayan. 2022. Embedding comparator: Visualizing differen"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": ". 2022. Embedding comparator: Visualizing differences in global structure and local\nneighborhoods via small multiples. In 27th international conference on intelligent user interfaces . 746–766.\n[8] Walter R Boot, Neil Charness, Sara J Czaja, Joseph Sharit, Wendy A Rogers, Arthur D Fisk, Tracy Mitzner, Chin Chin Lee, and Sankaran Nair. 2015.\nComputer proficiency questionnaire: assessing low and high computer proficient seniors. The Gerontologist 55, 3 (2015), 404–411.\n[9] Virginia Braun and Victoria Clarke. 2012. Thematic analysis. American Psychological Association.\n[10] Adrian Bussone, Simone Stumpf, and Dympna O’Sullivan. 2015. The role of explanations on trust and reliance in clinical decision support systems.\nIn2015 international conference on healthcare informatics . IEEE, 160–169.\n[1"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "ence on healthcare informatics . IEEE, 160–169.\n[11] Carrie J Cai, Jonas Jongejan, and Jess Holbrook. 2019. The effects of example-based explanations in a machine learning interface. In Proceedings of\nthe 24th international conference on intelligent user interfaces . 258–262.\n[12] Carrie J Cai, Emily Reif, Narayan Hegde, Jason Hipp, Been Kim, Daniel Smilkov, Martin Wattenberg, Fernanda Viegas, Greg S Corrado, Martin C\nStumpe, et al .2019. Human-centered tools for coping with imperfect algorithms during medical decision-making. In Proceedings of the 2019 chi\nconference on human factors in computing systems . 1–14.\n[13] Carrie J Cai, Samantha Winter, David Steiner, Lauren Wilcox, and Michael Terry. 2019. \" Hello AI\": uncovering the onboarding needs of medical\npractitioners for human-AI colla"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": " needs of medical\npractitioners for human-AI collaborative decision-making. Proceedings of the ACM on Human-computer Interaction 3, CSCW (2019), 1–24.\n[14] Carrie J Cai, Samantha Winter, David Steiner, Lauren Wilcox, and Michael Terry. 2021. Onboarding Materials as Cross-functional Boundary Objects\nfor Developing AI Assistants. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems . 1–7.\n[15] Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. 2015. Intelligible models for healthcare: Predicting\npneumonia risk and hospital 30-day readmission. In Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and\ndata mining . 1721–1730.\n[16] Anamaria Crisan, Margaret Drouhard, Jesse Vig, and Nazneen Rajani. "
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "Margaret Drouhard, Jesse Vig, and Nazneen Rajani. 2022. Interactive model cards: A human-centered approach to model\ndocumentation. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency . 427–439.\n[17] Sara J Czaja, Neil Charness, Arthur D Fisk, Christopher Hertzog, Sankaran N Nair, Wendy A Rogers, and Joseph Sharit. 2006. Factors predicting the\nuse of technology: Findings from the center for research and education on aging and technology enhancement (CREATE). Psychology and aging 21, 2\n(2006), 333.\n[18] Finale Doshi-Velez and Been Kim. 2017. Towards a rigorous science of interpretable machine learning. arXiv preprint arXiv:1702.08608 (2017).\n[19] Andre Esteva, Brett Kuprel, Roberto A Novoa, Justin Ko, Susan M Swetter, Helen M Blau, and Sebastian Thrun. 201"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": " M Swetter, Helen M Blau, and Sebastian Thrun. 2017. Dermatologist-level\nclassification of skin cancer with deep neural networks. nature 542, 7639 (2017), 115–118.\n[20] Luciano Floridi. 2019. Establishing the rules for building trustworthy AI. Nature Machine Intelligence 1, 6 (2019), 261–262.\n[21] Nicola K Gale, Gemma Heath, Elaine Cameron, Sabina Rashid, and Sabi Redwood. 2013. Using the framework method for the analysis of qualitative\ndata in multi-disciplinary health research. BMC medical research methodology 13, 1 (2013), 1–8.\n[22] Felix Gille, Anna Jobin, and Marcello Ienca. 2020. What we talk about when we talk about trust: Theory of trust for AI in healthcare. Intelligence-Based\nMedicine 1 (2020), 100001.\n[23] Leilani H Gilpin, David Bau, Ben Z Yuan, Ayesha Bajwa, Michael Specter, a"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": " Bau, Ben Z Yuan, Ayesha Bajwa, Michael Specter, and Lalana Kagal. 2018. Explaining explanations: An overview of\ninterpretability of machine learning. In 2018 IEEE 5th International Conference on data science and advanced analytics (DSAA) . IEEE, 80–89.\n[24] David J Gladstone, Cynthia J Danells, and Sandra E Black. 2002. The Fugl-Meyer assessment of motor recovery after stroke: a critical review of its\nmeasurement properties. Neurorehabilitation and neural repair 16, 3 (2002), 232–240.\n[25] Riccardo Guidotti, Anna Monreale, Fosca Giannotti, Dino Pedreschi, Salvatore Ruggieri, and Franco Turini. 2019. Factual and counterfactual\nexplanations for black box decision making. IEEE Intelligent Systems 34, 6 (2019), 14–23.\n[26] Alon Jacovi, Ana Marasovi ´c, Tim Miller, and Yoav Goldberg. 2021. For"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "asovi ´c, Tim Miller, and Yoav Goldberg. 2021. Formalizing trust in artificial intelligence: Prerequisites, causes and goals of\nhuman trust in AI. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency . 624–635.\n[27] Saif Khairat, David Marc, William Crosby, Ali Al Sanousi, et al .2018. Reasons for physicians not adopting clinical decision support systems: critical\nanalysis. JMIR medical informatics 6, 2 (2018), e8912.\n[28] Ajay Kohli and Saurabh Jha. 2018. Why CAD failed in mammography. Journal of the American College of Radiology 15, 3 (2018), 535–537.\n[29] Tzu-Sheng Kuo, Hong Shen, Jisoo Geum, Nev Jones, Jason I Hong, Haiyi Zhu, and Kenneth Holstein. 2023. Understanding Frontline Workers’ and\nUnhoused Individuals’ Perspectives on AI Used in Homeless Ser"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "dividuals’ Perspectives on AI Used in Homeless Services. In Proceedings of the 2023 CHI Conference on Human Factors in Computing\nSystems . 1–17.\n[30] Vivian Lai, Chacha Chen, Q Vera Liao, Alison Smith-Renner, and Chenhao Tan. 2021. Towards a science of human-ai decision making: a survey of\nempirical studies. arXiv preprint arXiv:2112.11471 (2021).\n[31] Himabindu Lakkaraju, Julius Adebayo, and Sameer Singh. 2020. Explaining machine learning predictions: State-of-the-art, challenges, and\nopportunities. NeurIPS Tutorial (2020).\n[32] Min Hun Lee and Chong Jun Chew. 2023. Understanding the Effect of Counterfactual Explanations on Trust and Reliance on AI for Human-AI\nCollaborative Clinical Decision Making. Proceedings of the ACM on Human-Computer Interaction 7, CSCW2 (2023), 1–22.\n[33] Min Hun "
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "r Interaction 7, CSCW2 (2023), 1–22.\n[33] Min Hun Lee, Daniel P Siewiorek, Asim Smailagic, Alexandre Bernardino, and Sergi Bermúdez i Badia. 2019. Learning to assess the quality of\nstroke rehabilitation exercises. In Proceedings of the 24th International Conference on Intelligent User Interfaces . 218–228.\nImproving Health Professionals’ Onboarding with AI and XAI for Trustworthy Human-AI Collaborative Decision\nMaking 25\n[34] Min Hun Lee, Daniel P Siewiorek, Asim Smailagic, Alexandre Bernardino, and Sergi Bermúdez i Badia. 2020. Co-design and evaluation of an\nintelligent decision support system for stroke rehabilitation assessment. Proceedings of the ACM on Human-Computer Interaction 4, CSCW2 (2020),\n1–27.\n[35] Min Hun Lee, Daniel P Siewiorek, Asim Smailagic, Alexandre Bernardino, and Serg"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "ek, Asim Smailagic, Alexandre Bernardino, and Sergi Bermúdez i Badia. 2021. A human-ai collaborative approach for\nclinical decision making on rehabilitation assessment. In Proceedings of the 2021 CHI conference on human factors in computing systems . 1–14.\n[36] Xin Li, Traci J Hess, and Joseph S Valacich. 2008. Why do we trust new technology? A study of initial trust formation with organizational information\nsystems. The Journal of Strategic Information Systems 17, 1 (2008), 39–71.\n[37] Weixin Liang, Girmaw Abebe Tadesse, Daniel Ho, L Fei-Fei, Matei Zaharia, Ce Zhang, and James Zou. 2022. Advances, challenges and opportunities\nin creating data for trustworthy AI. Nature Machine Intelligence 4, 8 (2022), 669–677.\n[38] Alex John London. 2019. Artificial intelligence and black-box medical dec"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": " Artificial intelligence and black-box medical decisions: accuracy versus explainability. Hastings Center Report 49, 1 (2019),\n15–21.\n[39] Scott M Lundberg and Su-In Lee. 2017. A unified approach to interpreting model predictions. Advances in neural information processing systems 30\n(2017).\n[40] Thomas M Maddox, John S Rumsfeld, and Philip RO Payne. 2019. Questions for artificial intelligence in health care. Jama 321, 1 (2019), 31–32.\n[41] Roger C Mayer, James H Davis, and F David Schoorman. 1995. An integrative model of organizational trust. Academy of management review 20, 3\n(1995), 709–734.\n[42] Nora McDonald, Sarita Schoenebeck, and Andrea Forte. 2019. Reliability and inter-rater reliability in qualitative research: Norms and guidelines for\nCSCW and HCI practice. Proceedings of the ACM"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": " for\nCSCW and HCI practice. Proceedings of the ACM on human-computer interaction 3, CSCW (2019), 1–23.\n[43] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit\nGebru. 2019. Model cards for model reporting. In Proceedings of the conference on fairness, accountability, and transparency . 220–229.\n[44] Jakob Mökander and Luciano Floridi. 2021. Ethics-based auditing to develop trustworthy AI. Minds and Machines 31, 2 (2021), 323–327.\n[45] Ramaravind K Mothilal, Amit Sharma, and Chenhao Tan. 2020. Explaining machine learning classifiers through diverse counterfactual explanations.\nInProceedings of the 2020 conference on fairness, accountability, and transparency . 607–617.\n[46] T Nathan Mundhenk, Barry Y "
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "arency . 607–617.\n[46] T Nathan Mundhenk, Barry Y Chen, and Gerald Friedland. 2019. Efficient saliency maps for explainable AI. arXiv preprint arXiv:1911.11293\n(2019).\n[47] Ivy Munoko, Helen L Brown-Liburd, and Miklos Vasarhelyi. 2020. The ethical implications of using artificial intelligence in auditing. Journal of\nBusiness Ethics 167 (2020), 209–234.\n[48] Ju Gang Nam, Sunggyun Park, Eui Jin Hwang, Jong Hyuk Lee, Kwang-Nam Jin, Kun Young Lim, Thienkai Huy Vu, Jae Ho Sohn, Sangheum Hwang,\nJin Mo Goo, et al .2019. Development and validation of deep learning–based automatic detection algorithm for malignant pulmonary nodules on chest\nradiographs. Radiology 290, 1 (2019), 218–228.\n[49] J Arturo Olvera-López, J Ariel Carrasco-Ochoa, J Martínez-Trinidad, and Josef Kittler. 2010. A review of ins"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "Trinidad, and Josef Kittler. 2010. A review of instance selection methods. Artificial\nIntelligence Review 34, 2 (2010), 133–143.\n[50] Google PAIR. 2019. People + AI Guidebook. https://pair.withgoogle.com/guidebook/\n[51] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al .2019. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems 32\n(2019).\n[52] Forough Poursabzi-Sangdeh, Daniel G Goldstein, Jake M Hofman, Jennifer Wortman Wortman Vaughan, and Hanna Wallach. 2021. Manipulating\nand measuring model interpretability. In Proceedings of the 2021 CHI conference on human factors in computing systems . 1–52.\n[53] Alun Preece. 2018. A"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "omputing systems . 1–52.\n[53] Alun Preece. 2018. Asking ‘Why’in AI: Explainability of intelligent systems–perspectives and challenges. Intelligent Systems in Accounting, Finance\nand Management 25, 2 (2018), 63–72.\n[54] Inioluwa Deborah Raji, Andrew Smart, Rebecca N White, Margaret Mitchell, Timnit Gebru, Ben Hutchinson, Jamila Smith-Loud, Daniel Theron, and\nParker Barnes. 2020. Closing the AI accountability gap: Defining an end-to-end framework for internal algorithmic auditing. In Proceedings of the\n2020 conference on fairness, accountability, and transparency . 33–44.\n[55] Pranav Rajpurkar, Emma Chen, Oishi Banerjee, and Eric J Topol. 2022. AI in health and medicine. Nature medicine 28, 1 (2022), 31–38.\n[56] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \" Why should i tru"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "ngh, and Carlos Guestrin. 2016. \" Why should i trust you?\" Explaining the predictions of any classifier. In Proceedings\nof the 22nd ACM SIGKDD international conference on knowledge discovery and data mining . 1135–1144.\n[57] Mark Sendak, Madeleine Clare Elish, Michael Gao, Joseph Futoma, William Ratliff, Marshall Nichols, Armando Bedoya, Suresh Balu, and Cara\nO’Brien. 2020. \" The human body is a black box\" supporting clinical decision-making with deep learning. In Proceedings of the 2020 conference on\nfairness, accountability, and transparency . 99–109.\n[58] Hong Shen, Haojian Jin, Ángel Alexander Cabrera, Adam Perer, Haiyi Zhu, and Jason I Hong. 2020. Designing alternative representations of\nconfusion matrices to support non-expert public understanding of algorithm performance. Proceeding"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "understanding of algorithm performance. Proceedings of the ACM on Human-Computer Interaction 4,\nCSCW2 (2020), 1–22.\n[59] Ramandeep Singh, Mannudeep K Kalra, Chayanin Nitiwarangkul, John A Patti, Fatemeh Homayounieh, Atul Padole, Pooja Rao, Preetham Putha,\nVictorine V Muse, Amita Sharma, et al .2018. Deep learning in chest radiography: detection of findings and presence of change. PloS one 13, 10\n(2018), e0204155.\n26 Lee et al.\n[60] Harini Suresh, Steven R Gomez, Kevin K Nam, and Arvind Satyanarayan. 2021. Beyond expertise and roles: A framework to characterize the\nstakeholders of interpretable machine learning and their needs. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems .\n1–16.\n[61] Reed T Sutton, David Pincock, Daniel C Baumgart, Daniel C Sadowski, Rich"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "incock, Daniel C Baumgart, Daniel C Sadowski, Richard N Fedorak, and Karen I Kroeker. 2020. An overview of clinical\ndecision support systems: benefits, risks, and strategies for success. NPJ digital medicine 3, 1 (2020), 17.\n[62] Eric J Topol. 2019. High-performance medicine: the convergence of human and artificial intelligence. Nature medicine 25, 1 (2019), 44–56.\n[63] Ehsan Toreini, Mhairi Aitken, Kovila Coopamootoo, Karen Elliott, Carlos Gonzalez Zelaya, and Aad Van Moorsel. 2020. The relationship between\ntrust in AI and trustworthy machine learning technologies. In Proceedings of the 2020 conference on fairness, accountability, and transparency .\n272–283.\n[64] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. Journal of machine learning research 9, 11 (200"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "E. Journal of machine learning research 9, 11 (2008).\n[65] Kush R Vashney. 2022. Trustworthy machine learning . Independently published.\n[66] Sahil Verma, Varich Boonsanong, Minh Hoang, Keegan E Hines, John P Dickerson, and Chirag Shah. 2020. Counterfactual explanations and\nalgorithmic recourses for machine learning: A review. arXiv preprint arXiv:2010.10596 (2020).\n[67] Dakuo Wang, Liuping Wang, Zhan Zhang, Ding Wang, Haiyi Zhu, Yvonne Gao, Xiangmin Fan, and Feng Tian. 2021. “Brilliant AI doctor” in rural\nclinics: Challenges in AI-powered clinical decision support system deployment. In Proceedings of the 2021 CHI conference on human factors in\ncomputing systems . 1–18.\n[68] Danding Wang, Qian Yang, Ashraf Abdul, and Brian Y Lim. 2019. Designing theory-driven user-centric explainable AI. I"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "gning theory-driven user-centric explainable AI. In Proceedings of the 2019\nCHI conference on human factors in computing systems . 1–15.\n[69] Xinru Wang and Ming Yin. 2021. Are explanations helpful? a comparative study of the effects of explanations in ai-assisted decision-making. In 26th\ninternational conference on intelligent user interfaces . 318–328.\n[70] Jeannette M Wing. 2021. Trustworthy ai. Commun. ACM 64, 10 (2021), 64–71.\n[71] Svante Wold, Kim Esbensen, and Paul Geladi. 1987. Principal component analysis. Chemometrics and intelligent laboratory systems 2, 1-3 (1987),\n37–52.\n[72] Qian Yang, Aaron Steinfeld, and John Zimmerman. 2019. Unremarkable AI: Fitting intelligent decision support into critical, clinical decision-making\nprocesses. In Proceedings of the 2019 CHI conference on "
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "ses. In Proceedings of the 2019 CHI conference on human factors in computing systems . 1–11.\n[73] Steven Zauderer. 2023. Statistics, Facts & Demographics of Physical Therapy. https://www.crossrivertherapy.com/research/physical-therapy-\nstatistics#:~:text=67%25%20of%20physical%20therapists%20are,common%20gender%20in%20the%20occupation.\n[74] Aleš Završnik. 2020. Criminal justice, artificial intelligence systems, and human rights. In ERA forum , V ol. 20. Springer, 567–583.\n[75] Yunfeng Zhang, Q Vera Liao, and Rachel KE Bellamy. 2020. Effect of confidence and explanation on accuracy and trust calibration in AI-assisted\ndecision making. In Proceedings of the 2020 conference on fairness, accountability, and transparency . 295–305.\nA IMPLEMENTATIONS OF AN AI MODEL\nWe followed the previous resear"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "ONS OF AN AI MODEL\nWe followed the previous research [ 33] to learn an AI model for rehabilitation assessment. Specifically, we processed the\nestimated joint positions of post-stroke survivors’ exercises to extract various kinematic features. The kinematic features\nof the ‘Range of Motion’ (ROM) include joint angles, such as elbow flexion, shoulder flexion, and elbow extension, and\nnormalized relative trajectory (i.e. the Euclidean distance between two joints - head and wrist; head and elbow), and the\nnormalized trajectory distance (i.e. the absolute distance between two joints - head and wrist, shoulder and wrist) in the\nx, y, and z coordinates [ 33]. The features of the ‘Compensation’ include the normalized trajectories, which indicate the\ndistances between joint positions of the head, s"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "e\ndistances between joint positions of the head, spine, and shoulder in the x, y, and z coordinates from the initial to the\ncurrent frame over the entire exercise motion [33].\nAs previous research demonstrated the outperformance of a feed-forward Neural Network (NN) model to classify\nthe quality of post-stroke survivors’ motion [ 33], we utilized the extracted kinematic features and labels of post-stroke\nsurvivors’ exercises to implement a feed-forward NN model using Pytorch libraries [ 51]. For the labels, we utilized\nthe labels by the expert therapist, who conducted the clinically validated assessment test. We grid-searched various\narchitectures (i.e. one to three layers with 32, 64, 128, 256, and 512 hidden units) and different learning rates (i.e. 0.0001,\n0.0005, 0.0001, 0.005, 0.001) "
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "rates (i.e. 0.0001,\n0.0005, 0.0001, 0.005, 0.001) while training a feed-forward NN model with cross-entropy loss and the mini-batch size of\n1 and epoch of 4. For training and evaluating the model, we utilized the leave-one-subject-out cross-validation, where\nwe trained the model with data from all post-stroke survivors except one post-stroke survivor and tested the model with\nImproving Health Professionals’ Onboarding with AI and XAI for Trustworthy Human-AI Collaborative Decision\nMaking 27\ndata from the held-out post-stroke survivor. The final model architectures and learning rates are three layers with 256\nhidden units and 0.005 of the learning rate for the ROM, one layer with 16 hidden units and 0.0001 of the learning rate for\nthe Smoothness, and three layers with 64 hidden units and 0."
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "ness, and three layers with 64 hidden units and 0.005 of the learning rate for the Compensation. The models\nachieved 82% F1-score, 79% F1-score and 77% F1-score to replicate therapists’ assessment on ‘ROM’, ‘Smoothness’,\nand ‘Compensation’ components respectively.\nB DETAILS OF THE STUDY: PARTICIPANTS, INTERVIEWS, AND DATA ANALYSIS\nTable 2. Detailed Demographics of Participants: Therapists who have experience in stroke rehabilitation ( P1 - P10 ) and\nother health professionals ( P11 - P12 ) and students majoring in medicine or health (e.g. therapy, nursing) ( P13 - P16 ).\nPID Sex Age Occupation Setting # of yrs Q. Tech Experience Q. ML Outputs\nP1 Female 25 - 34 years PhysioTherapist (PT) Outpatient Clinic 7 4.8 out of 7 3 out of 3\nP2 Male 25 - 34 years PhysioTherapist (PT) Inpatient Rehabil"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": " - 34 years PhysioTherapist (PT) Inpatient Rehabilitation 2 5.2 out of 7 1 out of 3\nP3 Male 25 - 34 years PhysioTherapist (PT) Home Care 8 4.4 out of 7 2 out of 3\nP4 Female 35 - 44 years PhysioTherapist (PT) Outpatient Clinic 11 5.8 out of 7 2 out of 3\nP5 Female 25 - 34 years PhysioTherapist (PT) Inpatient Rehabilitation 9 5.4 out of 7 2 out of 3\nP6 Female 45 - 54 years PhysioTherapist (PT) Skilled Nursing Facility 30 5.8 out of 7 2 out of 3\nP7 Female 35 - 44 years Occupational Therapist (OT) Outpatient Clinic 14 5.4 out of 7 2 out of 3\nP8 Female 35 - 44 years Occupational Therapist (OT) Homecare 11 6.2 out of 7 3 out of 3\nP9 Female 25 - 34 years Occupational Therapist (OT) Skilled Nursing Facility 6 4.4 out of 7 2 out of 3\nP10 Female 25 - 34 years Occupational Therapist (OT) Inpatient Reh"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "34 years Occupational Therapist (OT) Inpatient Rehabilitation 5 3.2 out of 7 3 out of 3\nP11 Female 25 - 34 years Speech Therapist Community outpatient 5 4.8 out of 7 2 out of 3\nP12 Female 25 - 34 years Medical Social Worker n/a 5 4.0 out of 7 1 out of 3\nP13 Female 25 - 34 years Student in Occupational Therapy n/a n/a 3.8 out of 7 3 out of 3\nP14 Female 25 - 34 years Student in Speech Therapy n/a n/a 3.8 out of 7 2 out of 3\nP15 Female 18 - 24 years Student in Medicine n/a n/a 3.8 out of 7 2 out of 3\nP16 Female 18 - 24 years Student in Nursing n/a n/a 4.0 out of 7 0 out of 3\n28 Lee et al.\nTable 3. List of questions for a semi-strucutured interview\nParts of a Semi-Structured Interview Prompt Questions\nRehabilitation practices\n& trustworthy relationshipsHow many colleagues you have\n& how freque"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "tionshipsHow many colleagues you have\n& how frequently you interact with them for rehabilitation assessment?\nHow do you build a trustworthy relationship with your colleagues?\nWhen you have an uncertain case, how do you know a particular colleague will be good for discussion?\nWhat aspects of your colleagues make them trustworthy?\nIntro to AIAny questions/information needs about how the system operates\nand the development and evaluation pipeline of an AI system\nWe tried to collect data from post-stroke survivors with diverse FMA scores.\nDo you have any particular post-stroke survivors that should be included in the dataset?\nAI system cannot be perfect.\nDo you have a particular performance that is required for your trustful usage?\nAt least XX F1-score an AI model needs to achieve to consider "
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "F1-score an AI model needs to achieve to consider using AI and your trustful usage if being used?\nDo you have any specific conditions (e.g. Full ROM, shoulder/trunk compensation) or edge cases\nthat AI should be evaluated and good at for consider using AI and your trustful usage if being used?\nAI ExplanationsAny questions or information needs about AI explanations\n(e.g. Feature Importance, Counterfactual, Example-based)?\nRank which AI explanations are the most useful to onboard with AI\nand understand overall performance & strengths/limitations of AI?\nWhy do you consider that a particular explanation is useful or not to onboard with AI\nand understand overall performance and strengths/limitations of AI?\nRank which AI explanations are the most useful to make a decision with AI\nand determine wh"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "useful to make a decision with AI\nand determine whether to trust an AI outcome or not?\nWhy do you consider that a particular explanation is useful or not to\nmake a decision with AI and determine whether to trust an AI prediction or not?\nWrap-upAny comments/suggestions on how we can address the following issues:\n>how to determine whether AI has sufficient performance\n>how to improve onboarding with AI and determine strengths/limitations of AI\n>how to improve decision-making with AI and inspect whether AI outputs are trustful or not\nImproving Health Professionals’ Onboarding with AI and XAI for Trustworthy Human-AI Collaborative Decision\nMaking 29\nTable 4. The five high-level themes, twenty one second-level themes, and eighty five third-level themes of qualitative data\nanalysis\nHigh-level Th"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": " themes of qualitative data\nanalysis\nHigh-level Themes Second-level Themes Third-level Themes\nPractices of Rehabilitation Practices on TherapyTeams, Assessment & Therapy,\nMeetings, Communications\nPractices on Uncertain Decision-MakingMeetings, Other Info,\nFactors to identify colleagues (Referral, Expertises, Soft Skills)\nProcess to Build a Trustworthy RelationshipPeriod of time, Understand role & needs,\nShare common goals, Add values,\nHonest, Friendly, Listen & Interact\nCharacteristics of Trustworthy ColleaguesPersonalities,\nKnowledge & Background, Consistent & Reliable,\nWork Ethics & Timeliness, Confidentiality\nIntro to AI Guesses on AI outputs -\nComments on the dataset of AIRecruitment criterion\n- Demographics of participants (age, sex, race, etc.)\n- Stroke conditions (impact, spasticity"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "ace, etc.)\n- Stroke conditions (impact, spasticity, tone, finer motions, etc.)\nNumber of samples\nLabels\nRequired performance of AIContext-specific thresholds, specific numbers,\nUnclear meaning of numbers,\nReference\nEvaluation and edge cases of AICommon symptoms, Difficult tasks,\nBorderline cases,\nRealistic, uncontrolled settings\nQuestions about AIDefinitions, Operation speed,\nMeaning of a confusion matrix and a confidence score,\nHow to perform and fail on a certain case\nStrengths of AI Process data quickly, Reduce workload, Objective data\nLimitations of AITroublesome and time-consuming,\nRequires a specific setup,\nCan’t show me exact symptoms\nSuggestions on AINot familiar with how to onboard & make a decision with AI\nNeed to be usable\nPeriodic audits,\nStudies from multiple sites\nOther rehab"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "ic audits,\nStudies from multiple sites\nOther rehabilitation-specific features\nOthers\nAI Explanations Comments/questions on AI explanations Clarifications on the concept\nParticipants with the same/different rankings\non onboarding & decision-support\nRanking of AI explanations for onboarding\nRanking of AI explanations for decision-support\nExample-based explanationsQuestions, Comments (General, Pros, Cons),\nOnboarding, Decision-support\nFeature importance explanationsQuestions, Comments (General, Pros, Cons),\nOnboarding, Decision-support\nCounterfactual explanationsQuestions, Comments (General, Pros, Cons),\nOnboarding, Decision-support\nOnboarding with AI Suggestions on onboardingValues of tutorials, Beyond numbers,\nBenchmark references, Demonstrate benefits,\nInteractions, Referrals and testimoni"
  },
  {
    "arxiv_id": "2405.16424",
    "title": "Improving Health Professionals' Onboarding with AI and XAI for\n  Trustworthy Human-AI Collaborative Decision Making",
    "chunk": "te benefits,\nInteractions, Referrals and testimonials,\nOther factors\nAI-assisted decision-support Suggestions on decision-supportTrial periods,\nUpdate AI with feedback,\nOther factors (e.g. setups, user-friendly)\n"
  }
]